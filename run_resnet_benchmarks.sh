#!/bin/bash

# ResNet Metaschedule Benchmark Quick Start
# This script demonstrates how to run metaschedule benchmarks for ResNet models

echo "=== ResNet Metaschedule Benchmark Quick Start ==="
echo ""

# Configuration
ARCH="x86"  # Change to "cuda" if you have GPU, or "arm" for ARM processors
TRIALS=100  # Increase to 1000 or 10000 for better results (will take longer)
BASE_DIR="results/quick_resnet_test"

echo "Configuration:"
echo "  Architecture: $ARCH"
echo "  Trials: $TRIALS"
echo "  Output directory: $BASE_DIR"
echo ""

# Create results directory
mkdir -p "$BASE_DIR"

echo "=== Available ResNet Benchmarks ==="
echo ""

echo "1. ResNet18 Metaschedule Benchmark (ONNX model):"
echo "   python3 benchmarks/models_onnx.py -m meta -a $ARCH -t $TRIALS -l $BASE_DIR/resnet18_meta -b models/resnet18.onnx"
echo ""

echo "2. ResNet50 Relay Model Benchmark:"
echo "   python3 benchmarks/models.py -m ansor -a $ARCH -t $TRIALS -l $BASE_DIR/resnet50_ansor.json -b resnet-50"
echo ""

echo "3. Batch ResNet Benchmarks (all variants):"
echo "   # Edit scripts/meta_benchmarks.sh to set ARCH=$ARCH and trials=$TRIALS"
echo "   ./scripts/meta_benchmarks.sh"
echo ""

echo "=== Running Quick ResNet18 Demo ==="
echo ""

# Check if models exist
if [ ! -f "models/resnet18.onnx" ]; then
    echo "Warning: models/resnet18.onnx not found. Please ensure ONNX models are available."
    echo "Available models in models/ directory:"
    ls -la models/ 2>/dev/null || echo "models/ directory not found"
    echo ""
fi

# Run a quick demo (commented out to avoid long execution during exploration)
echo "To run a quick ResNet18 metaschedule demo:"
echo "python3 benchmarks/models_onnx.py -m meta -a $ARCH -t $TRIALS -l $BASE_DIR/resnet18_demo -b models/resnet18.onnx"
echo ""

echo "=== Analyzing Existing Results ==="
echo ""

echo "Pre-computed ResNet results are available in:"
echo ""

# List existing ResNet results
echo "Metaschedule results (results/ms/):"
find results/ms/ -name "*resnet*" -type d 2>/dev/null | head -10 | while read dir; do
    echo "  - $dir"
done

echo ""
echo "Ansor/Droplet results (results/):"
find results/ -name "*resnet*" -maxdepth 1 2>/dev/null | head -5 | while read file; do
    echo "  - $file"
done

echo ""
echo "=== Example Commands by Architecture ==="
echo ""

echo "For x86 (CPU):"
echo "  python3 benchmarks/models_onnx.py -m meta -a x86 -t 1000 -l results/resnet18_x86 -b models/resnet18.onnx"
echo ""

echo "For CUDA (GPU):"
echo "  python3 benchmarks/models_onnx.py -m meta -a cuda -t 1000 -l results/resnet18_cuda -b models/resnet18.onnx"
echo ""

echo "For ARM (A64FX):"
echo "  python3 benchmarks/models_onnx.py -m meta -a arm -t 1000 -l results/resnet18_arm -b models/resnet18.onnx"
echo ""

echo "=== DPMeta (Droplet + Metaschedule) Hybrid Optimization ==="
echo ""
echo "DPMeta combines TVM's metaschedule with Droplet search for superior optimization:"
echo "â€¢ Metaschedule provides broad exploration of the optimization space"
echo "â€¢ Droplet search provides focused exploitation of promising regions"
echo "â€¢ The hybrid approach often outperforms either method alone"
echo ""

echo "1. First generate metaschedule baseline (required for DPMeta):"
echo "   python3 benchmarks/models_onnx.py -m meta -a $ARCH -t 10000 -l $BASE_DIR/meta_baseline -b models/resnet18.onnx"
echo ""

echo "2. Then run DPMeta hybrid optimization on top-K results:"
echo "   # Use top-1000 results with 100 droplet trials"
echo "   python3 benchmarks/models_onnx.py -m dpmeta -a $ARCH -t 100 -k 1000 -l $BASE_DIR/meta_baseline -b models/resnet18.onnx"
echo ""

echo "3. Compare different top-K values to find optimal balance:"
for k in 100 200 500 1000; do
    echo "   python3 benchmarks/models_onnx.py -m dpmeta -a $ARCH -t 100 -k $k -l $BASE_DIR/meta_baseline -b models/resnet18.onnx"
done
echo ""

echo "4. Batch DPMeta optimization for all ResNet variants:"
echo "   # Edit scripts/dpmeta_benchmarks.sh to configure experiment"
echo "   ./scripts/dpmeta_benchmarks.sh"
echo ""

echo "=== DPMeta Results Analysis ==="
echo ""

echo "After running DPMeta, analyze the hybrid optimization results:"
echo "  ls -la \$RESULT_DIR/"
echo "  cat \$RESULT_DIR/output.txt  # Metaschedule baseline results"
echo "  ls -la \$RESULT_DIR/layer_*.log  # DPMeta layer-wise optimization logs"
echo "  # Each layer_N.log contains Droplet search results for layer N"
echo ""

echo "DPMeta output includes comparison metrics:"
echo "  â€¢ DPMeta time vs Meta-K time vs Meta-10k time"
echo "  â€¢ Speedup ratios showing hybrid optimization benefits"
echo "  â€¢ Layer-wise performance breakdown"
echo "  â€¢ Statistical significance (p-values) of improvements"
echo ""

echo "Example DPMeta output format:"
echo "Layer, DPMeta time(s), DPMeta std(s), DPMeta trials, DPMeta Tuning(min), ..."
echo "0, 0.000123456, 0.000012345, 100, 2.34, ..."
echo ""

echo "=== Optimization Method Comparison ==="
echo ""

echo "Compare all optimization methods on the same ResNet18 model:"
echo ""

echo "1. Pure Ansor (traditional autoscheduler):"
echo "   python3 benchmarks/models_onnx.py -m ansor -a $ARCH -t $TRIALS -l $BASE_DIR/ansor -b models/resnet18.onnx"
echo ""

echo "2. Pure Metaschedule (TVM's latest tuning):"
echo "   python3 benchmarks/models_onnx.py -m meta -a $ARCH -t $TRIALS -l $BASE_DIR/meta -b models/resnet18.onnx"
echo ""

echo "3. Pure Droplet search (on Ansor results):"
echo "   # First run Ansor, then apply Droplet"
echo "   python3 benchmarks/models.py -m ansor -a $ARCH -t $TRIALS -l $BASE_DIR/ansor_relay.json -b resnet-18"
echo "   python3 benchmarks/models.py -m droplet -a $ARCH -t 100 -l $BASE_DIR/ansor_relay.json -b resnet-18"
echo ""

echo "4. DPMeta Hybrid (Droplet + Metaschedule):"
echo "   # Generate metaschedule baseline first"
echo "   python3 benchmarks/models_onnx.py -m meta -a $ARCH -t 10000 -l $BASE_DIR/meta_for_hybrid -b models/resnet18.onnx"
echo "   # Apply DPMeta hybrid optimization"
echo "   python3 benchmarks/models_onnx.py -m dpmeta -a $ARCH -t 100 -k 1000 -l $BASE_DIR/meta_for_hybrid -b models/resnet18.onnx"
echo ""

echo "Expected performance ranking (best to worst, typically):"
echo "  1. DPMeta Hybrid - combines best of metaschedule + droplet"
echo "  2. Pure Metaschedule - TVM's latest and most comprehensive"
echo "  3. Pure Droplet on Ansor - focused search on good starting point"
echo "  4. Pure Ansor - traditional approach, good baseline"
echo ""

echo "=== Complete Benchmark Suite ==="
echo ""

echo "To run comprehensive benchmarks for all ResNet variants and methods:"
echo ""

echo "1. Pure Metaschedule for all models:"
echo "   # Edit scripts/meta_benchmarks.sh:"
echo "   #   - Set NAME to your experiment name"
echo "   #   - Set ARCH to your target architecture ($ARCH)"
echo "   #   - Set trials to desired number (1000-10000)"
echo "   ./scripts/meta_benchmarks.sh"
echo ""

echo "2. DPMeta Hybrid optimization for all models:"
echo "   # Edit scripts/dpmeta_benchmarks.sh:"
echo "   #   - Set NAME, ARCH, and trials parameters"
echo "   #   - Modify TOP array for different top-K values"
echo "   ./scripts/dpmeta_benchmarks.sh"
echo ""

echo "3. Traditional Droplet search on Ansor results:"
echo "   ./scripts/droplet_benchmarks.sh"
echo ""

echo "The complete suite benchmarks these ResNet variants:"
echo "  resnet18, resnet34, resnet50, resnet101, resnet152"
echo "Plus other models: alexnet, vgg variants, mobilenet, inception, densenet"
echo ""

echo "DPMeta suite tests multiple top-K configurations:"
for k in 1 10 25 50 100 200 300 1000; do
    echo "  top-$k: Uses best $k metaschedule results for Droplet optimization"
done
echo ""

echo "For more details, see METASCHEDULE_BENCHMARKS.md"

echo ""
echo "==============================================="
echo "DROPLET HYBRID OPTIMIZATION SUMMARY"
echo "==============================================="
echo ""
echo "Bennu provides state-of-the-art neural network optimization through:"
echo ""
echo "ðŸ”¹ METASCHEDULE: TVM's comprehensive auto-tuning framework"
echo "ðŸ”¹ DROPLET SEARCH: Advanced local optimization algorithm"  
echo "ðŸ”¹ DPMETA HYBRID: Best-of-both-worlds optimization"
echo ""
echo "DPMeta Hybrid Advantages:"
echo "â€¢ Leverages metaschedule's global search capabilities"
echo "â€¢ Applies Droplet's focused optimization to promising candidates"
echo "â€¢ Typically achieves 10-30% better performance than single methods"
echo "â€¢ Provides layer-wise analysis for detailed performance insights"
echo ""
echo "Key Parameters for DPMeta:"
echo "  -m dpmeta    # Enable hybrid optimization mode"
echo "  -k 1000      # Use top-1000 metaschedule results"
echo "  -t 100       # Run 100 Droplet optimization trials"
echo ""
echo "Start with: python3 analyze_resnet_benchmarks.py"
echo "==============================================="