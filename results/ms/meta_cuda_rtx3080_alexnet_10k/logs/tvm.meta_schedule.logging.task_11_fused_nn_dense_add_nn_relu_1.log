2024-03-20 19:37:47 [INFO] [task_scheduler.cc:160] Initializing Task #11: "fused_nn_dense_add_nn_relu_1"
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT = T.alloc_buffer((T.int64(1), T.int64(4096)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(4096)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("T_matmul_NT"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(p0[v_i0, v_k], p1[v_i1, v_k])
                T.writes(T_matmul_NT[v_i0, v_i1])
                with T.init():
                    T_matmul_NT[v_i0, v_i1] = T.float32(0)
                T_matmul_NT[v_i0, v_i1] = T_matmul_NT[v_i0, v_i1] + p0[v_i0, v_k] * p1[v_i1, v_k]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(4096)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(T_matmul_NT[v_ax0, v_ax1], p2[v_ax0, v_ax1])
                T.writes(T_add[v_ax0, v_ax1])
                T_add[v_ax0, v_ax1] = T_matmul_NT[v_ax0, v_ax1] + p2[v_ax0, v_ax1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(4096)):
            with T.block("T_relu"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(T_add[v_ax0, v_ax1])
                T.writes(T_relu[v_ax0, v_ax1])
                T_relu[v_ax0, v_ax1] = T.max(T_add[v_ax0, v_ax1], T.float32(0))
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                        for k_0 in range(T.int64(16)):
                            for ax0_ax1_fused in range(T.int64(256)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(524288)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + ax0_ax1_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(256), T.int64(32), T.int64(1), T.int64(1)):
                                with T.block("T_matmul_NT"):
                                    v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                    v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(256) + i1_3 + i1_4)
                                    v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(32) + k_2)
                                    T.reads(p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                    T.writes(T_matmul_NT_local[v_i0, v_i1])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                                    T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(256)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(256) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_relu[v0, v1])
                                T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 2, 4, 256, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 8, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                        for k_0_fused in T.serial(T.int64(16), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_fused in range(T.int64(256)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(256) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(524288)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + ax0_ax1_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(256) + ax0_ax1_fused % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(256), T.int64(32), T.int64(1), T.int64(1)):
                                with T.block("T_matmul_NT"):
                                    v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                    v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(256) + i1_3 + i1_4)
                                    v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(256) + k_1 * T.int64(32) + k_2)
                                    T.reads(p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                    T.writes(T_matmul_NT_local[v_i0, v_i1])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                                    T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(256)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(256) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_relu[v0, v1])
                                T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 2, 4, 256, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 8, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                        for k_0_fused in T.serial(T.int64(16), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_fused in range(T.int64(256)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(256) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(524288)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + ax0_ax1_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(256) + ax0_ax1_fused % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(256), T.int64(32), T.int64(1), T.int64(1)):
                                with T.block("T_matmul_NT"):
                                    v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                    v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(256) + i1_3 + i1_4)
                                    v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(256) + k_1 * T.int64(32) + k_2)
                                    T.reads(p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                    T.writes(T_matmul_NT_local[v_i0, v_i1])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                                    T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(256)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(256) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_relu[v0, v1])
                                T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 2, 4, 256, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 8, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
2024-03-20 19:48:43 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 19:48:43 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-03-20 19:48:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 504 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1007 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1513 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2018 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2524 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3027 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3532 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:46 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-20 19:48:47 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:47 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 107 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:48 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:49 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 115 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:48:49 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9981  0.9969  0.9965  0.9958  0.9928  0.9924  0.9924  0.9896  0.9891  0.9871  0.9861  0.9822  0.9821  0.9820  0.9813  0.9809
[17 : 32]:	0.9791  0.9790  0.9789  0.9776  0.9751  0.9750  0.9749  0.9748  0.9748  0.9747  0.9718  0.9716  0.9707  0.9704  0.9697  0.9684
[33 : 48]:	0.9673  0.9672  0.9671  0.9657  0.9656  0.9650  0.9648  0.9640  0.9618  0.9617  0.9615  0.9610  0.9608  0.9607  0.9597  0.9594
[49 : 64]:	0.9591  0.9590  0.9561  0.9558  0.9553  0.9534  0.9533  0.9522  0.9517  0.9514  0.9499  0.9490  0.9472  0.9443  0.9443  0.9428
2024-03-20 19:48:49 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 19:48:49 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(16) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(8), T.int64(16), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(16) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(16)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(16) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 32, 8, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 1, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 8, 64, 4, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #3: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 < T.int64(8))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 128, 1, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 8, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #4: GFLOPs: 18.2193. Time: 1842.1494 us. Best GFLOPs: 18.2193
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #5: GFLOPs: 8.6318. Time: 3888.2437 us. Best GFLOPs: 18.2193
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #6: GFLOPs: 1.8033. Time: 18612.0528 us. Best GFLOPs: 18.2193
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #7: GFLOPs: 112.2466. Time: 299.0080 us. Best GFLOPs: 112.2466
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #8: GFLOPs: 61.7187. Time: 543.7996 us. Best GFLOPs: 112.2466
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #9: GFLOPs: 13.8083. Time: 2430.6103 us. Best GFLOPs: 112.2466
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #10: GFLOPs: 31.6072. Time: 1061.8664 us. Best GFLOPs: 112.2466
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #11: GFLOPs: 23.1930. Time: 1447.1008 us. Best GFLOPs: 112.2466
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #12: GFLOPs: 2.8780. Time: 11661.8592 us. Best GFLOPs: 112.2466
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #13: GFLOPs: 33.8637. Time: 991.1094 us. Best GFLOPs: 112.2466
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #14: GFLOPs: 94.3332. Time: 355.7881 us. Best GFLOPs: 112.2466
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #15: GFLOPs: 140.5230. Time: 238.8408 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #16: GFLOPs: 50.4998. Time: 664.6093 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #17: GFLOPs: 29.4803. Time: 1138.4778 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #18: GFLOPs: 34.8271. Time: 963.6923 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #19: GFLOPs: 27.6276. Time: 1214.8218 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #20: GFLOPs: 34.8671. Time: 962.5871 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #21: GFLOPs: 53.1059. Time: 631.9948 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #22: GFLOPs: 26.6607. Time: 1258.8800 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #23: GFLOPs: 35.9576. Time: 933.3940 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #24: GFLOPs: 27.6400. Time: 1214.2789 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #25: GFLOPs: 7.6155. Time: 4407.1177 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #26: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(32), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 32, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #27: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(16) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(8), T.int64(16), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(16) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(16)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(16) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 32, 8, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 1, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #28: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 32, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 2, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #29: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(16))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 1, 64, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 4, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b73)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b75)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #30: GFLOPs: 1.2187. Time: 27540.7350 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #31: GFLOPs: 3.3293. Time: 10080.9631 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #32: GFLOPs: 135.7284. Time: 247.2779 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #33: GFLOPs: 38.1701. Time: 879.2901 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #34: GFLOPs: 39.6949. Time: 845.5142 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #35: GFLOPs: 6.5649. Time: 5112.4226 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #36: GFLOPs: 12.0462. Time: 2786.1618 us. Best GFLOPs: 140.5230
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #37: GFLOPs: 190.6054. Time: 176.0844 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #38: GFLOPs: 24.0936. Time: 1393.0098 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #39: GFLOPs: 2.7462. Time: 12221.3261 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #40: GFLOPs: 22.3978. Time: 1498.4788 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #41: GFLOPs: 11.0302. Time: 3042.8004 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #42: GFLOPs: 7.1783. Time: 4675.5375 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #43: GFLOPs: 23.5170. Time: 1427.1667 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #44: GFLOPs: 67.1767. Time: 499.6170 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #45: GFLOPs: 51.9414. Time: 646.1638 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #46: GFLOPs: 104.7375. Time: 320.4451 us. Best GFLOPs: 190.6054
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #47: GFLOPs: 357.4986. Time: 93.8818 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #48: GFLOPs: 27.9400. Time: 1201.2374 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #49: GFLOPs: 20.0530. Time: 1673.6938 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #50: GFLOPs: 9.9124. Time: 3385.9243 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #51: GFLOPs: 11.9485. Time: 2808.9396 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #52: GFLOPs: 6.0119. Time: 5582.7344 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #53: GFLOPs: 2.0244. Time: 16579.4371 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #54: GFLOPs: 129.5981. Time: 258.9746 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #55: GFLOPs: 62.5765. Time: 536.3453 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #56: GFLOPs: 5.0007. Time: 6711.5692 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #57: GFLOPs: 2.1511. Time: 15602.6883 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #58: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(32), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 32, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #59: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  312: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  311: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  310: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  309: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  308: tvm::transform::Pass::operator()(tvm::IRModule) const
  307: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  306: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  305: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  304: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  303: _ZN3tvm7runtime13PackedFuncObj
  302: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  301: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  300: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  299: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  298: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  297: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  296: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  295: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  294: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  269: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  268: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  267: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  266: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  265: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  264: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  263: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  262: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  261: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  260: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  259: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  258: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  257: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  256: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  255: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  254: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  253: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  252: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  251: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  250: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  249: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  248: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  247: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  246: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  245: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  244: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  243: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  242: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  241: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  240: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  239: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  238: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  237: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  236: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  235: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  234: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  233: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  232: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  231: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  230: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  229: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  228: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  227: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  226: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  225: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  224: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  223: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  222: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  221: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  220: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  219: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  218: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  217: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  216: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  215: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  214: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  213: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  212: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  211: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  210: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  209: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  208: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  207: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  206: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  205: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  204: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  202: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  201: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  200: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  199: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  198: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  197: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  196: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  195: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  194: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  193: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  192: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  191: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  190: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  189: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  188: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  183: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  182: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  181: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  180: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  179: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  178: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  176: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  175: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  173: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  172: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  171: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  170: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  169: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  168: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  167: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  166: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  165: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  164: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  163: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  162: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  161: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  160: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  159: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  158: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  157: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  156: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  154: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  153: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  152: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  151: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  150: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  149: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  148: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  147: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  146: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  145: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  144: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  143: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  142: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  141: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  140: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  139: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  138: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  137: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  136: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  135: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  134: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  133: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  132: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  131: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  130: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  117: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  116: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  115: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  114: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  113: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  112: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  111: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  110: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  109: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  108: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  107: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  106: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  105: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  104: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  82: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  81: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  80: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  79: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  78: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  77: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  76: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  75: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 32, 4, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #60: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(1024)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(4))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(4))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 64, 1, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 2, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #61: GFLOPs: 6.9016. Time: 4863.0442 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #62: GFLOPs: 21.1286. Time: 1588.4960 us. Best GFLOPs: 357.4986
2024-03-20 19:50:59 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #63: GFLOPs: 29.5029. Time: 1137.6058 us. Best GFLOPs: 357.4986
2024-03-20 19:54:22 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 19:54:22 [INFO] [evolutionary_search.cc:715] Picked top 53 candidate(s) from database
2024-03-20 19:54:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 455 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 907 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1362 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1817 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2270 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2720 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3176 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3629 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4086 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4541 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4995 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:26 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-03-20 19:54:27 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 134 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:28 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 108 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:30 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 19:54:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.5849  1.5849  1.5804  1.5739  1.5739  1.5258  1.5247  1.5247  1.5146  1.5146  1.5136  1.5136  1.4985  1.4985  1.4932  1.4874
[17 : 32]:	1.4874  1.4821  1.4810  1.4810  1.4795  1.4683  1.4683  1.4673  1.4673  1.4370  1.4335  1.4258  1.4073  1.4018  1.3957  1.3946
[33 : 48]:	1.3906  1.3906  1.3895  1.3873  1.3868  1.3846  1.3783  1.3762  1.3762  1.3751  1.3521  1.3410  1.3410  1.3399  1.3399  1.3300
[49 : 64]:	1.3270  1.3262  1.3213  1.3150  1.3150  1.2713  1.2713  1.2685  1.2674  1.2614  1.2602  1.2185  1.2053  1.1677  1.1653  1.1649
2024-03-20 19:54:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 19:54:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #64: GFLOPs: 141.5926. Time: 237.0366 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #65: GFLOPs: 141.5580. Time: 237.0946 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #66: GFLOPs: 141.5715. Time: 237.0719 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #67: GFLOPs: 141.5499. Time: 237.1080 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #68: GFLOPs: 141.5502. Time: 237.1075 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #69: GFLOPs: 123.0892. Time: 272.6691 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #70: GFLOPs: 138.5543. Time: 242.2344 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #71: GFLOPs: 138.5596. Time: 242.2252 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #72: GFLOPs: 122.7673. Time: 273.3840 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #73: GFLOPs: 122.7652. Time: 273.3887 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #74: GFLOPs: 138.5227. Time: 242.2898 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #75: GFLOPs: 138.5546. Time: 242.2339 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #76: GFLOPs: 140.8534. Time: 238.2805 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #77: GFLOPs: 140.8323. Time: 238.3163 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #78: GFLOPs: 141.4630. Time: 237.2537 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #79: GFLOPs: 140.8471. Time: 238.2912 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #80: GFLOPs: 140.8153. Time: 238.3451 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #81: GFLOPs: 141.4771. Time: 237.2301 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #82: GFLOPs: 141.4770. Time: 237.2302 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #83: GFLOPs: 141.4474. Time: 237.2799 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #84: GFLOPs: 141.7147. Time: 236.8324 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #85: GFLOPs: 141.7189. Time: 236.8253 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #86: GFLOPs: 141.6707. Time: 236.9058 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #87: GFLOPs: 141.6948. Time: 236.8655 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #88: GFLOPs: 141.6930. Time: 236.8686 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #89: GFLOPs: 119.1318. Time: 281.7269 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #90: GFLOPs: 356.7917. Time: 94.0678 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #91: GFLOPs: 119.1302. Time: 281.7307 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #92: GFLOPs: 138.5193. Time: 242.2957 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #93: GFLOPs: 115.5537. Time: 290.4504 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #94: GFLOPs: 140.8566. Time: 238.2751 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #95: GFLOPs: 140.8869. Time: 238.2238 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #96: GFLOPs: 115.5539. Time: 290.4500 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #97: GFLOPs: 115.5474. Time: 290.4663 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #98: GFLOPs: 115.5583. Time: 290.4389 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #99: GFLOPs: 140.4102. Time: 239.0326 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #100: GFLOPs: 349.8280. Time: 95.9404 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #101: GFLOPs: 349.8157. Time: 95.9437 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #102: GFLOPs: 356.8235. Time: 94.0594 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #103: GFLOPs: 140.4576. Time: 238.9521 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #104: GFLOPs: 140.4861. Time: 238.9035 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #105: GFLOPs: 140.4503. Time: 238.9644 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #106: GFLOPs: 115.5577. Time: 290.4405 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #107: GFLOPs: 115.5724. Time: 290.4035 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #108: GFLOPs: 115.5575. Time: 290.4408 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #109: GFLOPs: 115.5833. Time: 290.3761 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #110: GFLOPs: 115.5746. Time: 290.3979 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #111: GFLOPs: 347.5224. Time: 96.5769 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #112: GFLOPs: 349.5902. Time: 96.0056 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #113: GFLOPs: 347.1937. Time: 96.6683 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #114: GFLOPs: 349.1357. Time: 96.1306 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #115: GFLOPs: 356.2993. Time: 94.1978 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #116: GFLOPs: 349.5857. Time: 96.0069 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #117: GFLOPs: 348.3778. Time: 96.3397 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #118: GFLOPs: 348.3990. Time: 96.3339 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #119: GFLOPs: 346.6849. Time: 96.8102 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #120: GFLOPs: 348.4647. Time: 96.3157 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #121: GFLOPs: 346.2706. Time: 96.9260 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #122: GFLOPs: 356.6246. Time: 94.1119 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #123: GFLOPs: 355.3343. Time: 94.4536 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #124: GFLOPs: 271.0278. Time: 123.8346 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #125: GFLOPs: 69.6293. Time: 482.0185 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #126: GFLOPs: 45.1250. Time: 743.7706 us. Best GFLOPs: 357.4986
2024-03-20 19:55:27 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #127: GFLOPs: 133.7344. Time: 250.9648 us. Best GFLOPs: 357.4986
2024-03-20 20:01:31 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:01:31 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:01:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 812 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2023 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2430 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2833 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3236 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3639 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:34 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-03-20 20:01:35 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:36 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:38 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:39 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 182 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:01:39 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0054  1.0054  1.0054  1.0054  1.0054  1.0054  1.0054  1.0035  1.0035  1.0035  1.0035  1.0035  1.0035  1.0035  1.0035  1.0035
[17 : 32]:	1.0035  1.0035  1.0035  1.0035  1.0035  1.0012  1.0012  1.0012  1.0012  1.0012  1.0012  1.0012  0.9978  0.9978  0.9978  0.9978
[33 : 48]:	0.9978  0.9978  0.9978  0.9978  0.9978  0.9978  0.9978  0.9978  0.9959  0.9959  0.9959  0.9959  0.9959  0.9959  0.9959  0.9959
[49 : 64]:	0.9959  0.9959  0.9959  0.9959  0.9959  0.9942  0.9936  0.9936  0.9936  0.9936  0.9936  0.9936  0.9936  0.9936  0.9936  0.9936
2024-03-20 20:01:40 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:01:40 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #128: GFLOPs: 351.2220. Time: 95.5596 us. Best GFLOPs: 357.4986
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #129: GFLOPs: 363.7376. Time: 92.2715 us. Best GFLOPs: 363.7376
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #130: GFLOPs: 365.0881. Time: 91.9302 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #131: GFLOPs: 356.2681. Time: 94.2061 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #132: GFLOPs: 365.0797. Time: 91.9323 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #133: GFLOPs: 355.3551. Time: 94.4481 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #134: GFLOPs: 363.6328. Time: 92.2981 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #135: GFLOPs: 356.1692. Time: 94.2323 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #136: GFLOPs: 365.0741. Time: 91.9337 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #137: GFLOPs: 357.2355. Time: 93.9510 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #138: GFLOPs: 363.6507. Time: 92.2936 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #139: GFLOPs: 355.6688. Time: 94.3648 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #140: GFLOPs: 362.5545. Time: 92.5726 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #141: GFLOPs: 355.3634. Time: 94.4459 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #142: GFLOPs: 355.7644. Time: 94.3395 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #143: GFLOPs: 356.3293. Time: 94.1899 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #144: GFLOPs: 363.6392. Time: 92.2965 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #145: GFLOPs: 364.0195. Time: 92.2001 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #146: GFLOPs: 355.4012. Time: 94.4359 us. Best GFLOPs: 365.0881
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #147: GFLOPs: 365.0898. Time: 91.9298 us. Best GFLOPs: 365.0898
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #148: GFLOPs: 355.7851. Time: 94.3340 us. Best GFLOPs: 365.0898
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #149: GFLOPs: 357.2912. Time: 93.9363 us. Best GFLOPs: 365.0898
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #150: GFLOPs: 365.8776. Time: 91.7318 us. Best GFLOPs: 365.8776
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #151: GFLOPs: 365.0571. Time: 91.9380 us. Best GFLOPs: 365.8776
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #152: GFLOPs: 364.8722. Time: 91.9846 us. Best GFLOPs: 365.8776
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #153: GFLOPs: 355.6600. Time: 94.3672 us. Best GFLOPs: 365.8776
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #154: GFLOPs: 364.9039. Time: 91.9766 us. Best GFLOPs: 365.8776
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #155: GFLOPs: 356.2161. Time: 94.2198 us. Best GFLOPs: 365.8776
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #156: GFLOPs: 362.4887. Time: 92.5894 us. Best GFLOPs: 365.8776
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #157: GFLOPs: 362.5069. Time: 92.5848 us. Best GFLOPs: 365.8776
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #158: GFLOPs: 362.7146. Time: 92.5318 us. Best GFLOPs: 365.8776
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #159: GFLOPs: 371.2432. Time: 90.4060 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #160: GFLOPs: 362.5009. Time: 92.5863 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #161: GFLOPs: 362.6560. Time: 92.5467 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #162: GFLOPs: 361.6868. Time: 92.7947 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #163: GFLOPs: 369.9850. Time: 90.7135 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #164: GFLOPs: 361.3790. Time: 92.8738 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #165: GFLOPs: 370.6113. Time: 90.5602 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #166: GFLOPs: 361.9449. Time: 92.7285 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #167: GFLOPs: 361.4327. Time: 92.8600 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #168: GFLOPs: 370.0079. Time: 90.7079 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #169: GFLOPs: 371.0207. Time: 90.4603 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #170: GFLOPs: 370.6016. Time: 90.5625 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #171: GFLOPs: 361.4996. Time: 92.8428 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #172: GFLOPs: 361.7004. Time: 92.7912 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #173: GFLOPs: 369.9636. Time: 90.7187 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #174: GFLOPs: 361.7349. Time: 92.7824 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #175: GFLOPs: 361.4758. Time: 92.8489 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #176: GFLOPs: 361.5030. Time: 92.8419 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #177: GFLOPs: 370.5934. Time: 90.5646 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #178: GFLOPs: 361.7143. Time: 92.7877 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #179: GFLOPs: 370.1714. Time: 90.6678 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #180: GFLOPs: 361.4537. Time: 92.8546 us. Best GFLOPs: 371.2432
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #181: GFLOPs: 371.8341. Time: 90.2624 us. Best GFLOPs: 371.8341
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #182: GFLOPs: 370.9424. Time: 90.4794 us. Best GFLOPs: 371.8341
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #183: GFLOPs: 362.4724. Time: 92.5936 us. Best GFLOPs: 371.8341
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #184: GFLOPs: 370.9562. Time: 90.4760 us. Best GFLOPs: 371.8341
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #185: GFLOPs: 362.5362. Time: 92.5773 us. Best GFLOPs: 371.8341
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #186: GFLOPs: 370.9559. Time: 90.4760 us. Best GFLOPs: 371.8341
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #187: GFLOPs: 370.9278. Time: 90.4829 us. Best GFLOPs: 371.8341
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #188: GFLOPs: 362.5715. Time: 92.5683 us. Best GFLOPs: 371.8341
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #189: GFLOPs: 22.2127. Time: 1510.9655 us. Best GFLOPs: 371.8341
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #190: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 64, 2, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:02:35 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #191: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(32), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(4096)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0)
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    v1 = T.axis.spatial(T.int64(4096), k_0)
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 32, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 20:09:23 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:09:23 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:09:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 400 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 805 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1616 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2017 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2420 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2826 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3232 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3639 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:26 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-03-20 20:09:27 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:28 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 160 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 189 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:31 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 133 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:09:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2107  1.2107  1.2011  1.1028  1.1028  1.1028  1.1019  1.1019  1.1019  1.0457  1.0425  1.0422  1.0416  1.0416  1.0416  1.0414
[17 : 32]:	1.0166  1.0112  0.9836  0.9836  0.9822  0.9822  0.9822  0.9822  0.9822  0.9822  0.9822  0.9822  0.9822  0.9822  0.9822  0.9822
[33 : 48]:	0.9818  0.9818  0.9818  0.9818  0.9792  0.9778  0.9778  0.9778  0.9778  0.9778  0.9778  0.9778  0.9778  0.9778  0.9778  0.9778
[49 : 64]:	0.9778  0.9778  0.9778  0.9774  0.9774  0.9766  0.9751  0.9751  0.9748  0.9748  0.9748  0.9722  0.9722  0.9722  0.9722  0.9708
2024-03-20 20:09:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:09:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #192: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 1, 128, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #193: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 1, 128, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #194: GFLOPs: 213.5484. Time: 157.1663 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #195: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #196: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #197: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #198: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #199: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #200: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #201: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 1, 256, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 256], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 256, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #202: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #203: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #204: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #205: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #206: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #207: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #208: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #209: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #210: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #211: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #212: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(64) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 16, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #213: GFLOPs: 348.7540. Time: 96.2358 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #214: GFLOPs: 350.3242. Time: 95.8045 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #215: GFLOPs: 357.0551. Time: 93.9985 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #216: GFLOPs: 357.0515. Time: 93.9994 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #217: GFLOPs: 356.9707. Time: 94.0207 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #218: GFLOPs: 356.9869. Time: 94.0164 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #219: GFLOPs: 356.7348. Time: 94.0828 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #220: GFLOPs: 356.6784. Time: 94.0977 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #221: GFLOPs: 349.9396. Time: 95.9098 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #222: GFLOPs: 350.0403. Time: 95.8822 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #223: GFLOPs: 349.9485. Time: 95.9073 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #224: GFLOPs: 349.9432. Time: 95.9088 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #225: GFLOPs: 349.9938. Time: 95.8949 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #226: GFLOPs: 356.7549. Time: 94.0775 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #227: GFLOPs: 349.9513. Time: 95.9066 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #228: GFLOPs: 356.8800. Time: 94.0446 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #229: GFLOPs: 356.9321. Time: 94.0308 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #230: GFLOPs: 349.9476. Time: 95.9076 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #231: GFLOPs: 356.7655. Time: 94.0747 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #232: GFLOPs: 356.7145. Time: 94.0882 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #233: GFLOPs: 349.3221. Time: 96.0793 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #234: GFLOPs: 349.2423. Time: 96.1013 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #235: GFLOPs: 356.6801. Time: 94.0973 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #236: GFLOPs: 356.8114. Time: 94.0626 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #237: GFLOPs: 356.9030. Time: 94.0385 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #238: GFLOPs: 349.4639. Time: 96.0403 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #239: GFLOPs: 356.8891. Time: 94.0422 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #240: GFLOPs: 356.7544. Time: 94.0777 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #241: GFLOPs: 356.9097. Time: 94.0367 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #242: GFLOPs: 349.9002. Time: 95.9206 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #243: GFLOPs: 349.9158. Time: 95.9163 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #244: GFLOPs: 349.4013. Time: 96.0575 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #245: GFLOPs: 356.4016. Time: 94.1708 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #246: GFLOPs: 349.5405. Time: 96.0193 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #247: GFLOPs: 349.5911. Time: 96.0054 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #248: GFLOPs: 356.3854. Time: 94.1751 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #249: GFLOPs: 349.5684. Time: 96.0116 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #250: GFLOPs: 349.6284. Time: 95.9951 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #251: GFLOPs: 355.2841. Time: 94.4670 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #252: GFLOPs: 346.9676. Time: 96.7313 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #253: GFLOPs: 229.2523. Time: 146.4004 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #254: GFLOPs: 136.5555. Time: 245.7802 us. Best GFLOPs: 371.8341
2024-03-20 20:10:18 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #255: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  312: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  311: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  310: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  309: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  308: tvm::transform::Pass::operator()(tvm::IRModule) const
  307: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  306: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  305: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  304: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  303: _ZN3tvm7runtime13PackedFuncObj
  302: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  301: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  300: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  299: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  298: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  297: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  296: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  295: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  294: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  269: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  268: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  267: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  266: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  265: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  264: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  263: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  262: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  261: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  260: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  259: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  258: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  257: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  256: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  255: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  254: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  253: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  252: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  251: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  250: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  249: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  248: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  247: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  246: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  245: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  244: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  243: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  242: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  241: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  240: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  239: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  238: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  237: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  236: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  235: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  234: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  233: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  232: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  231: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  230: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  229: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  228: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  227: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  226: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  225: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  224: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  223: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  222: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  221: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  220: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  219: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  218: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  217: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  216: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  215: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  214: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  213: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  212: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  211: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  210: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  209: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  208: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  207: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  206: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  205: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  204: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  202: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  201: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  200: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  199: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  198: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  197: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  196: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  195: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  194: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  193: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  192: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  191: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  190: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  189: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  188: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  183: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  182: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  181: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  180: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  179: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  178: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  176: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  175: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  173: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  172: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  171: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  170: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  169: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  168: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  167: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  166: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  165: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  164: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  163: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  162: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  161: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  160: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  159: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  158: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  157: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  156: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  154: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  153: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  152: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  151: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  150: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  149: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  148: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  147: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  146: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  145: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  144: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  143: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  142: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  141: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  140: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  139: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  138: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  137: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  136: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  135: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  134: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  133: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  132: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  131: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  130: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  117: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  116: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  115: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  114: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  113: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  112: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  111: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  110: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  109: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  108: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  107: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  106: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  105: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  104: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  82: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  81: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  80: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  79: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  78: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  77: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  76: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  75: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 64, 2, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-03-20 20:15:43 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:15:43 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:15:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 811 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1212 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1619 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2025 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2428 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2831 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3238 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3641 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4045 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:46 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-03-20 20:15:47 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:48 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 134 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:50 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:51 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:15:51 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0604  1.0206  1.0206  1.0075  1.0046  0.9725  0.9702  0.9695  0.9694  0.9651  0.9651  0.9649  0.9647  0.9645  0.9645  0.9632
[17 : 32]:	0.9632  0.9632  0.9632  0.9632  0.9632  0.9630  0.9626  0.9626  0.9626  0.9626  0.9626  0.9624  0.9624  0.9624  0.9624  0.9624
[33 : 48]:	0.9624  0.9609  0.9581  0.9581  0.9581  0.9581  0.9581  0.9581  0.9581  0.9577  0.9575  0.9575  0.9575  0.9575  0.9575  0.9575
[49 : 64]:	0.9575  0.9575  0.9575  0.9571  0.9561  0.9561  0.9561  0.9561  0.9561  0.9561  0.9561  0.9561  0.9551  0.9551  0.9509  0.9503
2024-03-20 20:15:52 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:15:52 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #256: GFLOPs: 211.0442. Time: 159.0312 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #257: GFLOPs: 356.9207. Time: 94.0338 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #258: GFLOPs: 356.4181. Time: 94.1664 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #259: GFLOPs: 357.7655. Time: 93.8118 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #260: GFLOPs: 357.4098. Time: 93.9052 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #261: GFLOPs: 349.2131. Time: 96.1093 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #262: GFLOPs: 356.3198. Time: 94.1924 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #263: GFLOPs: 356.5722. Time: 94.1257 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #264: GFLOPs: 349.5294. Time: 96.0223 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #265: GFLOPs: 356.3604. Time: 94.1817 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #266: GFLOPs: 356.7960. Time: 94.0667 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #267: GFLOPs: 355.2297. Time: 94.4815 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #268: GFLOPs: 347.0933. Time: 96.6963 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #269: GFLOPs: 356.7640. Time: 94.0751 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #270: GFLOPs: 356.8503. Time: 94.0524 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #271: GFLOPs: 346.5625. Time: 96.8444 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #272: GFLOPs: 354.0377. Time: 94.7996 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #273: GFLOPs: 353.9304. Time: 94.8283 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #274: GFLOPs: 354.6463. Time: 94.6369 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #275: GFLOPs: 346.5583. Time: 96.8455 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #276: GFLOPs: 353.9293. Time: 94.8286 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #277: GFLOPs: 346.5054. Time: 96.8603 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #278: GFLOPs: 346.8591. Time: 96.7616 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #279: GFLOPs: 355.1436. Time: 94.5044 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #280: GFLOPs: 346.8052. Time: 96.7766 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #281: GFLOPs: 346.8591. Time: 96.7615 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #282: GFLOPs: 354.7096. Time: 94.6200 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #283: GFLOPs: 346.8459. Time: 96.7652 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #284: GFLOPs: 354.6195. Time: 94.6440 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #285: GFLOPs: 355.2252. Time: 94.4827 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #286: GFLOPs: 354.8364. Time: 94.5862 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #287: GFLOPs: 346.9273. Time: 96.7425 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #288: GFLOPs: 346.8218. Time: 96.7720 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #289: GFLOPs: 355.2754. Time: 94.4693 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #290: GFLOPs: 346.5550. Time: 96.8465 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #291: GFLOPs: 354.6780. Time: 94.6284 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #292: GFLOPs: 346.5320. Time: 96.8529 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #293: GFLOPs: 345.7249. Time: 97.0790 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #294: GFLOPs: 353.9664. Time: 94.8187 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #295: GFLOPs: 346.5636. Time: 96.8440 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #296: GFLOPs: 353.9906. Time: 94.8122 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #297: GFLOPs: 356.2950. Time: 94.1990 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #298: GFLOPs: 346.8338. Time: 96.7686 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #299: GFLOPs: 355.1486. Time: 94.5030 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #300: GFLOPs: 355.2151. Time: 94.4854 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #301: GFLOPs: 346.8382. Time: 96.7674 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #302: GFLOPs: 346.8579. Time: 96.7619 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #303: GFLOPs: 346.8349. Time: 96.7683 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #304: GFLOPs: 354.6771. Time: 94.6287 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #305: GFLOPs: 346.8117. Time: 96.7748 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #306: GFLOPs: 355.0207. Time: 94.5371 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #307: GFLOPs: 356.5439. Time: 94.1332 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #308: GFLOPs: 355.0478. Time: 94.5299 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #309: GFLOPs: 354.7554. Time: 94.6078 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #310: GFLOPs: 346.9427. Time: 96.7382 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #311: GFLOPs: 354.9039. Time: 94.5682 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #312: GFLOPs: 354.8922. Time: 94.5713 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #313: GFLOPs: 346.9244. Time: 96.7433 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #314: GFLOPs: 345.2684. Time: 97.2073 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #315: GFLOPs: 345.3813. Time: 97.1756 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #316: GFLOPs: 348.2083. Time: 96.3866 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #317: GFLOPs: 22.1766. Time: 1513.4261 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #318: GFLOPs: 44.7067. Time: 750.7296 us. Best GFLOPs: 371.8341
2024-03-20 20:16:49 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #319: GFLOPs: 58.5170. Time: 573.5530 us. Best GFLOPs: 371.8341
2024-03-20 20:24:33 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:24:33 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:24:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 811 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1624 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2029 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2431 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2838 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3244 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3654 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4060 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4464 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4870 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:36 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 20:24:37 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:38 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:39 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:24:41 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9959  0.9875  0.9821  0.9821  0.9821  0.9816  0.9816  0.9811  0.9811  0.9810  0.9801  0.9796  0.9795  0.9795  0.9789  0.9783
[17 : 32]:	0.9783  0.9782  0.9776  0.9776  0.9770  0.9769  0.9765  0.9764  0.9753  0.9749  0.9739  0.9733  0.9713  0.9713  0.9707  0.9696
[33 : 48]:	0.9696  0.9693  0.9673  0.9644  0.9639  0.9639  0.9639  0.9634  0.9633  0.9629  0.9629  0.9629  0.9629  0.9627  0.9627  0.9627
[49 : 64]:	0.9624  0.9618  0.9618  0.9618  0.9616  0.9604  0.9594  0.9585  0.9585  0.9579  0.9579  0.9566  0.9566  0.9555  0.9551  0.9531
2024-03-20 20:24:41 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:24:41 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #320: GFLOPs: 319.4958. Time: 105.0487 us. Best GFLOPs: 371.8341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #321: GFLOPs: 378.8418. Time: 88.5927 us. Best GFLOPs: 378.8418
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #322: GFLOPs: 388.2239. Time: 86.4517 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #323: GFLOPs: 388.2027. Time: 86.4564 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #324: GFLOPs: 388.0452. Time: 86.4915 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #325: GFLOPs: 388.0915. Time: 86.4812 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #326: GFLOPs: 388.2203. Time: 86.4525 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #327: GFLOPs: 388.0643. Time: 86.4873 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #328: GFLOPs: 388.2042. Time: 86.4561 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #329: GFLOPs: 388.0706. Time: 86.4859 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #330: GFLOPs: 388.0706. Time: 86.4859 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #331: GFLOPs: 388.0979. Time: 86.4798 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #332: GFLOPs: 388.2024. Time: 86.4565 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #333: GFLOPs: 388.1292. Time: 86.4728 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #334: GFLOPs: 388.2175. Time: 86.4531 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #335: GFLOPs: 378.7613. Time: 88.6115 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #336: GFLOPs: 378.7740. Time: 88.6086 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #337: GFLOPs: 378.7636. Time: 88.6110 us. Best GFLOPs: 388.2239
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #338: GFLOPs: 388.2341. Time: 86.4494 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #339: GFLOPs: 388.1046. Time: 86.4783 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #340: GFLOPs: 378.7918. Time: 88.6044 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #341: GFLOPs: 378.8204. Time: 88.5977 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #342: GFLOPs: 378.8478. Time: 88.5913 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #343: GFLOPs: 377.3329. Time: 88.9470 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #344: GFLOPs: 377.2996. Time: 88.9548 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #345: GFLOPs: 387.9496. Time: 86.5129 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #346: GFLOPs: 387.8271. Time: 86.5402 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #347: GFLOPs: 345.0476. Time: 97.2696 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #348: GFLOPs: 377.1295. Time: 88.9950 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #349: GFLOPs: 387.7175. Time: 86.5646 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #350: GFLOPs: 387.6513. Time: 86.5794 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #351: GFLOPs: 341.7646. Time: 98.2039 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #352: GFLOPs: 377.1275. Time: 88.9954 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #353: GFLOPs: 387.6908. Time: 86.5706 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #354: GFLOPs: 377.2714. Time: 88.9615 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #355: GFLOPs: 375.8180. Time: 89.3055 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #356: GFLOPs: 375.7855. Time: 89.3132 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #357: GFLOPs: 366.1317. Time: 91.6682 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #358: GFLOPs: 375.8706. Time: 89.2930 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #359: GFLOPs: 376.5633. Time: 89.1288 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #360: GFLOPs: 375.8807. Time: 89.2906 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #361: GFLOPs: 376.5451. Time: 89.1331 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #362: GFLOPs: 376.5652. Time: 89.1283 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #363: GFLOPs: 366.6247. Time: 91.5449 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #364: GFLOPs: 377.2570. Time: 88.9649 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #365: GFLOPs: 375.9150. Time: 89.2825 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #366: GFLOPs: 366.1446. Time: 91.6649 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #367: GFLOPs: 366.1836. Time: 91.6552 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #368: GFLOPs: 376.5732. Time: 89.1264 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #369: GFLOPs: 376.5665. Time: 89.1280 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #370: GFLOPs: 376.5932. Time: 89.1217 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #371: GFLOPs: 366.5695. Time: 91.5587 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #372: GFLOPs: 375.8801. Time: 89.2908 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #373: GFLOPs: 375.8682. Time: 89.2936 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #374: GFLOPs: 384.7177. Time: 87.2396 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #375: GFLOPs: 376.1622. Time: 89.2238 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #376: GFLOPs: 366.1078. Time: 91.6741 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #377: GFLOPs: 375.8399. Time: 89.3003 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #378: GFLOPs: 375.8654. Time: 89.2943 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #379: GFLOPs: 377.2192. Time: 88.9738 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #380: GFLOPs: 375.8612. Time: 89.2953 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #381: GFLOPs: 19.0296. Time: 1763.7031 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #382: GFLOPs: 8.3879. Time: 4001.3003 us. Best GFLOPs: 388.2341
2024-03-20 20:25:37 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #383: GFLOPs: 45.4531. Time: 738.4019 us. Best GFLOPs: 388.2341
2024-03-20 20:34:04 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:34:04 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:34:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1615 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2021 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2428 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2833 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3237 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3640 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:06 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 20:34:07 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:08 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:10 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:11 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 134 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:34:12 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3024  1.2958  1.2958  1.2958  1.2953  1.2953  1.2953  1.2953  1.2945  1.2945  1.2945  1.2929  1.1558  1.1558  1.1558  1.1552
[17 : 32]:	1.1522  1.1487  1.1487  1.1487  1.1487  1.1481  1.1481  1.1481  1.1481  1.1474  1.1371  1.1358  1.1308  1.1306  1.1187  1.1187
[33 : 48]:	1.1182  1.1174  1.1123  1.1123  1.1047  1.0965  1.0942  1.0942  1.0906  1.0894  1.0894  1.0894  1.0862  1.0812  1.0607  1.0069
[49 : 64]:	1.0006  0.9980  0.9953  0.9941  0.9941  0.9939  0.9938  0.9938  0.9938  0.9938  0.9936  0.9936  0.9935  0.9933  0.9933  0.9933
2024-03-20 20:34:12 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:34:12 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #384: GFLOPs: 320.3398. Time: 104.7720 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #385: GFLOPs: 321.9284. Time: 104.2549 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #386: GFLOPs: 321.9175. Time: 104.2585 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #387: GFLOPs: 326.1663. Time: 102.9003 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #388: GFLOPs: 321.8643. Time: 104.2757 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #389: GFLOPs: 326.1696. Time: 102.8993 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #390: GFLOPs: 326.1252. Time: 102.9133 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #391: GFLOPs: 322.0014. Time: 104.2313 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #392: GFLOPs: 321.9975. Time: 104.2326 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #393: GFLOPs: 325.4708. Time: 103.1202 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #394: GFLOPs: 326.1766. Time: 102.8971 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #395: GFLOPs: 304.4049. Time: 110.2565 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #396: GFLOPs: 312.3237. Time: 107.4610 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #397: GFLOPs: 312.2807. Time: 107.4758 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #398: GFLOPs: 312.2717. Time: 107.4789 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #399: GFLOPs: 312.2946. Time: 107.4710 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #400: GFLOPs: 306.4813. Time: 109.5095 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #401: GFLOPs: 306.5375. Time: 109.4894 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #402: GFLOPs: 305.9712. Time: 109.6921 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #403: GFLOPs: 305.8307. Time: 109.7425 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #404: GFLOPs: 306.5687. Time: 109.4783 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #405: GFLOPs: 306.1386. Time: 109.6321 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #406: GFLOPs: 306.5777. Time: 109.4751 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #407: GFLOPs: 306.6487. Time: 109.4497 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #408: GFLOPs: 305.6450. Time: 109.8092 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #409: GFLOPs: 306.6797. Time: 109.4387 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #410: GFLOPs: 309.3454. Time: 108.4956 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #411: GFLOPs: 309.0707. Time: 108.5921 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #412: GFLOPs: 203.3518. Time: 165.0471 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #413: GFLOPs: 309.5443. Time: 108.4259 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #414: GFLOPs: 310.7466. Time: 108.0064 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #415: GFLOPs: 312.0737. Time: 107.5471 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #416: GFLOPs: 310.7660. Time: 107.9997 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #417: GFLOPs: 310.8067. Time: 107.9855 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #418: GFLOPs: 310.7155. Time: 108.0172 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #419: GFLOPs: 312.1168. Time: 107.5322 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #420: GFLOPs: 303.7248. Time: 110.5034 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #421: GFLOPs: 353.3688. Time: 94.9790 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #422: GFLOPs: 354.9755. Time: 94.5491 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #423: GFLOPs: 355.0429. Time: 94.5312 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #424: GFLOPs: 355.0056. Time: 94.5411 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #425: GFLOPs: 355.3586. Time: 94.4472 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #426: GFLOPs: 358.5644. Time: 93.6028 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #427: GFLOPs: 355.0929. Time: 94.5179 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #428: GFLOPs: 302.2737. Time: 111.0339 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #429: GFLOPs: 303.3604. Time: 110.6361 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #430: GFLOPs: 352.6834. Time: 95.1636 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #431: GFLOPs: 377.0714. Time: 89.0087 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #432: GFLOPs: 375.5496. Time: 89.3693 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #433: GFLOPs: 312.1484. Time: 107.5214 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #434: GFLOPs: 375.4396. Time: 89.3955 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #435: GFLOPs: 387.0162. Time: 86.7215 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #436: GFLOPs: 387.2556. Time: 86.6679 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #437: GFLOPs: 387.2829. Time: 86.6618 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #438: GFLOPs: 387.4295. Time: 86.6290 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #439: GFLOPs: 387.3347. Time: 86.6502 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #440: GFLOPs: 387.4428. Time: 86.6260 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #441: GFLOPs: 387.4929. Time: 86.6148 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #442: GFLOPs: 387.3518. Time: 86.6464 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #443: GFLOPs: 387.4781. Time: 86.6181 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #444: GFLOPs: 387.2842. Time: 86.6615 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #445: GFLOPs: 52.9549. Time: 633.7961 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #446: GFLOPs: 14.0120. Time: 2395.2822 us. Best GFLOPs: 388.2341
2024-03-20 20:35:09 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #447: GFLOPs: 12.4408. Time: 2697.7818 us. Best GFLOPs: 388.2341
2024-03-20 20:38:39 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:38:39 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:38:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1615 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2023 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2427 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2830 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3237 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3644 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4049 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:42 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-03-20 20:38:42 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:44 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:45 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:38:47 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2216  1.2161  1.1626  1.0983  0.9989  0.9989  0.9989  0.9989  0.9989  0.9973  0.9973  0.9969  0.9969  0.9969  0.9969  0.9969
[17 : 32]:	0.9963  0.9940  0.9940  0.9940  0.9940  0.9940  0.9929  0.9929  0.9926  0.9926  0.9926  0.9903  0.9903  0.9895  0.9878  0.9878
[33 : 48]:	0.9878  0.9878  0.9878  0.9878  0.9878  0.9878  0.9878  0.9878  0.9866  0.9863  0.9863  0.9863  0.9863  0.9863  0.9863  0.9853
[49 : 64]:	0.9853  0.9853  0.9853  0.9853  0.9851  0.9851  0.9851  0.9851  0.9841  0.9833  0.9824  0.9823  0.9818  0.9818  0.9818  0.9818
2024-03-20 20:38:47 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:38:47 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #448: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #449: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #450: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #451: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #452: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 64, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #453: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #454: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #455: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #456: GFLOPs: 384.0186. Time: 87.3984 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #457: GFLOPs: 384.8346. Time: 87.2131 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #458: GFLOPs: 384.5443. Time: 87.2790 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #459: GFLOPs: 382.4738. Time: 87.7514 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #460: GFLOPs: 383.0014. Time: 87.6306 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #461: GFLOPs: 382.8596. Time: 87.6630 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #462: GFLOPs: 382.8431. Time: 87.6668 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #463: GFLOPs: 382.5056. Time: 87.7441 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #464: GFLOPs: 384.5845. Time: 87.2698 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #465: GFLOPs: 378.8809. Time: 88.5836 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #466: GFLOPs: 379.4430. Time: 88.4523 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #467: GFLOPs: 379.6201. Time: 88.4111 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #468: GFLOPs: 379.3873. Time: 88.4653 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #469: GFLOPs: 379.5928. Time: 88.4174 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #470: GFLOPs: 381.6223. Time: 87.9472 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #471: GFLOPs: 381.6145. Time: 87.9490 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #472: GFLOPs: 378.9457. Time: 88.5684 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #473: GFLOPs: 379.1766. Time: 88.5145 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #474: GFLOPs: 379.3519. Time: 88.4736 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #475: GFLOPs: 381.6450. Time: 87.9420 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #476: GFLOPs: 381.1695. Time: 88.0517 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #477: GFLOPs: 356.4762. Time: 94.1511 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #478: GFLOPs: 348.5641. Time: 96.2882 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #479: GFLOPs: 348.7406. Time: 96.2395 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #480: GFLOPs: 349.0409. Time: 96.1567 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #481: GFLOPs: 348.7191. Time: 96.2454 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #482: GFLOPs: 366.4348. Time: 91.5923 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #483: GFLOPs: 366.4885. Time: 91.5789 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #484: GFLOPs: 348.9274. Time: 96.1880 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #485: GFLOPs: 366.4403. Time: 91.5910 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #486: GFLOPs: 348.3452. Time: 96.3488 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #487: GFLOPs: 366.4852. Time: 91.5798 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #488: GFLOPs: 356.1950. Time: 94.2254 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #489: GFLOPs: 348.2121. Time: 96.3856 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #490: GFLOPs: 348.5929. Time: 96.2803 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #491: GFLOPs: 348.6521. Time: 96.2639 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #492: GFLOPs: 348.4765. Time: 96.3124 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #493: GFLOPs: 349.0025. Time: 96.1673 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #494: GFLOPs: 348.5764. Time: 96.2848 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #495: GFLOPs: 366.4364. Time: 91.5920 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #496: GFLOPs: 365.9660. Time: 91.7097 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #497: GFLOPs: 365.9355. Time: 91.7173 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #498: GFLOPs: 365.8900. Time: 91.7287 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #499: GFLOPs: 366.1207. Time: 91.6709 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #500: GFLOPs: 350.9159. Time: 95.6429 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #501: GFLOPs: 350.7417. Time: 95.6904 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #502: GFLOPs: 345.9728. Time: 97.0094 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #503: GFLOPs: 350.7354. Time: 95.6922 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #504: GFLOPs: 356.2653. Time: 94.2068 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #505: GFLOPs: 356.5125. Time: 94.1415 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #506: GFLOPs: 356.2768. Time: 94.2038 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #507: GFLOPs: 363.7636. Time: 92.2649 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #508: GFLOPs: 366.4805. Time: 91.5809 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #509: GFLOPs: 58.4216. Time: 574.4895 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #510: GFLOPs: 3.3140. Time: 10127.5650 us. Best GFLOPs: 388.2341
2024-03-20 20:39:39 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #511: GFLOPs: 42.9891. Time: 780.7233 us. Best GFLOPs: 388.2341
2024-03-20 20:45:20 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:45:20 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:45:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 811 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1220 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1625 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2030 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2432 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2831 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3234 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3641 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4047 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:23 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-20 20:45:24 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 171 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:25 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 162 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 183 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:27 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:45:28 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9860  0.9852  0.9852  0.9829  0.9826  0.9812  0.9812  0.9730  0.9727  0.9724  0.9722  0.9685  0.9682  0.9682  0.9681  0.9677
[17 : 32]:	0.9674  0.9670  0.9670  0.9662  0.9660  0.9657  0.9653  0.9652  0.9645  0.9634  0.9634  0.9626  0.9623  0.9623  0.9620  0.9619
[33 : 48]:	0.9611  0.9601  0.9576  0.9575  0.9551  0.9551  0.9547  0.9538  0.9531  0.9524  0.9520  0.9507  0.9499  0.9490  0.9479  0.9471
[49 : 64]:	0.9453  0.9445  0.9441  0.9425  0.9425  0.9416  0.9394  0.9394  0.9383  0.9381  0.9377  0.9335  0.9335  0.9331  0.9331  0.9328
2024-03-20 20:45:28 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:45:28 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #512: GFLOPs: 383.6815. Time: 87.4752 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #513: GFLOPs: 386.1568. Time: 86.9145 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #514: GFLOPs: 385.6148. Time: 87.0367 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #515: GFLOPs: 385.6541. Time: 87.0278 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #516: GFLOPs: 385.5241. Time: 87.0571 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #517: GFLOPs: 385.9438. Time: 86.9625 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #518: GFLOPs: 386.2323. Time: 86.8975 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #519: GFLOPs: 378.9323. Time: 88.5716 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #520: GFLOPs: 378.7400. Time: 88.6165 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #521: GFLOPs: 273.5135. Time: 122.7092 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #522: GFLOPs: 377.4674. Time: 88.9153 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #523: GFLOPs: 378.6978. Time: 88.6264 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #524: GFLOPs: 378.6894. Time: 88.6284 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #525: GFLOPs: 378.6973. Time: 88.6265 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #526: GFLOPs: 273.4892. Time: 122.7201 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #527: GFLOPs: 378.6693. Time: 88.6331 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #528: GFLOPs: 378.6805. Time: 88.6305 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #529: GFLOPs: 378.9014. Time: 88.5788 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #530: GFLOPs: 378.9150. Time: 88.5756 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #531: GFLOPs: 378.9014. Time: 88.5788 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #532: GFLOPs: 377.2936. Time: 88.9562 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #533: GFLOPs: 273.5182. Time: 122.7071 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #534: GFLOPs: 378.8634. Time: 88.5877 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #535: GFLOPs: 377.2973. Time: 88.9554 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #536: GFLOPs: 377.3466. Time: 88.9437 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #537: GFLOPs: 374.6850. Time: 89.5756 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #538: GFLOPs: 374.7203. Time: 89.5671 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #539: GFLOPs: 374.7705. Time: 89.5551 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #540: GFLOPs: 378.6764. Time: 88.6314 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #541: GFLOPs: 378.6675. Time: 88.6335 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #542: GFLOPs: 375.0937. Time: 89.4780 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #543: GFLOPs: 378.8668. Time: 88.5869 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #544: GFLOPs: 378.8585. Time: 88.5888 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #545: GFLOPs: 378.6751. Time: 88.6317 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #546: GFLOPs: 377.3236. Time: 88.9492 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #547: GFLOPs: 374.7595. Time: 89.5578 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #548: GFLOPs: 375.1337. Time: 89.4684 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #549: GFLOPs: 373.4518. Time: 89.8714 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #550: GFLOPs: 374.7461. Time: 89.5610 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #551: GFLOPs: 375.1358. Time: 89.4679 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #552: GFLOPs: 273.4586. Time: 122.7338 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #553: GFLOPs: 274.9724. Time: 122.0582 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #554: GFLOPs: 272.2484. Time: 123.2794 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #555: GFLOPs: 373.6210. Time: 89.8307 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #556: GFLOPs: 371.7255. Time: 90.2887 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #557: GFLOPs: 377.2899. Time: 88.9571 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #558: GFLOPs: 371.5713. Time: 90.3262 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #559: GFLOPs: 373.4981. Time: 89.8602 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #560: GFLOPs: 373.6632. Time: 89.8205 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #561: GFLOPs: 375.7596. Time: 89.3194 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #562: GFLOPs: 377.3279. Time: 88.9482 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #563: GFLOPs: 377.2925. Time: 88.9565 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #564: GFLOPs: 376.0109. Time: 89.2597 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #565: GFLOPs: 373.6849. Time: 89.8153 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #566: GFLOPs: 373.6860. Time: 89.8150 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #567: GFLOPs: 373.7047. Time: 89.8106 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #568: GFLOPs: 376.4566. Time: 89.1540 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #569: GFLOPs: 374.7043. Time: 89.5709 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #570: GFLOPs: 374.7260. Time: 89.5658 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #571: GFLOPs: 371.6164. Time: 90.3152 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #572: GFLOPs: 371.0968. Time: 90.4417 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #573: GFLOPs: 12.4172. Time: 2702.9088 us. Best GFLOPs: 388.2341
2024-03-20 20:46:22 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #574: GFLOPs: 72.2580. Time: 464.4829 us. Best GFLOPs: 388.2341
2024-03-20 20:56:25 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:56:25 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:56:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1212 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1619 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2024 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2427 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2831 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3238 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3644 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4052 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4456 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:29 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-03-20 20:56:29 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 158 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:31 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 147 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:33 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 20:56:34 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1847  1.1847  1.1134  1.1111  1.1111  1.1111  1.0727  1.0600  1.0600  1.0569  1.0204  1.0048  1.0008  0.9973  0.9911  0.9757
[17 : 32]:	0.9739  0.9739  0.9735  0.9735  0.9735  0.9735  0.9730  0.9728  0.9727  0.9727  0.9727  0.9724  0.9724  0.9724  0.9720  0.9720
[33 : 48]:	0.9720  0.9706  0.9706  0.9706  0.9706  0.9702  0.9702  0.9702  0.9700  0.9700  0.9699  0.9699  0.9699  0.9661  0.9659  0.9657
[49 : 64]:	0.9657  0.9656  0.9655  0.9655  0.9655  0.9652  0.9647  0.9643  0.9643  0.9643  0.9643  0.9643  0.9643  0.9638  0.9638  0.9629
2024-03-20 20:56:34 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:56:34 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #575: GFLOPs: 197.0447. Time: 170.3300 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #576: GFLOPs: 204.3817. Time: 164.2154 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #577: GFLOPs: 196.8678. Time: 170.4830 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #578: GFLOPs: 204.2049. Time: 164.3576 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #579: GFLOPs: 204.3031. Time: 164.2786 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #580: GFLOPs: 204.3200. Time: 164.2650 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #581: GFLOPs: 199.2723. Time: 168.4259 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #582: GFLOPs: 206.2268. Time: 162.7461 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #583: GFLOPs: 206.2379. Time: 162.7374 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #584: GFLOPs: 198.6602. Time: 168.9448 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #585: GFLOPs: 88.5722. Time: 378.9295 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #586: GFLOPs: 109.1578. Time: 307.4688 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #587: GFLOPs: 108.9921. Time: 307.9364 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #588: GFLOPs: 109.1681. Time: 307.4399 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #589: GFLOPs: 109.1812. Time: 307.4028 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #590: GFLOPs: 48.7997. Time: 687.7633 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #591: GFLOPs: 357.7929. Time: 93.8046 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #592: GFLOPs: 357.8549. Time: 93.7884 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #593: GFLOPs: 357.8682. Time: 93.7849 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #594: GFLOPs: 357.5104. Time: 93.8787 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #595: GFLOPs: 357.4918. Time: 93.8836 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #596: GFLOPs: 357.4736. Time: 93.8884 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #597: GFLOPs: 357.0779. Time: 93.9924 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #598: GFLOPs: 357.8271. Time: 93.7957 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #599: GFLOPs: 357.0890. Time: 93.9895 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #600: GFLOPs: 357.0806. Time: 93.9917 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #601: GFLOPs: 356.9820. Time: 94.0177 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #602: GFLOPs: 357.0174. Time: 94.0084 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #603: GFLOPs: 357.0827. Time: 93.9912 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #604: GFLOPs: 357.8431. Time: 93.7915 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #605: GFLOPs: 357.0416. Time: 94.0020 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #606: GFLOPs: 357.0327. Time: 94.0043 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #607: GFLOPs: 357.0348. Time: 94.0038 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #608: GFLOPs: 357.0279. Time: 94.0056 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #609: GFLOPs: 357.0079. Time: 94.0109 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #610: GFLOPs: 336.5450. Time: 99.7270 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #611: GFLOPs: 336.0733. Time: 99.8670 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #612: GFLOPs: 357.0208. Time: 94.0075 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #613: GFLOPs: 356.9843. Time: 94.0171 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #614: GFLOPs: 336.2411. Time: 99.8171 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #615: GFLOPs: 357.8732. Time: 93.7836 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #616: GFLOPs: 357.8591. Time: 93.7873 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #617: GFLOPs: 357.0972. Time: 93.9874 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #618: GFLOPs: 357.0272. Time: 94.0058 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #619: GFLOPs: 358.1714. Time: 93.7055 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #620: GFLOPs: 356.6451. Time: 94.1065 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #621: GFLOPs: 355.8100. Time: 94.3274 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #622: GFLOPs: 356.8299. Time: 94.0578 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #623: GFLOPs: 356.8619. Time: 94.0493 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #624: GFLOPs: 356.0252. Time: 94.2704 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #625: GFLOPs: 357.9014. Time: 93.7762 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #626: GFLOPs: 357.5123. Time: 93.8782 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #627: GFLOPs: 357.8671. Time: 93.7852 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #628: GFLOPs: 356.8320. Time: 94.0572 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #629: GFLOPs: 334.1603. Time: 100.4387 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #630: GFLOPs: 332.7033. Time: 100.8785 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #631: GFLOPs: 333.7311. Time: 100.5679 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #632: GFLOPs: 333.1136. Time: 100.7543 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #633: GFLOPs: 332.9628. Time: 100.7999 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #634: GFLOPs: 331.5926. Time: 101.2164 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #635: GFLOPs: 335.1674. Time: 100.1369 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #636: GFLOPs: 34.8588. Time: 962.8160 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #637: GFLOPs: 3.9622. Time: 8470.7838 us. Best GFLOPs: 388.2341
2024-03-20 20:57:28 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #638: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 8, 64, 1, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:02:38 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:02:38 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:02:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 813 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1215 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1612 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2014 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2418 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2821 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3225 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:41 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-03-20 21:02:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:42 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 131 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:44 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 135 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:45 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:02:45 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2636  1.2521  1.2239  1.2179  1.1095  1.1056  0.9614  0.9598  0.9585  0.9585  0.9552  0.9551  0.9551  0.9550  0.9550  0.9550
[17 : 32]:	0.9550  0.9550  0.9542  0.9540  0.9540  0.9540  0.9536  0.9529  0.9520  0.9510  0.9507  0.9505  0.9497  0.9497  0.9493  0.9493
[33 : 48]:	0.9487  0.9481  0.9481  0.9481  0.9474  0.9462  0.9462  0.9462  0.9453  0.9449  0.9438  0.9429  0.9429  0.9428  0.9428  0.9425
[49 : 64]:	0.9424  0.9424  0.9424  0.9412  0.9354  0.9353  0.9353  0.9324  0.9307  0.9307  0.9307  0.9307  0.9307  0.9307  0.9307  0.9307
2024-03-20 21:02:45 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:02:45 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #639: GFLOPs: 228.4840. Time: 146.8927 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #640: GFLOPs: 230.4285. Time: 145.6531 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #641: GFLOPs: 230.4241. Time: 145.6559 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #642: GFLOPs: 270.2643. Time: 124.1844 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #643: GFLOPs: 182.9364. Time: 183.4661 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #644: GFLOPs: 185.8246. Time: 180.6146 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #645: GFLOPs: 357.0005. Time: 94.0128 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #646: GFLOPs: 356.8253. Time: 94.0590 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #647: GFLOPs: 356.8680. Time: 94.0477 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #648: GFLOPs: 356.8972. Time: 94.0400 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #649: GFLOPs: 356.6767. Time: 94.0982 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #650: GFLOPs: 356.8820. Time: 94.0440 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #651: GFLOPs: 357.0457. Time: 94.0009 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #652: GFLOPs: 356.9272. Time: 94.0321 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #653: GFLOPs: 356.8980. Time: 94.0398 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #654: GFLOPs: 356.9002. Time: 94.0392 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #655: GFLOPs: 356.8632. Time: 94.0490 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #656: GFLOPs: 356.8907. Time: 94.0417 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #657: GFLOPs: 356.9321. Time: 94.0308 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #658: GFLOPs: 356.9022. Time: 94.0387 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #659: GFLOPs: 356.8594. Time: 94.0500 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #660: GFLOPs: 356.8616. Time: 94.0494 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #661: GFLOPs: 356.8455. Time: 94.0536 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #662: GFLOPs: 357.7868. Time: 93.8062 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #663: GFLOPs: 350.5710. Time: 95.7370 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #664: GFLOPs: 356.9549. Time: 94.0248 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #665: GFLOPs: 357.8111. Time: 93.7998 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #666: GFLOPs: 356.9343. Time: 94.0302 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #667: GFLOPs: 356.9709. Time: 94.0206 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #668: GFLOPs: 356.9058. Time: 94.0378 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #669: GFLOPs: 356.4578. Time: 94.1560 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #670: GFLOPs: 356.4353. Time: 94.1619 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #671: GFLOPs: 356.2745. Time: 94.2044 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #672: GFLOPs: 357.5549. Time: 93.8671 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #673: GFLOPs: 357.5624. Time: 93.8651 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #674: GFLOPs: 357.7177. Time: 93.8243 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #675: GFLOPs: 356.3826. Time: 94.1758 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #676: GFLOPs: 356.2747. Time: 94.2043 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #677: GFLOPs: 356.1999. Time: 94.2241 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #678: GFLOPs: 356.1545. Time: 94.2361 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #679: GFLOPs: 356.2085. Time: 94.2219 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #680: GFLOPs: 356.2926. Time: 94.1996 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #681: GFLOPs: 356.2518. Time: 94.2104 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #682: GFLOPs: 356.5360. Time: 94.1353 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #683: GFLOPs: 356.4612. Time: 94.1551 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #684: GFLOPs: 356.2566. Time: 94.2091 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #685: GFLOPs: 356.2358. Time: 94.2146 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #686: GFLOPs: 357.1423. Time: 93.9755 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #687: GFLOPs: 357.1339. Time: 93.9777 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #688: GFLOPs: 357.1346. Time: 93.9775 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #689: GFLOPs: 357.0804. Time: 93.9918 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #690: GFLOPs: 356.2041. Time: 94.2230 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #691: GFLOPs: 356.1269. Time: 94.2434 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #692: GFLOPs: 356.1314. Time: 94.2422 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #693: GFLOPs: 356.1545. Time: 94.2361 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #694: GFLOPs: 356.2154. Time: 94.2200 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #695: GFLOPs: 366.3048. Time: 91.6249 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #696: GFLOPs: 366.2996. Time: 91.6261 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #697: GFLOPs: 366.2917. Time: 91.6281 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #698: GFLOPs: 366.4251. Time: 91.5948 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #699: GFLOPs: 366.3450. Time: 91.6148 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #700: GFLOPs: 31.5102. Time: 1065.1343 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #701: GFLOPs: 16.4677. Time: 2038.0876 us. Best GFLOPs: 388.2341
2024-03-20 21:03:42 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #702: GFLOPs: 45.1451. Time: 743.4382 us. Best GFLOPs: 388.2341
2024-03-20 21:09:26 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:09:26 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:09:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1622 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2028 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2431 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2837 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3238 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3641 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4047 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:29 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-20 21:09:30 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 190 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:31 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 141 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 152 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:33 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:09:34 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9466  0.9409  0.9407  0.9407  0.9404  0.9397  0.9397  0.9393  0.9391  0.9389  0.9383  0.9368  0.9365  0.9360  0.9359  0.9359
[17 : 32]:	0.9355  0.9355  0.9355  0.9353  0.9350  0.9343  0.9335  0.9331  0.9329  0.9317  0.9310  0.9308  0.9300  0.9278  0.9268  0.9268
[33 : 48]:	0.9265  0.9262  0.9262  0.9261  0.9249  0.9249  0.9240  0.9235  0.9233  0.9233  0.9232  0.9232  0.9231  0.9230  0.9223  0.9218
[49 : 64]:	0.9211  0.9210  0.9209  0.9208  0.9208  0.9206  0.9178  0.9174  0.9127  0.9116  0.9067  0.9067  0.9064  0.9063  0.9058  0.9057
2024-03-20 21:09:34 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:09:34 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #703: GFLOPs: 368.0823. Time: 91.1824 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #704: GFLOPs: 371.9643. Time: 90.2308 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #705: GFLOPs: 371.9170. Time: 90.2422 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #706: GFLOPs: 372.3453. Time: 90.1384 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #707: GFLOPs: 371.8982. Time: 90.2468 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #708: GFLOPs: 369.0027. Time: 90.9550 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #709: GFLOPs: 368.9192. Time: 90.9755 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #710: GFLOPs: 368.9019. Time: 90.9798 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #711: GFLOPs: 369.0662. Time: 90.9393 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #712: GFLOPs: 368.9396. Time: 90.9705 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #713: GFLOPs: 368.9979. Time: 90.9561 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #714: GFLOPs: 369.0658. Time: 90.9394 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #715: GFLOPs: 369.0425. Time: 90.9451 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #716: GFLOPs: 370.0267. Time: 90.7033 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #717: GFLOPs: 368.6405. Time: 91.0443 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #718: GFLOPs: 368.6614. Time: 91.0392 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #719: GFLOPs: 369.1856. Time: 90.9099 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #720: GFLOPs: 370.0314. Time: 90.7021 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #721: GFLOPs: 369.9646. Time: 90.7185 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #722: GFLOPs: 370.0196. Time: 90.7050 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #723: GFLOPs: 370.0583. Time: 90.6955 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #724: GFLOPs: 370.0273. Time: 90.7031 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #725: GFLOPs: 363.8444. Time: 92.2445 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #726: GFLOPs: 369.2213. Time: 90.9011 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #727: GFLOPs: 363.8740. Time: 92.2369 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #728: GFLOPs: 361.7846. Time: 92.7696 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #729: GFLOPs: 361.6006. Time: 92.8168 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #730: GFLOPs: 361.5425. Time: 92.8317 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #731: GFLOPs: 362.5038. Time: 92.5856 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #732: GFLOPs: 369.2677. Time: 90.8897 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #733: GFLOPs: 369.1431. Time: 90.9204 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #734: GFLOPs: 368.9977. Time: 90.9562 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #735: GFLOPs: 365.2798. Time: 91.8820 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #736: GFLOPs: 365.5869. Time: 91.8048 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #737: GFLOPs: 365.5823. Time: 91.8059 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #738: GFLOPs: 354.9161. Time: 94.5650 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #739: GFLOPs: 365.7901. Time: 91.7538 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #740: GFLOPs: 365.9838. Time: 91.7052 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #741: GFLOPs: 369.1578. Time: 90.9167 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #742: GFLOPs: 369.2321. Time: 90.8985 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #743: GFLOPs: 364.4323. Time: 92.0956 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #744: GFLOPs: 369.1752. Time: 90.9125 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #745: GFLOPs: 369.4329. Time: 90.8490 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #746: GFLOPs: 369.1937. Time: 90.9079 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #747: GFLOPs: 365.5642. Time: 91.8105 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #748: GFLOPs: 369.1431. Time: 90.9204 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #749: GFLOPs: 365.4176. Time: 91.8473 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #750: GFLOPs: 365.3237. Time: 91.8709 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #751: GFLOPs: 369.2516. Time: 90.8936 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #752: GFLOPs: 362.5960. Time: 92.5620 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #753: GFLOPs: 369.2517. Time: 90.8936 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #754: GFLOPs: 362.8351. Time: 92.5010 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #755: GFLOPs: 362.6026. Time: 92.5603 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #756: GFLOPs: 363.1556. Time: 92.4194 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #757: GFLOPs: 350.0398. Time: 95.8823 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #758: GFLOPs: 351.5055. Time: 95.4825 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #759: GFLOPs: 355.0447. Time: 94.5307 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #760: GFLOPs: 351.2570. Time: 95.5500 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #761: GFLOPs: 387.9571. Time: 86.5112 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #762: GFLOPs: 385.4146. Time: 87.0819 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #763: GFLOPs: 385.3225. Time: 87.1027 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #764: GFLOPs: 4.1823. Time: 8024.8465 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #765: GFLOPs: 48.7932. Time: 687.8545 us. Best GFLOPs: 388.2341
2024-03-20 21:10:30 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #766: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 8, 32, 4, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:13:15 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:13:15 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:13:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1213 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1621 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2025 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2428 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2830 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3235 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3640 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:18 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 21:13:19 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 133 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 152 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 151 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:22 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 133 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:13:23 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9745  0.9745  0.9743  0.9547  0.9547  0.9545  0.9545  0.9545  0.9425  0.9425  0.9423  0.9423  0.9423  0.9423  0.9411  0.9410
[17 : 32]:	0.9405  0.9400  0.9400  0.9400  0.9400  0.9389  0.9387  0.9379  0.9377  0.9370  0.9370  0.9368  0.9368  0.9368  0.9368  0.9361
[33 : 48]:	0.9345  0.9345  0.9302  0.9300  0.9300  0.9248  0.9195  0.9164  0.9162  0.9158  0.9157  0.9153  0.9064  0.9046  0.9044  0.9044
[49 : 64]:	0.9044  0.9027  0.9027  0.9027  0.8965  0.8962  0.8951  0.8950  0.8849  0.8843  0.8828  0.8819  0.8799  0.8799  0.8791  0.8764
2024-03-20 21:13:23 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:13:23 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #767: GFLOPs: 377.8646. Time: 88.8218 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #768: GFLOPs: 379.5909. Time: 88.4179 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #769: GFLOPs: 379.4367. Time: 88.4538 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #770: GFLOPs: 379.4487. Time: 88.4510 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #771: GFLOPs: 379.6031. Time: 88.4150 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #772: GFLOPs: 379.1323. Time: 88.5248 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #773: GFLOPs: 379.5412. Time: 88.4295 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #774: GFLOPs: 378.9037. Time: 88.5783 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #775: GFLOPs: 380.1627. Time: 88.2849 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #776: GFLOPs: 380.0760. Time: 88.3050 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #777: GFLOPs: 379.5961. Time: 88.4167 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #778: GFLOPs: 380.1651. Time: 88.2843 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #779: GFLOPs: 384.9208. Time: 87.1936 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #780: GFLOPs: 380.1246. Time: 88.2937 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #781: GFLOPs: 366.3433. Time: 91.6152 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #782: GFLOPs: 357.2140. Time: 93.9566 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #783: GFLOPs: 357.3031. Time: 93.9332 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #784: GFLOPs: 379.7152. Time: 88.3889 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #785: GFLOPs: 379.6504. Time: 88.4040 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #786: GFLOPs: 384.6330. Time: 87.2588 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #787: GFLOPs: 379.7057. Time: 88.3911 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #788: GFLOPs: 363.9065. Time: 92.2287 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #789: GFLOPs: 364.3990. Time: 92.1040 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #790: GFLOPs: 357.1602. Time: 93.9708 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #791: GFLOPs: 357.1852. Time: 93.9642 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #792: GFLOPs: 382.9895. Time: 87.6333 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #793: GFLOPs: 383.0209. Time: 87.6261 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #794: GFLOPs: 383.0251. Time: 87.6251 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #795: GFLOPs: 382.9644. Time: 87.6390 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #796: GFLOPs: 382.7445. Time: 87.6894 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #797: GFLOPs: 384.2032. Time: 87.3565 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #798: GFLOPs: 356.8361. Time: 94.0561 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #799: GFLOPs: 384.4814. Time: 87.2932 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #800: GFLOPs: 382.5617. Time: 87.7313 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #801: GFLOPs: 356.2926. Time: 94.1996 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #802: GFLOPs: 356.6247. Time: 94.1119 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #803: GFLOPs: 356.3198. Time: 94.1924 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #804: GFLOPs: 356.5811. Time: 94.1234 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #805: GFLOPs: 355.8776. Time: 94.3094 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #806: GFLOPs: 355.0396. Time: 94.5321 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #807: GFLOPs: 355.1126. Time: 94.5126 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #808: GFLOPs: 356.7259. Time: 94.0852 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #809: GFLOPs: 354.4464. Time: 94.6903 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #810: GFLOPs: 356.5327. Time: 94.1362 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #811: GFLOPs: 356.3062. Time: 94.1960 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #812: GFLOPs: 363.8602. Time: 92.2404 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #813: GFLOPs: 364.4966. Time: 92.0794 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #814: GFLOPs: 363.9614. Time: 92.2148 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #815: GFLOPs: 363.7959. Time: 92.2567 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #816: GFLOPs: 356.8414. Time: 94.0547 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #817: GFLOPs: 356.8682. Time: 94.0477 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #818: GFLOPs: 356.7609. Time: 94.0760 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #819: GFLOPs: 356.5495. Time: 94.1317 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #820: GFLOPs: 356.5605. Time: 94.1288 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #821: GFLOPs: 353.7397. Time: 94.8794 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #822: GFLOPs: 356.2812. Time: 94.2026 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #823: GFLOPs: 357.4644. Time: 93.8908 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #824: GFLOPs: 336.0214. Time: 99.8824 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #825: GFLOPs: 354.2937. Time: 94.7311 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #826: GFLOPs: 355.3010. Time: 94.4625 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #827: GFLOPs: 354.9026. Time: 94.5685 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #828: GFLOPs: 21.3357. Time: 1573.0740 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #829: GFLOPs: 10.4502. Time: 3211.6799 us. Best GFLOPs: 388.2341
2024-03-20 21:14:20 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #830: GFLOPs: 34.5708. Time: 970.8383 us. Best GFLOPs: 388.2341
2024-03-20 21:20:47 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:20:47 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:20:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1614 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2018 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2422 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2825 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3230 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:49 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 21:20:50 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:51 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:53 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 149 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:54 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 140 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:20:54 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9839  0.9807  0.9785  0.9785  0.9785  0.9785  0.9785  0.9785  0.9782  0.9782  0.9782  0.9782  0.9606  0.9550  0.9550  0.9441
[17 : 32]:	0.9438  0.9438  0.9406  0.9344  0.9320  0.9320  0.9201  0.9194  0.9176  0.9171  0.9171  0.9171  0.9161  0.9161  0.9161  0.9161
[33 : 48]:	0.9161  0.9154  0.9154  0.9151  0.9122  0.9122  0.9103  0.9102  0.9099  0.9090  0.9090  0.9090  0.9090  0.9090  0.9068  0.9068
[49 : 64]:	0.9068  0.9068  0.9068  0.9068  0.9068  0.9068  0.9068  0.9068  0.9068  0.9068  0.9068  0.9065  0.9065  0.9065  0.9065  0.9065
2024-03-20 21:20:54 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:20:54 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #831: GFLOPs: 378.0029. Time: 88.7893 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #832: GFLOPs: 382.7337. Time: 87.6919 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #833: GFLOPs: 383.4637. Time: 87.5249 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #834: GFLOPs: 383.5183. Time: 87.5124 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #835: GFLOPs: 383.5207. Time: 87.5119 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #836: GFLOPs: 383.5476. Time: 87.5058 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #837: GFLOPs: 383.3206. Time: 87.5576 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #838: GFLOPs: 383.0674. Time: 87.6154 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #839: GFLOPs: 380.1711. Time: 88.2830 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #840: GFLOPs: 380.2338. Time: 88.2684 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #841: GFLOPs: 380.6213. Time: 88.1785 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #842: GFLOPs: 380.5758. Time: 88.1891 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #843: GFLOPs: 264.9158. Time: 126.6917 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #844: GFLOPs: 264.9683. Time: 126.6666 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #845: GFLOPs: 265.0508. Time: 126.6271 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #846: GFLOPs: 364.2836. Time: 92.1332 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #847: GFLOPs: 264.4933. Time: 126.8940 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #848: GFLOPs: 264.5066. Time: 126.8877 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #849: GFLOPs: 356.4307. Time: 94.1631 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #850: GFLOPs: 151.1687. Time: 222.0210 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #851: GFLOPs: 356.5154. Time: 94.1407 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #852: GFLOPs: 356.5427. Time: 94.1335 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #853: GFLOPs: 354.7268. Time: 94.6154 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #854: GFLOPs: 355.2506. Time: 94.4759 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #855: GFLOPs: 151.1108. Time: 222.1060 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #856: GFLOPs: 151.1295. Time: 222.0786 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #857: GFLOPs: 356.5522. Time: 94.1310 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #858: GFLOPs: 356.1048. Time: 94.2493 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #859: GFLOPs: 356.4066. Time: 94.1695 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #860: GFLOPs: 356.3332. Time: 94.1889 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #861: GFLOPs: 356.2336. Time: 94.2152 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #862: GFLOPs: 355.8324. Time: 94.3214 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #863: GFLOPs: 355.8675. Time: 94.3121 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #864: GFLOPs: 355.9420. Time: 94.2924 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #865: GFLOPs: 356.3816. Time: 94.1761 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #866: GFLOPs: 355.7784. Time: 94.3358 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #867: GFLOPs: 366.3853. Time: 91.6047 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #868: GFLOPs: 364.2682. Time: 92.1371 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #869: GFLOPs: 354.6511. Time: 94.6356 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #870: GFLOPs: 151.1891. Time: 221.9910 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #871: GFLOPs: 356.1095. Time: 94.2480 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #872: GFLOPs: 364.1501. Time: 92.1670 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #873: GFLOPs: 363.8791. Time: 92.2356 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #874: GFLOPs: 364.2425. Time: 92.1436 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #875: GFLOPs: 363.9860. Time: 92.2086 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #876: GFLOPs: 366.4636. Time: 91.5852 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #877: GFLOPs: 351.0254. Time: 95.6131 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #878: GFLOPs: 350.1736. Time: 95.8457 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #879: GFLOPs: 350.3845. Time: 95.7880 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #880: GFLOPs: 349.0048. Time: 96.1667 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #881: GFLOPs: 350.4968. Time: 95.7573 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #882: GFLOPs: 350.6488. Time: 95.7158 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #883: GFLOPs: 350.5668. Time: 95.7382 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #884: GFLOPs: 350.6736. Time: 95.7090 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #885: GFLOPs: 350.6613. Time: 95.7124 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #886: GFLOPs: 350.7737. Time: 95.6817 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #887: GFLOPs: 350.5103. Time: 95.7536 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #888: GFLOPs: 348.9763. Time: 96.1745 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #889: GFLOPs: 350.6800. Time: 95.7073 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #890: GFLOPs: 363.8728. Time: 92.2373 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #891: GFLOPs: 363.6845. Time: 92.2850 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #892: GFLOPs: 114.1129. Time: 294.1176 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #893: GFLOPs: 3.6650. Time: 9157.6205 us. Best GFLOPs: 388.2341
2024-03-20 21:21:50 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #894: GFLOPs: 12.3358. Time: 2720.7402 us. Best GFLOPs: 388.2341
2024-03-20 21:31:09 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:31:09 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:31:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1212 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1616 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2021 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2425 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2828 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3231 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3637 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:12 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-20 21:31:12 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:14 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 150 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:15 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 151 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:16 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:31:17 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9834  0.9411  0.9280  0.9249  0.9233  0.9233  0.9226  0.9221  0.9221  0.9215  0.9208  0.9208  0.9195  0.9190  0.9187  0.9182
[17 : 32]:	0.9182  0.9182  0.9178  0.9175  0.9171  0.9168  0.9155  0.9151  0.9150  0.9150  0.9150  0.9147  0.9124  0.9121  0.9116  0.9040
[33 : 48]:	0.9040  0.8929  0.8905  0.8883  0.8865  0.8809  0.8775  0.8774  0.8774  0.8773  0.8772  0.8768  0.8768  0.8765  0.8764  0.8763
[49 : 64]:	0.8763  0.8760  0.8760  0.8760  0.8758  0.8758  0.8758  0.8758  0.8758  0.8758  0.8757  0.8755  0.8750  0.8750  0.8750  0.8745
2024-03-20 21:31:17 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:31:17 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #895: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #896: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #897: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #898: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #899: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 16, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #900: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #901: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #902: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 16, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #903: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 2, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #904: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(32) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 1, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #905: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #906: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #907: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #908: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #909: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 64, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #910: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #911: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #912: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #913: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 16, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #914: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #915: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(32) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 32, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #916: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(32) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 1, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #917: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #918: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #919: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 16, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #920: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 2, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #921: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 4, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #922: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 1, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #923: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(32) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 32, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #924: GFLOPs: 355.5592. Time: 94.3939 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #925: GFLOPs: 357.0323. Time: 94.0044 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #926: GFLOPs: 269.1985. Time: 124.6761 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #927: GFLOPs: 269.1971. Time: 124.6768 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #928: GFLOPs: 355.3310. Time: 94.4545 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #929: GFLOPs: 337.0858. Time: 99.5670 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #930: GFLOPs: 356.0768. Time: 94.2567 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #931: GFLOPs: 339.2510. Time: 98.9315 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #932: GFLOPs: 352.0306. Time: 95.3401 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #933: GFLOPs: 356.0797. Time: 94.2559 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #934: GFLOPs: 353.0686. Time: 95.0598 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #935: GFLOPs: 353.0853. Time: 95.0553 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #936: GFLOPs: 351.9088. Time: 95.3731 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #937: GFLOPs: 342.0054. Time: 98.1348 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #938: GFLOPs: 353.0918. Time: 95.0535 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #939: GFLOPs: 353.0980. Time: 95.0519 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #940: GFLOPs: 329.5585. Time: 101.8412 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #941: GFLOPs: 348.5307. Time: 96.2975 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #942: GFLOPs: 348.4883. Time: 96.3092 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #943: GFLOPs: 347.0858. Time: 96.6983 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #944: GFLOPs: 329.8330. Time: 101.7564 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #945: GFLOPs: 329.5914. Time: 101.8310 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #946: GFLOPs: 329.6146. Time: 101.8238 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #947: GFLOPs: 347.2573. Time: 96.6506 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #948: GFLOPs: 348.4642. Time: 96.3158 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #949: GFLOPs: 348.4970. Time: 96.3068 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #950: GFLOPs: 347.1484. Time: 96.6809 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #951: GFLOPs: 347.1000. Time: 96.6944 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #952: GFLOPs: 347.1219. Time: 96.6883 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #953: GFLOPs: 329.7696. Time: 101.7760 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #954: GFLOPs: 347.0824. Time: 96.6993 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #955: GFLOPs: 353.0469. Time: 95.0656 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #956: GFLOPs: 317.3420. Time: 105.7617 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #957: GFLOPs: 4.5400. Time: 7392.6947 us. Best GFLOPs: 388.2341
2024-03-20 21:31:51 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #958: GFLOPs: 4.4657. Time: 7515.6386 us. Best GFLOPs: 388.2341
2024-03-20 21:33:14 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:33:14 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:33:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1212 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1619 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2023 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2430 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2837 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3243 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3649 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4054 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4457 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:17 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-20 21:33:18 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:19 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 181 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:20 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 140 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:21 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 149 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:33:22 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9391  0.9334  0.9273  0.9169  0.9169  0.9168  0.9165  0.9156  0.9150  0.9121  0.9109  0.9106  0.9105  0.9104  0.9098  0.9098
[17 : 32]:	0.9062  0.9062  0.9061  0.9061  0.9058  0.8981  0.8950  0.8934  0.8923  0.8922  0.8920  0.8919  0.8914  0.8914  0.8890  0.8881
[33 : 48]:	0.8879  0.8879  0.8879  0.8879  0.8879  0.8877  0.8877  0.8877  0.8875  0.8875  0.8874  0.8874  0.8874  0.8868  0.8868  0.8868
[49 : 64]:	0.8867  0.8866  0.8865  0.8865  0.8848  0.8829  0.8822  0.8822  0.8820  0.8820  0.8820  0.8820  0.8820  0.8816  0.8813  0.8813
2024-03-20 21:33:22 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:33:22 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #959: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 16, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #960: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 2, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #961: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #962: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #963: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 64, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #964: GFLOPs: 363.1719. Time: 92.4152 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #965: GFLOPs: 364.9591. Time: 91.9627 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #966: GFLOPs: 355.3459. Time: 94.4506 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #967: GFLOPs: 351.2907. Time: 95.5409 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #968: GFLOPs: 356.8594. Time: 94.0500 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #969: GFLOPs: 354.6369. Time: 94.6394 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #970: GFLOPs: 358.3574. Time: 93.6568 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #971: GFLOPs: 357.0345. Time: 94.0039 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #972: GFLOPs: 359.2180. Time: 93.4325 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #973: GFLOPs: 356.9345. Time: 94.0302 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #974: GFLOPs: 356.6354. Time: 94.1091 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #975: GFLOPs: 356.6064. Time: 94.1167 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #976: GFLOPs: 356.5994. Time: 94.1186 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #977: GFLOPs: 356.5585. Time: 94.1294 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #978: GFLOPs: 356.6110. Time: 94.1155 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #979: GFLOPs: 356.5677. Time: 94.1269 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #980: GFLOPs: 338.5264. Time: 99.1433 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #981: GFLOPs: 358.1647. Time: 93.7072 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #982: GFLOPs: 351.9676. Time: 95.3571 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #983: GFLOPs: 348.4040. Time: 96.3325 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #984: GFLOPs: 344.2522. Time: 97.4943 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #985: GFLOPs: 348.4467. Time: 96.3207 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #986: GFLOPs: 344.1497. Time: 97.5233 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #987: GFLOPs: 333.8262. Time: 100.5392 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #988: GFLOPs: 355.1289. Time: 94.5083 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #989: GFLOPs: 351.7091. Time: 95.4272 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #990: GFLOPs: 350.7786. Time: 95.6804 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #991: GFLOPs: 351.9409. Time: 95.3644 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #992: GFLOPs: 353.0995. Time: 95.0515 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #993: GFLOPs: 351.9319. Time: 95.3668 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #994: GFLOPs: 353.1004. Time: 95.0512 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #995: GFLOPs: 353.1025. Time: 95.0507 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #996: GFLOPs: 347.1052. Time: 96.6929 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #997: GFLOPs: 345.8503. Time: 97.0438 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #998: GFLOPs: 355.1650. Time: 94.4987 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #999: GFLOPs: 347.1430. Time: 96.6824 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1000: GFLOPs: 347.0973. Time: 96.6951 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1001: GFLOPs: 348.0408. Time: 96.4330 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1002: GFLOPs: 345.7857. Time: 97.0619 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1003: GFLOPs: 347.0657. Time: 96.7039 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1004: GFLOPs: 354.4796. Time: 94.6814 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1005: GFLOPs: 348.5140. Time: 96.3021 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1006: GFLOPs: 348.5401. Time: 96.2949 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1007: GFLOPs: 354.3623. Time: 94.7127 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1008: GFLOPs: 347.3120. Time: 96.6354 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1009: GFLOPs: 347.1352. Time: 96.6846 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1010: GFLOPs: 348.4479. Time: 96.3203 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1011: GFLOPs: 355.7962. Time: 94.3310 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1012: GFLOPs: 355.1595. Time: 94.5001 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1013: GFLOPs: 347.0527. Time: 96.7076 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1014: GFLOPs: 347.0763. Time: 96.7010 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1015: GFLOPs: 345.9127. Time: 97.0263 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1016: GFLOPs: 347.1046. Time: 96.6931 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1017: GFLOPs: 347.0177. Time: 96.7173 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1018: GFLOPs: 347.0529. Time: 96.7075 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1019: GFLOPs: 345.8856. Time: 97.0339 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1020: GFLOPs: 14.8475. Time: 2260.4914 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1021: GFLOPs: 3.3887. Time: 9904.3142 us. Best GFLOPs: 388.2341
2024-03-20 21:34:16 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1022: GFLOPs: 7.6416. Time: 4392.1210 us. Best GFLOPs: 388.2341
2024-03-20 21:38:12 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:38:12 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:38:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1620 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2023 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2426 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2833 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3236 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3641 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4047 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:15 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-20 21:38:16 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:17 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 147 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:18 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:19 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 140 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:38:20 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9285  0.9073  0.9029  0.9013  0.8974  0.8973  0.8973  0.8941  0.8936  0.8911  0.8884  0.8884  0.8884  0.8884  0.8874  0.8858
[17 : 32]:	0.8858  0.8854  0.8854  0.8835  0.8812  0.8812  0.8812  0.8812  0.8812  0.8812  0.8751  0.8751  0.8717  0.8717  0.8717  0.8717
[33 : 48]:	0.8717  0.8717  0.8712  0.8712  0.8712  0.8712  0.8712  0.8712  0.8712  0.8665  0.8658  0.8658  0.8650  0.8648  0.8641  0.8625
[49 : 64]:	0.8618  0.8616  0.8616  0.8616  0.8616  0.8616  0.8616  0.8616  0.8611  0.8611  0.8611  0.8611  0.8611  0.8611  0.8611  0.8601
2024-03-20 21:38:20 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:38:20 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1023: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 8, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1024: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 4, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1025: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 1, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1026: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1027: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1028: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1029: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1030: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1031: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 32, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1032: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1033: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 8, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1034: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 32, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1035: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 8, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1036: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 128, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1037: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 4, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1038: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 16, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1039: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1040: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1041: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1042: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1043: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1044: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1045: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1046: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1047: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 64, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1048: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1049: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1050: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1051: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1052: GFLOPs: 330.9102. Time: 101.4252 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1053: GFLOPs: 333.5700. Time: 100.6164 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1054: GFLOPs: 346.9725. Time: 96.7299 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1055: GFLOPs: 333.4154. Time: 100.6631 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1056: GFLOPs: 333.3471. Time: 100.6837 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1057: GFLOPs: 333.5276. Time: 100.6292 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1058: GFLOPs: 333.3392. Time: 100.6861 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1059: GFLOPs: 345.8044. Time: 97.0567 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1060: GFLOPs: 332.2550. Time: 101.0147 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1061: GFLOPs: 332.3559. Time: 100.9840 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1062: GFLOPs: 332.4382. Time: 100.9590 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1063: GFLOPs: 332.4723. Time: 100.9486 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1064: GFLOPs: 316.0238. Time: 106.2029 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1065: GFLOPs: 315.9716. Time: 106.2204 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1066: GFLOPs: 316.6747. Time: 105.9845 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1067: GFLOPs: 315.5487. Time: 106.3627 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1068: GFLOPs: 318.6362. Time: 105.3321 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1069: GFLOPs: 318.7223. Time: 105.3037 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1070: GFLOPs: 318.6127. Time: 105.3399 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1071: GFLOPs: 315.6178. Time: 106.3395 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1072: GFLOPs: 348.2497. Time: 96.3752 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1073: GFLOPs: 348.3057. Time: 96.3597 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1074: GFLOPs: 348.3374. Time: 96.3509 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1075: GFLOPs: 351.5562. Time: 95.4687 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1076: GFLOPs: 348.4530. Time: 96.3189 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1077: GFLOPs: 349.5405. Time: 96.0193 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1078: GFLOPs: 349.0606. Time: 96.1513 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1079: GFLOPs: 350.2906. Time: 95.8137 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1080: GFLOPs: 348.3538. Time: 96.3464 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1081: GFLOPs: 351.1964. Time: 95.5665 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1082: GFLOPs: 348.3006. Time: 96.3611 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1083: GFLOPs: 349.0777. Time: 96.1466 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1084: GFLOPs: 11.3788. Time: 2949.5717 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1085: GFLOPs: 2.1366. Time: 15708.5984 us. Best GFLOPs: 388.2341
2024-03-20 21:38:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1086: GFLOPs: 4.1324. Time: 8121.8044 us. Best GFLOPs: 388.2341
2024-03-20 21:42:06 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:42:06 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:42:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1213 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1621 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2024 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2429 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2830 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3231 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3638 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:08 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-20 21:42:09 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 162 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:10 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 179 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:12 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:13 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:42:14 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9986  0.9293  0.9153  0.9093  0.9082  0.9061  0.9036  0.9032  0.9003  0.9002  0.8991  0.8972  0.8966  0.8956  0.8936  0.8936
[17 : 32]:	0.8935  0.8927  0.8881  0.8871  0.8842  0.8842  0.8828  0.8806  0.8803  0.8803  0.8802  0.8802  0.8795  0.8795  0.8795  0.8795
[33 : 48]:	0.8795  0.8795  0.8795  0.8789  0.8789  0.8789  0.8789  0.8789  0.8789  0.8789  0.8777  0.8763  0.8746  0.8721  0.8704  0.8678
[49 : 64]:	0.8393  0.8393  0.8385  0.8382  0.8382  0.8382  0.8382  0.8376  0.8376  0.8368  0.8368  0.8368  0.8368  0.8368  0.8307  0.8307
2024-03-20 21:42:14 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:42:14 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1087: GFLOPs: 3.3392. Time: 10051.0528 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1088: GFLOPs: 376.6412. Time: 89.1103 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1089: GFLOPs: 376.1907. Time: 89.2171 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1090: GFLOPs: 345.5270. Time: 97.1346 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1091: GFLOPs: 347.7934. Time: 96.5016 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1092: GFLOPs: 346.5723. Time: 96.8416 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1093: GFLOPs: 346.2860. Time: 96.9217 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1094: GFLOPs: 346.8272. Time: 96.7704 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1095: GFLOPs: 346.1063. Time: 96.9720 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1096: GFLOPs: 343.7992. Time: 97.6228 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1097: GFLOPs: 344.3919. Time: 97.4547 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1098: GFLOPs: 366.9261. Time: 91.4697 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1099: GFLOPs: 345.1667. Time: 97.2360 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1100: GFLOPs: 343.9122. Time: 97.5907 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1101: GFLOPs: 368.2957. Time: 91.1296 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1102: GFLOPs: 368.2259. Time: 91.1468 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1103: GFLOPs: 367.0226. Time: 91.4457 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1104: GFLOPs: 346.7978. Time: 96.7786 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1105: GFLOPs: 346.0123. Time: 96.9984 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1106: GFLOPs: 354.9497. Time: 94.5560 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1107: GFLOPs: 343.9324. Time: 97.5850 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1108: GFLOPs: 343.9551. Time: 97.5785 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1109: GFLOPs: 368.7166. Time: 91.0255 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1110: GFLOPs: 343.9830. Time: 97.5706 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1111: GFLOPs: 345.2122. Time: 97.2232 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1112: GFLOPs: 345.3048. Time: 97.1971 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1113: GFLOPs: 344.3225. Time: 97.4744 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1114: GFLOPs: 343.7980. Time: 97.6231 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1115: GFLOPs: 345.5992. Time: 97.1143 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1116: GFLOPs: 345.9582. Time: 97.0135 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1117: GFLOPs: 344.4390. Time: 97.4414 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1118: GFLOPs: 345.9892. Time: 97.0048 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1119: GFLOPs: 346.0829. Time: 96.9786 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1120: GFLOPs: 346.0231. Time: 96.9953 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1121: GFLOPs: 346.0266. Time: 96.9944 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1122: GFLOPs: 342.9985. Time: 97.8507 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1123: GFLOPs: 344.4131. Time: 97.4487 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1124: GFLOPs: 344.5030. Time: 97.4233 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1125: GFLOPs: 344.4001. Time: 97.4524 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1126: GFLOPs: 344.0899. Time: 97.5403 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1127: GFLOPs: 345.4703. Time: 97.1505 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1128: GFLOPs: 344.4637. Time: 97.4344 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1129: GFLOPs: 341.4742. Time: 98.2874 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1130: GFLOPs: 344.2723. Time: 97.4886 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1131: GFLOPs: 352.1127. Time: 95.3178 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1132: GFLOPs: 353.1524. Time: 95.0372 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1133: GFLOPs: 363.7010. Time: 92.2808 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1134: GFLOPs: 364.4138. Time: 92.1003 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1135: GFLOPs: 328.3829. Time: 102.2057 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1136: GFLOPs: 328.5191. Time: 102.1634 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1137: GFLOPs: 327.0884. Time: 102.6103 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1138: GFLOPs: 328.0000. Time: 102.3251 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1139: GFLOPs: 326.3276. Time: 102.8495 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1140: GFLOPs: 326.3472. Time: 102.8433 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1141: GFLOPs: 326.3823. Time: 102.8322 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1142: GFLOPs: 327.1717. Time: 102.5841 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1143: GFLOPs: 327.2605. Time: 102.5563 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1144: GFLOPs: 328.4123. Time: 102.1966 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1145: GFLOPs: 325.6795. Time: 103.0542 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1146: GFLOPs: 327.1639. Time: 102.5866 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1147: GFLOPs: 321.4647. Time: 104.4053 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1148: GFLOPs: 43.4921. Time: 771.6943 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1149: GFLOPs: 44.6681. Time: 751.3778 us. Best GFLOPs: 388.2341
2024-03-20 21:43:05 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1150: GFLOPs: 34.7792. Time: 965.0197 us. Best GFLOPs: 388.2341
2024-03-20 21:49:03 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:49:03 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:49:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 814 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1221 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1624 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2029 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2434 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2840 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3244 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3649 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4052 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4460 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:07 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 21:49:07 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 158 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:10 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 150 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:11 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:49:12 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9505  0.9324  0.9260  0.9218  0.9203  0.9194  0.9173  0.9148  0.9145  0.9104  0.9087  0.9044  0.9044  0.9024  0.9022  0.9014
[17 : 32]:	0.8999  0.8987  0.8987  0.8971  0.8961  0.8904  0.8895  0.8873  0.8829  0.8825  0.8553  0.8552  0.8530  0.8505  0.8505  0.8500
[33 : 48]:	0.8494  0.8494  0.8474  0.8460  0.8444  0.8430  0.8430  0.8419  0.8412  0.8408  0.8406  0.8398  0.8397  0.8369  0.8355  0.8344
[49 : 64]:	0.8341  0.8319  0.8319  0.8317  0.8317  0.8309  0.8307  0.8246  0.8246  0.8238  0.8238  0.8238  0.8180  0.8178  0.8161  0.8161
2024-03-20 21:49:12 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:49:12 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1151: GFLOPs: 3.8485. Time: 8720.9596 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1152: GFLOPs: 3.8087. Time: 8812.2024 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1153: GFLOPs: 371.4541. Time: 90.3547 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1154: GFLOPs: 371.9930. Time: 90.2238 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1155: GFLOPs: 364.5737. Time: 92.0599 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1156: GFLOPs: 351.5355. Time: 95.4743 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1157: GFLOPs: 364.6666. Time: 92.0365 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1158: GFLOPs: 370.8271. Time: 90.5075 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1159: GFLOPs: 365.6587. Time: 91.7868 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1160: GFLOPs: 364.6279. Time: 92.0462 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1161: GFLOPs: 364.5943. Time: 92.0547 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1162: GFLOPs: 364.5753. Time: 92.0595 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1163: GFLOPs: 364.5048. Time: 92.0773 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1164: GFLOPs: 362.7911. Time: 92.5123 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1165: GFLOPs: 348.9374. Time: 96.1852 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1166: GFLOPs: 347.7391. Time: 96.5167 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1167: GFLOPs: 347.9052. Time: 96.4706 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1168: GFLOPs: 349.0697. Time: 96.1488 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1169: GFLOPs: 348.9965. Time: 96.1689 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1170: GFLOPs: 344.8141. Time: 97.3354 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1171: GFLOPs: 344.3403. Time: 97.4694 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1172: GFLOPs: 343.4451. Time: 97.7234 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1173: GFLOPs: 343.5209. Time: 97.7018 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1174: GFLOPs: 348.3702. Time: 96.3418 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1175: GFLOPs: 342.7933. Time: 97.9092 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1176: GFLOPs: 344.6769. Time: 97.3742 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1177: GFLOPs: 327.5907. Time: 102.4529 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1178: GFLOPs: 326.9426. Time: 102.6560 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1179: GFLOPs: 328.8237. Time: 102.0687 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1180: GFLOPs: 328.7636. Time: 102.0874 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1181: GFLOPs: 328.7574. Time: 102.0893 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1182: GFLOPs: 327.3785. Time: 102.5193 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1183: GFLOPs: 327.5213. Time: 102.4746 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1184: GFLOPs: 328.8729. Time: 102.0535 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1185: GFLOPs: 327.6984. Time: 102.4192 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1186: GFLOPs: 326.8230. Time: 102.6936 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1187: GFLOPs: 327.7291. Time: 102.4097 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1188: GFLOPs: 311.8940. Time: 107.6091 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1189: GFLOPs: 325.4724. Time: 103.1197 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1190: GFLOPs: 308.7744. Time: 108.6963 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1191: GFLOPs: 325.4935. Time: 103.1130 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1192: GFLOPs: 309.0056. Time: 108.6149 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1193: GFLOPs: 308.5021. Time: 108.7922 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1194: GFLOPs: 307.7627. Time: 109.0536 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1195: GFLOPs: 323.5595. Time: 103.7294 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1196: GFLOPs: 319.4522. Time: 105.0631 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1197: GFLOPs: 301.3088. Time: 111.3895 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1198: GFLOPs: 323.5910. Time: 103.7193 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1199: GFLOPs: 273.6965. Time: 122.6272 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1200: GFLOPs: 347.8286. Time: 96.4918 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1201: GFLOPs: 347.8110. Time: 96.4967 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1202: GFLOPs: 319.1220. Time: 105.1718 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1203: GFLOPs: 319.1493. Time: 105.1628 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1204: GFLOPs: 307.7209. Time: 109.0684 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1205: GFLOPs: 307.3957. Time: 109.1838 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1206: GFLOPs: 326.9378. Time: 102.6575 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1207: GFLOPs: 327.4352. Time: 102.5016 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1208: GFLOPs: 283.2808. Time: 118.4783 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1209: GFLOPs: 283.3485. Time: 118.4500 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1210: GFLOPs: 283.3022. Time: 118.4693 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1211: GFLOPs: 314.8810. Time: 106.5883 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1212: GFLOPs: 77.5013. Time: 433.0589 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1213: GFLOPs: 3.2341. Time: 10377.7282 us. Best GFLOPs: 388.2341
2024-03-20 21:50:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1214: GFLOPs: 77.7354. Time: 431.7546 us. Best GFLOPs: 388.2341
2024-03-20 21:52:22 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:52:22 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:52:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1206 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1613 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2015 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2423 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2828 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3235 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3642 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4048 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:25 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-20 21:52:26 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 176 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:27 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 165 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 132 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:30 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 152 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:52:30 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9304  0.9184  0.9141  0.9137  0.9114  0.9074  0.9062  0.9037  0.9037  0.9015  0.8939  0.8905  0.8902  0.8795  0.8532  0.8531
[17 : 32]:	0.8503  0.8502  0.8467  0.8467  0.8462  0.8460  0.8460  0.8451  0.8420  0.8412  0.8362  0.8319  0.8301  0.8301  0.8297  0.8276
[33 : 48]:	0.8150  0.8139  0.8139  0.8128  0.8128  0.8128  0.8128  0.8126  0.8125  0.8116  0.8116  0.8104  0.8090  0.8080  0.8080  0.8080
[49 : 64]:	0.8079  0.8079  0.8078  0.8060  0.8060  0.8059  0.8059  0.8055  0.8052  0.8052  0.8045  0.8043  0.8043  0.8037  0.8036  0.8036
2024-03-20 21:52:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:52:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1215: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1216: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 4, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1217: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1218: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 8, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1219: GFLOPs: 353.8901. Time: 94.8391 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1220: GFLOPs: 352.9882. Time: 95.0814 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1221: GFLOPs: 353.5317. Time: 94.9353 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1222: GFLOPs: 351.9126. Time: 95.3720 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1223: GFLOPs: 351.8841. Time: 95.3798 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1224: GFLOPs: 335.8193. Time: 99.9425 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1225: GFLOPs: 355.9579. Time: 94.2882 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1226: GFLOPs: 333.6380. Time: 100.5959 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1227: GFLOPs: 334.8536. Time: 100.2307 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1228: GFLOPs: 333.0664. Time: 100.7686 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1229: GFLOPs: 352.3854. Time: 95.2441 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1230: GFLOPs: 352.4033. Time: 95.2393 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1231: GFLOPs: 327.6141. Time: 102.4456 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1232: GFLOPs: 326.2916. Time: 102.8608 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1233: GFLOPs: 328.3025. Time: 102.2308 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1234: GFLOPs: 328.2652. Time: 102.2424 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1235: GFLOPs: 326.4310. Time: 102.8169 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1236: GFLOPs: 327.3436. Time: 102.5303 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1237: GFLOPs: 327.3063. Time: 102.5420 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1238: GFLOPs: 326.0424. Time: 102.9395 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1239: GFLOPs: 328.2342. Time: 102.2521 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1240: GFLOPs: 326.2835. Time: 102.8634 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1241: GFLOPs: 327.3240. Time: 102.5364 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1242: GFLOPs: 323.9366. Time: 103.6086 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1243: GFLOPs: 340.8834. Time: 98.4578 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1244: GFLOPs: 340.8153. Time: 98.4775 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1245: GFLOPs: 338.7768. Time: 99.0700 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1246: GFLOPs: 319.9204. Time: 104.9093 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1247: GFLOPs: 336.0911. Time: 99.8617 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1248: GFLOPs: 317.1882. Time: 105.8130 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1249: GFLOPs: 316.0805. Time: 106.1838 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1250: GFLOPs: 313.7239. Time: 106.9814 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1251: GFLOPs: 314.6622. Time: 106.6624 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1252: GFLOPs: 314.6247. Time: 106.6751 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1253: GFLOPs: 314.7368. Time: 106.6371 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1254: GFLOPs: 314.7715. Time: 106.6253 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1255: GFLOPs: 315.9367. Time: 106.2321 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1256: GFLOPs: 314.3153. Time: 106.7801 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1257: GFLOPs: 314.3678. Time: 106.7623 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1258: GFLOPs: 304.3725. Time: 110.2683 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1259: GFLOPs: 314.3210. Time: 106.7782 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1260: GFLOPs: 307.5750. Time: 109.1201 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1261: GFLOPs: 307.5871. Time: 109.1158 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1262: GFLOPs: 307.5349. Time: 109.1344 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1263: GFLOPs: 336.9438. Time: 99.6090 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1264: GFLOPs: 336.9830. Time: 99.5974 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1265: GFLOPs: 316.3286. Time: 106.1005 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1266: GFLOPs: 301.9074. Time: 111.1686 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1267: GFLOPs: 301.6184. Time: 111.2751 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1268: GFLOPs: 303.1766. Time: 110.7032 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1269: GFLOPs: 303.6279. Time: 110.5387 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1270: GFLOPs: 317.1154. Time: 105.8372 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1271: GFLOPs: 316.0831. Time: 106.1829 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1272: GFLOPs: 316.0730. Time: 106.1863 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1273: GFLOPs: 331.8063. Time: 101.1513 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1274: GFLOPs: 334.7062. Time: 100.2749 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1275: GFLOPs: 331.9348. Time: 101.1121 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1276: GFLOPs: 51.3415. Time: 653.7135 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1277: GFLOPs: 94.2706. Time: 356.0242 us. Best GFLOPs: 388.2341
2024-03-20 21:53:23 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1278: GFLOPs: 69.4194. Time: 483.4764 us. Best GFLOPs: 388.2341
2024-03-20 21:58:47 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:58:47 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:58:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1214 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2023 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2429 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2832 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3233 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3638 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:50 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-20 21:58:50 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:52 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:53 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:54 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 151 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 21:58:55 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1165  0.9315  0.9077  0.9048  0.8995  0.8970  0.8970  0.8959  0.8932  0.8932  0.8906  0.8897  0.8865  0.8809  0.8789  0.8753
[17 : 32]:	0.8715  0.8600  0.8549  0.8549  0.8529  0.8477  0.8469  0.8469  0.8469  0.8469  0.8469  0.8468  0.8460  0.8436  0.8436  0.8436
[33 : 48]:	0.8436  0.8436  0.8436  0.8412  0.8390  0.8390  0.8359  0.8293  0.8273  0.8267  0.8267  0.8169  0.8169  0.8157  0.8145  0.8127
[49 : 64]:	0.8060  0.8057  0.8037  0.8037  0.8031  0.8031  0.8007  0.7999  0.7977  0.7977  0.7977  0.7963  0.7963  0.7929  0.7929  0.7929
2024-03-20 21:58:55 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:58:55 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1279: GFLOPs: 77.1535. Time: 435.0110 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1280: GFLOPs: 374.9327. Time: 89.5164 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1281: GFLOPs: 354.5651. Time: 94.6586 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1282: GFLOPs: 364.9706. Time: 91.9598 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1283: GFLOPs: 347.2159. Time: 96.6621 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1284: GFLOPs: 353.4473. Time: 94.9579 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1285: GFLOPs: 353.2812. Time: 95.0026 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1286: GFLOPs: 349.1732. Time: 96.1203 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1287: GFLOPs: 364.9039. Time: 91.9766 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1288: GFLOPs: 364.8442. Time: 91.9917 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1289: GFLOPs: 343.3745. Time: 97.7435 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1290: GFLOPs: 346.6783. Time: 96.8120 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1291: GFLOPs: 344.8488. Time: 97.3256 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1292: GFLOPs: 341.6406. Time: 98.2396 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1293: GFLOPs: 343.0312. Time: 97.8413 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1294: GFLOPs: 340.2162. Time: 98.6509 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1295: GFLOPs: 320.1467. Time: 104.8351 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1296: GFLOPs: 329.1831. Time: 101.9573 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1297: GFLOPs: 321.2169. Time: 104.4859 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1298: GFLOPs: 321.3091. Time: 104.4559 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1299: GFLOPs: 321.1397. Time: 104.5110 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1300: GFLOPs: 326.5273. Time: 102.7866 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1301: GFLOPs: 327.1935. Time: 102.5773 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1302: GFLOPs: 327.3068. Time: 102.5418 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1303: GFLOPs: 327.2461. Time: 102.5608 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1304: GFLOPs: 327.9619. Time: 102.3370 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1305: GFLOPs: 327.9666. Time: 102.3355 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1306: GFLOPs: 326.4109. Time: 102.8232 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1307: GFLOPs: 327.2179. Time: 102.5696 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1308: GFLOPs: 329.9343. Time: 101.7252 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1309: GFLOPs: 327.9602. Time: 102.3375 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1310: GFLOPs: 327.8877. Time: 102.3601 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1311: GFLOPs: 328.7392. Time: 102.0950 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1312: GFLOPs: 327.2509. Time: 102.5593 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1313: GFLOPs: 327.3060. Time: 102.5420 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1314: GFLOPs: 316.1601. Time: 106.1571 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1315: GFLOPs: 225.3449. Time: 148.9389 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1316: GFLOPs: 225.3733. Time: 148.9201 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1317: GFLOPs: 219.0613. Time: 153.2111 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1318: GFLOPs: 325.1867. Time: 103.2103 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1319: GFLOPs: 321.8938. Time: 104.2661 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1320: GFLOPs: 324.1094. Time: 103.5534 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1321: GFLOPs: 324.1413. Time: 103.5432 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1322: GFLOPs: 306.0686. Time: 109.6572 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1323: GFLOPs: 305.8863. Time: 109.7225 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1324: GFLOPs: 305.7718. Time: 109.7636 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1325: GFLOPs: 309.8839. Time: 108.3071 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1326: GFLOPs: 306.4876. Time: 109.5073 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1327: GFLOPs: 312.0241. Time: 107.5642 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1328: GFLOPs: 306.4636. Time: 109.5159 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1329: GFLOPs: 306.6738. Time: 109.4408 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1330: GFLOPs: 306.5991. Time: 109.4675 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1331: GFLOPs: 310.7518. Time: 108.0046 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1332: GFLOPs: 311.9709. Time: 107.5826 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1333: GFLOPs: 222.1160. Time: 151.1041 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1334: GFLOPs: 308.8762. Time: 108.6605 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1335: GFLOPs: 312.0097. Time: 107.5692 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1336: GFLOPs: 312.0375. Time: 107.5596 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1337: GFLOPs: 312.0158. Time: 107.5671 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1338: GFLOPs: 309.7001. Time: 108.3714 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1339: GFLOPs: 309.7585. Time: 108.3509 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1340: GFLOPs: 93.1036. Time: 360.4868 us. Best GFLOPs: 388.2341
2024-03-20 21:59:48 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1341: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 16, 256, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 256], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 256, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:04:06 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:04:06 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:04:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 408 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1215 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1622 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2027 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2433 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2836 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3237 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3643 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4048 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:09 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-20 22:04:10 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:11 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:12 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:14 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:04:14 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9335  0.9249  0.9199  0.9179  0.9123  0.9042  0.8986  0.8966  0.8933  0.8811  0.8787  0.8753  0.8576  0.8557  0.8513  0.8489
[17 : 32]:	0.8443  0.8443  0.8434  0.8434  0.8433  0.8433  0.8433  0.8433  0.8433  0.8421  0.8418  0.8333  0.8295  0.8288  0.8264  0.8184
[33 : 48]:	0.8141  0.8141  0.8126  0.8112  0.8112  0.8111  0.8109  0.8095  0.8095  0.8095  0.8093  0.8089  0.8088  0.8078  0.8059  0.8059
[49 : 64]:	0.8054  0.8054  0.8053  0.8053  0.8044  0.8044  0.8043  0.8043  0.8043  0.8043  0.8043  0.8039  0.8039  0.8029  0.8009  0.7989
2024-03-20 22:04:14 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:04:14 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1342: GFLOPs: 365.1971. Time: 91.9028 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1343: GFLOPs: 367.0910. Time: 91.4286 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1344: GFLOPs: 366.9764. Time: 91.4572 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1345: GFLOPs: 366.9510. Time: 91.4635 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1346: GFLOPs: 346.6609. Time: 96.8169 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1347: GFLOPs: 363.1080. Time: 92.4315 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1348: GFLOPs: 351.6653. Time: 95.4391 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1349: GFLOPs: 343.1106. Time: 97.8187 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1350: GFLOPs: 341.4102. Time: 98.3059 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1351: GFLOPs: 340.9006. Time: 98.4528 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1352: GFLOPs: 342.2270. Time: 98.0712 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1353: GFLOPs: 339.1898. Time: 98.9494 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1354: GFLOPs: 311.2412. Time: 107.8348 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1355: GFLOPs: 318.4060. Time: 105.4083 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1356: GFLOPs: 329.8744. Time: 101.7436 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1357: GFLOPs: 329.9675. Time: 101.7149 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1358: GFLOPs: 327.1379. Time: 102.5947 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1359: GFLOPs: 328.5798. Time: 102.1445 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1360: GFLOPs: 327.0984. Time: 102.6071 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1361: GFLOPs: 327.1137. Time: 102.6023 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1362: GFLOPs: 327.3016. Time: 102.5434 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1363: GFLOPs: 329.7438. Time: 101.7840 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1364: GFLOPs: 327.0709. Time: 102.6157 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1365: GFLOPs: 328.3587. Time: 102.2133 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1366: GFLOPs: 328.4527. Time: 102.1840 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1367: GFLOPs: 327.8763. Time: 102.3637 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1368: GFLOPs: 324.3670. Time: 103.4711 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1369: GFLOPs: 314.7320. Time: 106.6388 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1370: GFLOPs: 321.5188. Time: 104.3878 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1371: GFLOPs: 324.1145. Time: 103.5518 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1372: GFLOPs: 321.5159. Time: 104.3887 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1373: GFLOPs: 307.5882. Time: 109.1154 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1374: GFLOPs: 305.7615. Time: 109.7673 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1375: GFLOPs: 305.6193. Time: 109.8184 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1376: GFLOPs: 76.1408. Time: 440.7968 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1377: GFLOPs: 307.2880. Time: 109.2220 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1378: GFLOPs: 306.4047. Time: 109.5369 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1379: GFLOPs: 307.7484. Time: 109.0587 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1380: GFLOPs: 310.0155. Time: 108.2611 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1381: GFLOPs: 313.1670. Time: 107.1717 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1382: GFLOPs: 313.1353. Time: 107.1825 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1383: GFLOPs: 311.6951. Time: 107.6777 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1384: GFLOPs: 304.4563. Time: 110.2379 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1385: GFLOPs: 312.3341. Time: 107.4574 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1386: GFLOPs: 305.0708. Time: 110.0159 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1387: GFLOPs: 310.0111. Time: 108.2627 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1388: GFLOPs: 310.0404. Time: 108.2524 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1389: GFLOPs: 309.9758. Time: 108.2750 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1390: GFLOPs: 310.0659. Time: 108.2435 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1391: GFLOPs: 310.0473. Time: 108.2500 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1392: GFLOPs: 312.6728. Time: 107.3411 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1393: GFLOPs: 312.6771. Time: 107.3396 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1394: GFLOPs: 312.6801. Time: 107.3385 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1395: GFLOPs: 312.7054. Time: 107.3298 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1396: GFLOPs: 311.3577. Time: 107.7944 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1397: GFLOPs: 311.3354. Time: 107.8021 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1398: GFLOPs: 311.3897. Time: 107.7833 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1399: GFLOPs: 312.4008. Time: 107.4345 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1400: GFLOPs: 312.3475. Time: 107.4528 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1401: GFLOPs: 312.2752. Time: 107.4777 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1402: GFLOPs: 312.2517. Time: 107.4858 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1403: GFLOPs: 11.2380. Time: 2986.5260 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1404: GFLOPs: 21.3197. Time: 1574.2559 us. Best GFLOPs: 388.2341
2024-03-20 22:05:07 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1405: GFLOPs: 13.5688. Time: 2473.5094 us. Best GFLOPs: 388.2341
2024-03-20 22:07:39 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:07:39 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:07:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1212 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1620 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2022 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2429 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2830 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3235 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3640 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:42 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 22:07:42 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 148 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:43 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 154 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:45 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 137 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:07:47 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9411  0.9300  0.9298  0.9261  0.9217  0.9172  0.9163  0.9152  0.9131  0.9073  0.9036  0.9030  0.8927  0.8914  0.8914  0.8875
[17 : 32]:	0.8872  0.8858  0.8846  0.8795  0.8784  0.8782  0.8777  0.8775  0.8707  0.8447  0.8443  0.8441  0.8428  0.8426  0.8415  0.8415
[33 : 48]:	0.8408  0.8395  0.8395  0.8395  0.8313  0.8313  0.8313  0.8304  0.8292  0.8289  0.8229  0.8153  0.8150  0.8140  0.8100  0.8099
[49 : 64]:	0.8086  0.8079  0.8077  0.8062  0.8060  0.8040  0.8037  0.8027  0.8020  0.8020  0.8006  0.7980  0.7979  0.7936  0.7884  0.7823
2024-03-20 22:07:47 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:07:47 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1406: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 2, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1407: GFLOPs: 355.7492. Time: 94.3435 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1408: GFLOPs: 357.7655. Time: 93.8118 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1409: GFLOPs: 357.3374. Time: 93.9242 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1410: GFLOPs: 357.0245. Time: 94.0065 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1411: GFLOPs: 355.3679. Time: 94.4447 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1412: GFLOPs: 351.9311. Time: 95.3670 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1413: GFLOPs: 355.4580. Time: 94.4208 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1414: GFLOPs: 355.4994. Time: 94.4098 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1415: GFLOPs: 352.6509. Time: 95.1724 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1416: GFLOPs: 352.6809. Time: 95.1643 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1417: GFLOPs: 353.9030. Time: 94.8357 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1418: GFLOPs: 364.3486. Time: 92.1168 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1419: GFLOPs: 356.0401. Time: 94.2664 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1420: GFLOPs: 355.9986. Time: 94.2774 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1421: GFLOPs: 334.9771. Time: 100.1938 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1422: GFLOPs: 339.9522. Time: 98.7275 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1423: GFLOPs: 335.2782. Time: 100.1038 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1424: GFLOPs: 1.9737. Time: 17005.0557 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1425: GFLOPs: 333.6843. Time: 100.5820 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1426: GFLOPs: 350.7675. Time: 95.6834 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1427: GFLOPs: 350.7485. Time: 95.6886 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1428: GFLOPs: 364.2471. Time: 92.1425 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1429: GFLOPs: 364.1500. Time: 92.1670 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1430: GFLOPs: 333.8329. Time: 100.5372 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1431: GFLOPs: 329.1128. Time: 101.9791 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1432: GFLOPs: 326.5949. Time: 102.7653 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1433: GFLOPs: 327.7852. Time: 102.3921 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1434: GFLOPs: 327.4761. Time: 102.4888 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1435: GFLOPs: 327.4608. Time: 102.4936 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1436: GFLOPs: 327.8768. Time: 102.3635 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1437: GFLOPs: 327.8471. Time: 102.3728 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1438: GFLOPs: 311.3686. Time: 107.7907 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1439: GFLOPs: 327.4864. Time: 102.4856 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1440: GFLOPs: 328.6324. Time: 102.1282 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1441: GFLOPs: 327.5152. Time: 102.4765 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1442: GFLOPs: 324.1066. Time: 103.5543 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1443: GFLOPs: 323.3501. Time: 103.7965 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1444: GFLOPs: 323.3180. Time: 103.8068 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1445: GFLOPs: 323.9864. Time: 103.5927 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1446: GFLOPs: 267.9214. Time: 125.2704 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1447: GFLOPs: 268.9985. Time: 124.7688 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1448: GFLOPs: 268.6334. Time: 124.9384 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1449: GFLOPs: 267.8231. Time: 125.3164 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1450: GFLOPs: 268.6420. Time: 124.9344 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1451: GFLOPs: 316.2091. Time: 106.1406 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1452: GFLOPs: 316.1800. Time: 106.1504 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1453: GFLOPs: 316.2469. Time: 106.1279 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1454: GFLOPs: 316.7173. Time: 105.9703 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1455: GFLOPs: 314.1239. Time: 106.8452 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1456: GFLOPs: 9.0190. Time: 3721.3299 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1457: GFLOPs: 313.2188. Time: 107.1539 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1458: GFLOPs: 316.6854. Time: 105.9810 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1459: GFLOPs: 314.0360. Time: 106.8751 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1460: GFLOPs: 307.6845. Time: 109.0813 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1461: GFLOPs: 303.2357. Time: 110.6816 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1462: GFLOPs: 301.5504. Time: 111.3002 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1463: GFLOPs: 301.5430. Time: 111.3029 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1464: GFLOPs: 312.4745. Time: 107.4092 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1465: GFLOPs: 314.1961. Time: 106.8206 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1466: GFLOPs: 269.0645. Time: 124.7382 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1467: GFLOPs: 16.5849. Time: 2023.6883 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1468: GFLOPs: 26.8381. Time: 1250.5600 us. Best GFLOPs: 388.2341
2024-03-20 22:08:44 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1469: GFLOPs: 109.8166. Time: 305.6243 us. Best GFLOPs: 388.2341
2024-03-20 22:11:32 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:11:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:11:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 812 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1622 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2026 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2431 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2835 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3241 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3647 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4051 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4454 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:36 [INFO] [evolutionary_search.cc:723] Sampled 56 candidate(s)
2024-03-20 22:11:36 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 168 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:37 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:39 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:40 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:11:41 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9199  0.9125  0.9092  0.9092  0.9003  0.9001  0.8984  0.8983  0.8843  0.8486  0.8479  0.8479  0.8477  0.8472  0.8465  0.8410
[17 : 32]:	0.8291  0.8210  0.8210  0.8120  0.8112  0.8110  0.8108  0.8100  0.8056  0.8039  0.7927  0.7927  0.7902  0.7892  0.7883  0.7740
[33 : 48]:	0.7730  0.7726  0.7668  0.7668  0.7668  0.7668  0.7668  0.7627  0.7619  0.7533  0.7528  0.7528  0.7528  0.7511  0.7511  0.7506
[49 : 64]:	0.7506  0.7506  0.7486  0.7486  0.7479  0.7479  0.7479  0.7468  0.7464  0.7462  0.7457  0.7427  0.7365  0.7311  0.7311  0.7311
2024-03-20 22:11:41 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:11:41 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1470: GFLOPs: 357.4712. Time: 93.8890 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1471: GFLOPs: 368.1616. Time: 91.1628 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1472: GFLOPs: 368.1842. Time: 91.1571 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1473: GFLOPs: 369.3563. Time: 90.8679 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1474: GFLOPs: 365.0820. Time: 91.9317 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1475: GFLOPs: 355.7629. Time: 94.3399 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1476: GFLOPs: 345.3890. Time: 97.1734 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1477: GFLOPs: 345.3163. Time: 97.1939 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1478: GFLOPs: 343.4807. Time: 97.7133 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1479: GFLOPs: 328.7429. Time: 102.0938 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1480: GFLOPs: 329.0836. Time: 101.9882 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1481: GFLOPs: 322.3285. Time: 104.1255 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1482: GFLOPs: 329.8429. Time: 101.7534 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1483: GFLOPs: 327.7621. Time: 102.3993 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1484: GFLOPs: 329.3894. Time: 101.8935 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1485: GFLOPs: 321.8675. Time: 104.2747 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1486: GFLOPs: 321.8770. Time: 104.2716 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1487: GFLOPs: 318.3065. Time: 105.4412 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1488: GFLOPs: 318.1367. Time: 105.4975 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1489: GFLOPs: 306.6755. Time: 109.4402 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1490: GFLOPs: 309.6307. Time: 108.3957 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1491: GFLOPs: 312.8820. Time: 107.2693 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1492: GFLOPs: 312.0534. Time: 107.5541 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1493: GFLOPs: 311.9718. Time: 107.5822 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1494: GFLOPs: 310.8226. Time: 107.9800 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1495: GFLOPs: 308.7608. Time: 108.7010 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1496: GFLOPs: 302.4126. Time: 110.9829 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1497: GFLOPs: 302.7500. Time: 110.8592 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1498: GFLOPs: 302.4908. Time: 110.9542 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1499: GFLOPs: 302.7389. Time: 110.8633 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1500: GFLOPs: 302.4338. Time: 110.9751 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1501: GFLOPs: 302.4335. Time: 110.9752 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1502: GFLOPs: 302.7905. Time: 110.8444 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1503: GFLOPs: 303.8846. Time: 110.4453 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1504: GFLOPs: 285.7276. Time: 117.4637 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1505: GFLOPs: 285.1367. Time: 117.7071 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1506: GFLOPs: 285.5978. Time: 117.5171 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1507: GFLOPs: 286.3670. Time: 117.2014 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1508: GFLOPs: 285.4835. Time: 117.5641 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1509: GFLOPs: 285.1364. Time: 117.7072 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1510: GFLOPs: 285.1475. Time: 117.7027 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1511: GFLOPs: 285.8798. Time: 117.4012 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1512: GFLOPs: 285.8336. Time: 117.4201 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1513: GFLOPs: 285.8315. Time: 117.4210 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1514: GFLOPs: 285.8058. Time: 117.4316 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1515: GFLOPs: 286.3809. Time: 117.1958 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1516: GFLOPs: 286.3048. Time: 117.2269 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1517: GFLOPs: 286.3251. Time: 117.2186 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1518: GFLOPs: 286.2939. Time: 117.2313 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1519: GFLOPs: 286.3826. Time: 117.1951 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1520: GFLOPs: 285.8185. Time: 117.4263 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1521: GFLOPs: 285.8338. Time: 117.4201 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1522: GFLOPs: 285.7829. Time: 117.4410 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1523: GFLOPs: 285.8348. Time: 117.4197 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1524: GFLOPs: 285.8591. Time: 117.4097 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1525: GFLOPs: 286.3586. Time: 117.2049 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1526: GFLOPs: 286.3236. Time: 117.2192 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1527: GFLOPs: 286.3030. Time: 117.2276 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1528: GFLOPs: 286.2850. Time: 117.2350 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1529: GFLOPs: 286.3406. Time: 117.2122 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1530: GFLOPs: 285.1635. Time: 117.6961 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1531: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  312: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  311: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  310: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  309: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  308: tvm::transform::Pass::operator()(tvm::IRModule) const
  307: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  306: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  305: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  304: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  303: _ZN3tvm7runtime13PackedFuncObj
  302: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  301: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  300: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  299: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  298: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  297: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  296: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  295: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  294: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  269: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  268: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  267: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  266: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  265: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  264: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  263: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  262: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  261: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  260: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  259: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  258: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  257: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  256: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  255: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  254: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  253: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  252: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  251: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  250: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  249: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  248: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  247: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  246: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  245: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  244: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  243: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  242: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  241: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  240: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  239: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  238: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  237: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  236: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  235: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  234: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  233: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  232: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  231: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  230: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  229: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  228: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  227: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  226: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  225: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  224: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  223: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  222: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  221: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  220: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  219: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  218: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  217: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  216: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  215: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  214: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  213: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  212: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  211: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  210: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  209: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  208: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  207: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  206: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  205: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  204: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  202: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  201: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  200: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  199: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  198: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  197: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  196: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  195: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  194: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  193: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  192: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  191: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  190: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  189: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  188: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  183: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  182: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  181: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  180: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  179: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  178: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  176: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  175: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  173: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  172: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  171: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  170: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  169: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  168: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  167: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  166: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  165: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  164: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  163: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  162: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  161: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  160: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  159: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  158: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  157: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  156: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  154: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  153: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  152: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  151: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  150: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  149: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  148: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  147: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  146: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  145: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  144: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  143: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  142: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  141: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  140: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  139: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  138: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  137: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  136: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  135: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  134: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  133: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  132: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  131: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  130: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  117: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  116: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  115: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  114: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  113: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  112: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  111: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  110: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  109: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  108: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  107: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  106: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  105: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  104: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  82: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  81: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  80: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  79: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  78: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  77: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  76: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  75: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 64, 1, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1532: GFLOPs: 11.4036. Time: 2943.1567 us. Best GFLOPs: 388.2341
2024-03-20 22:12:35 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1533: GFLOPs: 6.1443. Time: 5462.3814 us. Best GFLOPs: 388.2341
2024-03-20 22:16:42 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:16:42 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:16:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 805 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1619 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2025 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2428 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2834 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3238 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3642 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4044 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:45 [INFO] [evolutionary_search.cc:723] Sampled 56 candidate(s)
2024-03-20 22:16:46 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 152 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:47 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:48 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:49 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:16:50 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9279  0.9187  0.9136  0.9058  0.9058  0.9058  0.9000  0.9000  0.8983  0.8880  0.8852  0.8825  0.8819  0.8480  0.8469  0.8452
[17 : 32]:	0.8442  0.8306  0.8284  0.8277  0.8275  0.8271  0.8247  0.8146  0.8118  0.8091  0.8071  0.8069  0.8048  0.8017  0.8012  0.7954
[33 : 48]:	0.7865  0.7851  0.7818  0.7818  0.7818  0.7818  0.7818  0.7809  0.7809  0.7809  0.7766  0.7755  0.7412  0.7409  0.7396  0.7387
[49 : 64]:	0.7267  0.7251  0.7234  0.7192  0.7190  0.7178  0.7178  0.7159  0.7134  0.7133  0.7123  0.7123  0.7003  0.7003  0.7003  0.6976
2024-03-20 22:16:50 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:16:50 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1534: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 4, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1535: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 1, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1536: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 16, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1537: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 16, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1538: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(256), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 256, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1539: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 4, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1540: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 128, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1541: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 16, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1542: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1543: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1544: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 64, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1545: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1546: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1547: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1548: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 8, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1549: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1550: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1551: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1552: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 16, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1553: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1554: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1555: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1556: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 16, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1557: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 1, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1558: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(256), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 256, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1559: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(256), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 256, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1560: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 64, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1561: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 8, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1562: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1563: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 128, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1564: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(256), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 256, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1565: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 4, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1566: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 128, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1567: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 4, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1568: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 128, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1569: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 128, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1570: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 16, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1571: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 4, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1572: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 16, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1573: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 16, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1574: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 64, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1575: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 128, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1576: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 4, 64])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1577: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(256) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[16, 8, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1578: GFLOPs: 262.5370. Time: 127.8396 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1579: GFLOPs: 265.9806. Time: 126.1845 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1580: GFLOPs: 265.7677. Time: 126.2856 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1581: GFLOPs: 266.0417. Time: 126.1555 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1582: GFLOPs: 268.9577. Time: 124.7878 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1583: GFLOPs: 221.7620. Time: 151.3452 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1584: GFLOPs: 268.8243. Time: 124.8496 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1585: GFLOPs: 265.2340. Time: 126.5397 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1586: GFLOPs: 261.4567. Time: 128.3678 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1587: GFLOPs: 265.6760. Time: 126.3291 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1588: GFLOPs: 265.6705. Time: 126.3318 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1589: GFLOPs: 265.2617. Time: 126.5264 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1590: GFLOPs: 269.1170. Time: 124.7139 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1591: GFLOPs: 268.7780. Time: 124.8712 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1592: GFLOPs: 265.6640. Time: 126.3348 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1593: GFLOPs: 265.6543. Time: 126.3395 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1594: GFLOPs: 265.3936. Time: 126.4636 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1595: GFLOPs: 27.8820. Time: 1203.7363 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1596: GFLOPs: 14.1766. Time: 2367.4641 us. Best GFLOPs: 388.2341
2024-03-20 22:17:15 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1597: GFLOPs: 24.4958. Time: 1370.1383 us. Best GFLOPs: 388.2341
2024-03-20 22:22:24 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:22:24 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:22:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1212 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1616 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2023 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2428 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2833 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3239 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3645 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4052 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4455 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:27 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-03-20 22:22:28 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:29 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:30 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:31 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:22:32 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9244  0.9209  0.9175  0.9126  0.9099  0.9074  0.9054  0.9029  0.9020  0.8943  0.8922  0.8909  0.8902  0.8860  0.8831  0.8488
[17 : 32]:	0.8476  0.8476  0.8461  0.8461  0.8456  0.8449  0.8362  0.8343  0.8292  0.8271  0.8248  0.8173  0.8135  0.8052  0.8041  0.8028
[33 : 48]:	0.8028  0.7960  0.7939  0.7836  0.7836  0.7833  0.7833  0.7820  0.7820  0.7791  0.7766  0.7766  0.7766  0.7397  0.7325  0.7297
[49 : 64]:	0.7297  0.7289  0.7258  0.7258  0.7246  0.7246  0.7170  0.7162  0.7151  0.7135  0.7000  0.6995  0.6992  0.6980  0.6975  0.6946
2024-03-20 22:22:32 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:22:32 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1598: GFLOPs: 361.6456. Time: 92.8053 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1599: GFLOPs: 369.2149. Time: 90.9027 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1600: GFLOPs: 368.2514. Time: 91.1405 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1601: GFLOPs: 363.3244. Time: 92.3765 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1602: GFLOPs: 352.0470. Time: 95.3356 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1603: GFLOPs: 349.0643. Time: 96.1503 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1604: GFLOPs: 363.2379. Time: 92.3985 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1605: GFLOPs: 351.9382. Time: 95.3651 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1606: GFLOPs: 352.8933. Time: 95.1070 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1607: GFLOPs: 345.8971. Time: 97.0306 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1608: GFLOPs: 340.6017. Time: 98.5392 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1609: GFLOPs: 343.3880. Time: 97.7396 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1610: GFLOPs: 343.5861. Time: 97.6833 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1611: GFLOPs: 342.6328. Time: 97.9551 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1612: GFLOPs: 344.8148. Time: 97.3352 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1613: GFLOPs: 321.6703. Time: 104.3386 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1614: GFLOPs: 329.2202. Time: 101.9458 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1615: GFLOPs: 329.1522. Time: 101.9669 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1616: GFLOPs: 327.4578. Time: 102.4945 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1617: GFLOPs: 327.4505. Time: 102.4968 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1618: GFLOPs: 329.2272. Time: 101.9437 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1619: GFLOPs: 328.2917. Time: 102.2342 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1620: GFLOPs: 139.7427. Time: 240.1744 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1621: GFLOPs: 317.2108. Time: 105.8054 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1622: GFLOPs: 271.4506. Time: 123.6417 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1623: GFLOPs: 324.8624. Time: 103.3133 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1624: GFLOPs: 73.8348. Time: 454.5637 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1625: GFLOPs: 307.1972. Time: 109.2543 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1626: GFLOPs: 305.6938. Time: 109.7917 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1627: GFLOPs: 307.5804. Time: 109.1182 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1628: GFLOPs: 313.6290. Time: 107.0138 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1629: GFLOPs: 310.9695. Time: 107.9290 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1630: GFLOPs: 311.0209. Time: 107.9112 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1631: GFLOPs: 73.5787. Time: 456.1457 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1632: GFLOPs: 304.0008. Time: 110.4031 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1633: GFLOPs: 303.6626. Time: 110.5260 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1634: GFLOPs: 303.9987. Time: 110.4038 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1635: GFLOPs: 303.7024. Time: 110.5116 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1636: GFLOPs: 303.9985. Time: 110.4039 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1637: GFLOPs: 303.4853. Time: 110.5906 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1638: GFLOPs: 303.4604. Time: 110.5997 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1639: GFLOPs: 302.1251. Time: 111.0885 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1640: GFLOPs: 303.4853. Time: 110.5906 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1641: GFLOPs: 303.4911. Time: 110.5885 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1642: GFLOPs: 303.4549. Time: 110.6017 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1643: GFLOPs: 225.3606. Time: 148.9286 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1644: GFLOPs: 285.6120. Time: 117.5113 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1645: GFLOPs: 285.1372. Time: 117.7069 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1646: GFLOPs: 284.9494. Time: 117.7845 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1647: GFLOPs: 285.5330. Time: 117.5438 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1648: GFLOPs: 286.0575. Time: 117.3283 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1649: GFLOPs: 286.0450. Time: 117.3334 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1650: GFLOPs: 284.9599. Time: 117.7802 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1651: GFLOPs: 284.9943. Time: 117.7660 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1652: GFLOPs: 300.2618. Time: 111.7779 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1653: GFLOPs: 300.5324. Time: 111.6772 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1654: GFLOPs: 299.6325. Time: 112.0126 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1655: GFLOPs: 288.4092. Time: 116.3716 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1656: GFLOPs: 286.2352. Time: 117.2554 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1657: GFLOPs: 286.2704. Time: 117.2410 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1658: GFLOPs: 288.3780. Time: 116.3841 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1659: GFLOPs: 71.3877. Time: 470.1458 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1660: GFLOPs: 5.4707. Time: 6134.9649 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1661: GFLOPs: 3.1837. Time: 10542.1699 us. Best GFLOPs: 388.2341
2024-03-20 22:23:29 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:23:29 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:23:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 803 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1613 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2021 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2424 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2829 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3231 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3638 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:32 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-20 22:23:33 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 168 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:34 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:35 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:36 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 165 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:23:37 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9210  0.9209  0.8979  0.8968  0.8937  0.8525  0.8471  0.8454  0.8395  0.8123  0.8106  0.8085  0.8067  0.8067  0.8047  0.8047
[17 : 32]:	0.8029  0.8029  0.7947  0.7939  0.7939  0.7939  0.7877  0.7336  0.7323  0.7312  0.7296  0.7296  0.7261  0.7257  0.7246  0.7242
[33 : 48]:	0.7242  0.7240  0.7240  0.7234  0.7234  0.7234  0.7224  0.7224  0.7218  0.7218  0.7217  0.7185  0.7145  0.7145  0.7144  0.7129
[49 : 64]:	0.7102  0.7085  0.7085  0.7074  0.7069  0.7068  0.7059  0.7059  0.7049  0.7013  0.6997  0.6992  0.6967  0.6960  0.6960  0.6957
2024-03-20 22:23:37 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:23:37 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1662: GFLOPs: 362.5052. Time: 92.5852 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1663: GFLOPs: 367.8897. Time: 91.2301 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1664: GFLOPs: 362.4955. Time: 92.5877 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1665: GFLOPs: 350.8390. Time: 95.6639 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1666: GFLOPs: 344.1845. Time: 97.5135 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1667: GFLOPs: 325.8328. Time: 103.0057 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1668: GFLOPs: 327.9527. Time: 102.3398 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1669: GFLOPs: 328.2622. Time: 102.2433 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1670: GFLOPs: 326.7701. Time: 102.7102 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1671: GFLOPs: 307.7785. Time: 109.0480 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1672: GFLOPs: 304.6349. Time: 110.1733 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1673: GFLOPs: 312.6768. Time: 107.3397 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1674: GFLOPs: 312.8032. Time: 107.2963 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1675: GFLOPs: 312.3381. Time: 107.4561 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1676: GFLOPs: 313.3603. Time: 107.1055 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1677: GFLOPs: 313.3868. Time: 107.0965 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1678: GFLOPs: 310.0699. Time: 108.2421 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1679: GFLOPs: 310.0668. Time: 108.2432 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1680: GFLOPs: 302.5970. Time: 110.9153 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1681: GFLOPs: 303.4757. Time: 110.5941 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1682: GFLOPs: 303.4815. Time: 110.5920 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1683: GFLOPs: 303.4680. Time: 110.5969 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1684: GFLOPs: 303.4973. Time: 110.5862 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1685: GFLOPs: 285.8494. Time: 117.4136 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1686: GFLOPs: 284.7782. Time: 117.8553 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1687: GFLOPs: 285.5559. Time: 117.5343 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1688: GFLOPs: 284.9168. Time: 117.7980 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1689: GFLOPs: 284.9384. Time: 117.7890 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1690: GFLOPs: 284.9440. Time: 117.7867 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1691: GFLOPs: 286.0212. Time: 117.3431 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1692: GFLOPs: 284.9080. Time: 117.8016 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1693: GFLOPs: 284.9278. Time: 117.7934 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1694: GFLOPs: 284.7558. Time: 117.8646 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1695: GFLOPs: 285.5432. Time: 117.5396 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1696: GFLOPs: 285.5307. Time: 117.5447 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1697: GFLOPs: 286.0419. Time: 117.3346 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1698: GFLOPs: 286.0810. Time: 117.3186 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1699: GFLOPs: 286.0158. Time: 117.3453 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1700: GFLOPs: 285.5470. Time: 117.5380 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1701: GFLOPs: 285.5457. Time: 117.5386 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1702: GFLOPs: 286.0434. Time: 117.3340 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1703: GFLOPs: 286.0485. Time: 117.3319 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1704: GFLOPs: 285.5597. Time: 117.5328 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1705: GFLOPs: 281.0748. Time: 119.4082 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1706: GFLOPs: 286.2678. Time: 117.2421 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1707: GFLOPs: 286.2424. Time: 117.2524 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1708: GFLOPs: 281.0114. Time: 119.4351 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1709: GFLOPs: 287.2193. Time: 116.8537 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1710: GFLOPs: 299.8344. Time: 111.9372 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1711: GFLOPs: 297.4402. Time: 112.8382 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1712: GFLOPs: 297.4808. Time: 112.8228 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1713: GFLOPs: 297.1019. Time: 112.9667 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1714: GFLOPs: 297.3760. Time: 112.8626 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1715: GFLOPs: 286.2478. Time: 117.2502 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1716: GFLOPs: 280.9811. Time: 119.4480 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1717: GFLOPs: 281.0051. Time: 119.4378 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1718: GFLOPs: 287.2100. Time: 116.8574 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1719: GFLOPs: 273.8663. Time: 122.5511 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1720: GFLOPs: 272.6071. Time: 123.1172 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1721: GFLOPs: 276.2470. Time: 121.4950 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1722: GFLOPs: 276.2866. Time: 121.4776 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1723: GFLOPs: 85.2313. Time: 393.7829 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1724: GFLOPs: 64.0281. Time: 524.1861 us. Best GFLOPs: 388.2341
2024-03-20 22:24:32 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1725: GFLOPs: 163.8810. Time: 204.7987 us. Best GFLOPs: 388.2341
2024-03-20 22:28:51 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:28:51 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:28:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 801 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1201 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1607 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2012 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2417 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2823 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3225 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:54 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-03-20 22:28:54 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 189 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:56 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:57 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 149 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:58 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:28:59 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9261  0.9119  0.9071  0.8982  0.8836  0.8531  0.8451  0.8444  0.8328  0.8069  0.8056  0.8055  0.8052  0.7987  0.7840  0.7840
[17 : 32]:	0.7377  0.7348  0.7348  0.7288  0.7275  0.7232  0.7230  0.7219  0.7218  0.7210  0.7196  0.7196  0.7194  0.7116  0.7097  0.7095
[33 : 48]:	0.7080  0.7077  0.7062  0.7051  0.7024  0.7024  0.7007  0.6966  0.6933  0.6925  0.6922  0.6920  0.6909  0.6903  0.6831  0.6791
[49 : 64]:	0.6728  0.6696  0.6680  0.6609  0.6579  0.6367  0.6298  0.6261  0.6211  0.6124  0.6027  0.5979  0.5975  0.5946  0.5828  0.5792
2024-03-20 22:28:59 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:28:59 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1726: GFLOPs: 370.7179. Time: 90.5341 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1727: GFLOPs: 368.9009. Time: 90.9801 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1728: GFLOPs: 355.0232. Time: 94.5364 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1729: GFLOPs: 351.1004. Time: 95.5927 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1730: GFLOPs: 346.6404. Time: 96.8226 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1731: GFLOPs: 329.1893. Time: 101.9554 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1732: GFLOPs: 328.0247. Time: 102.3174 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1733: GFLOPs: 327.2126. Time: 102.5713 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1734: GFLOPs: 323.7443. Time: 103.6702 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1735: GFLOPs: 305.3937. Time: 109.8995 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1736: GFLOPs: 309.5146. Time: 108.4363 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1737: GFLOPs: 305.7521. Time: 109.7707 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1738: GFLOPs: 308.4196. Time: 108.8213 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1739: GFLOPs: 307.8534. Time: 109.0215 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1740: GFLOPs: 303.6489. Time: 110.5310 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1741: GFLOPs: 303.0667. Time: 110.7434 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1742: GFLOPs: 286.5584. Time: 117.1232 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1743: GFLOPs: 287.2760. Time: 116.8306 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1744: GFLOPs: 287.2857. Time: 116.8267 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1745: GFLOPs: 287.3100. Time: 116.8168 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1746: GFLOPs: 282.7118. Time: 118.7167 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1747: GFLOPs: 287.7488. Time: 116.6386 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1748: GFLOPs: 286.5564. Time: 117.1240 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1749: GFLOPs: 284.0763. Time: 118.1465 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1750: GFLOPs: 282.7299. Time: 118.7092 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1751: GFLOPs: 283.9991. Time: 118.1786 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1752: GFLOPs: 82.0363. Time: 409.1193 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1753: GFLOPs: 82.0254. Time: 409.1738 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1754: GFLOPs: 283.9778. Time: 118.1875 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1755: GFLOPs: 286.2642. Time: 117.2435 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1756: GFLOPs: 272.8038. Time: 123.0284 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1757: GFLOPs: 82.0019. Time: 409.2907 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1758: GFLOPs: 278.8948. Time: 120.3415 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1759: GFLOPs: 278.9208. Time: 120.3303 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1760: GFLOPs: 275.5512. Time: 121.8018 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1761: GFLOPs: 275.5439. Time: 121.8050 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1762: GFLOPs: 268.5755. Time: 124.9653 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1763: GFLOPs: 278.8527. Time: 120.3597 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1764: GFLOPs: 268.6147. Time: 124.9471 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1765: GFLOPs: 278.9083. Time: 120.3357 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1766: GFLOPs: 268.6421. Time: 124.9344 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1767: GFLOPs: 277.6978. Time: 120.8602 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1768: GFLOPs: 268.6472. Time: 124.9320 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1769: GFLOPs: 277.8228. Time: 120.8059 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1770: GFLOPs: 278.1491. Time: 120.6641 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1771: GFLOPs: 278.1707. Time: 120.6548 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1772: GFLOPs: 272.7622. Time: 123.0472 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1773: GFLOPs: 151.9872. Time: 220.8254 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1774: GFLOPs: 81.9946. Time: 409.3271 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1775: GFLOPs: 274.4695. Time: 122.2818 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1776: GFLOPs: 81.4018. Time: 412.3080 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1777: GFLOPs: 271.2315. Time: 123.7416 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1778: GFLOPs: 81.3615. Time: 412.5122 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1779: GFLOPs: 79.9882. Time: 419.5948 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1780: GFLOPs: 80.1100. Time: 418.9566 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1781: GFLOPs: 81.3894. Time: 412.3708 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1782: GFLOPs: 81.3708. Time: 412.4651 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1783: GFLOPs: 220.1682. Time: 152.4408 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1784: GFLOPs: 219.7586. Time: 152.7250 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1785: GFLOPs: 296.1718. Time: 113.3215 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1786: GFLOPs: 79.9796. Time: 419.6399 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1787: GFLOPs: 105.4655. Time: 318.2333 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1788: GFLOPs: 57.8691. Time: 579.9747 us. Best GFLOPs: 388.2341
2024-03-20 22:29:54 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1789: GFLOPs: 47.2740. Time: 709.9588 us. Best GFLOPs: 388.2341
2024-03-20 22:37:25 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:37:25 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:37:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 814 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1221 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1628 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2032 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2440 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2848 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3255 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3661 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4066 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4471 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4874 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 5277 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:29 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-20 22:37:29 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 164 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:31 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 179 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:33 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 148 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:37:34 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9264  0.9264  0.9159  0.9053  0.9050  0.9016  0.8534  0.8453  0.8450  0.8345  0.8285  0.8036  0.8019  0.8016  0.8014  0.7828
[17 : 32]:	0.7800  0.7799  0.7432  0.7382  0.7363  0.7345  0.7344  0.7336  0.7329  0.7318  0.7271  0.7271  0.7264  0.7232  0.7090  0.7090
[33 : 48]:	0.7090  0.7087  0.7087  0.7061  0.7048  0.7032  0.7031  0.7031  0.7028  0.7026  0.7026  0.7021  0.7002  0.6998  0.6800  0.6797
[49 : 64]:	0.6632  0.6558  0.6512  0.6508  0.6461  0.6257  0.6249  0.6165  0.6133  0.6001  0.5980  0.5926  0.5898  0.5895  0.5853  0.5840
2024-03-20 22:37:34 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:37:34 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1790: GFLOPs: 362.3445. Time: 92.6263 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1791: GFLOPs: 366.6492. Time: 91.5388 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1792: GFLOPs: 350.5828. Time: 95.7338 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1793: GFLOPs: 362.6070. Time: 92.5592 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1794: GFLOPs: 351.0816. Time: 95.5978 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1795: GFLOPs: 345.9726. Time: 97.0095 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1796: GFLOPs: 329.4344. Time: 101.8795 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1797: GFLOPs: 328.2804. Time: 102.2377 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1798: GFLOPs: 328.1857. Time: 102.2672 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1799: GFLOPs: 317.0091. Time: 105.8727 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1800: GFLOPs: 324.0982. Time: 103.5570 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1801: GFLOPs: 307.7403. Time: 109.0615 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1802: GFLOPs: 305.4673. Time: 109.8731 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1803: GFLOPs: 306.3302. Time: 109.5635 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1804: GFLOPs: 309.0773. Time: 108.5897 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1805: GFLOPs: 302.0025. Time: 111.1336 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1806: GFLOPs: 303.5006. Time: 110.5850 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1807: GFLOPs: 303.3050. Time: 110.6564 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1808: GFLOPs: 298.1549. Time: 112.5678 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1809: GFLOPs: 286.8102. Time: 117.0203 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1810: GFLOPs: 286.8193. Time: 117.0166 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1811: GFLOPs: 285.6089. Time: 117.5125 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1812: GFLOPs: 300.4486. Time: 111.7084 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1813: GFLOPs: 286.0898. Time: 117.3150 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1814: GFLOPs: 284.9960. Time: 117.7652 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1815: GFLOPs: 286.0797. Time: 117.3191 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1816: GFLOPs: 281.0524. Time: 119.4177 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1817: GFLOPs: 282.2989. Time: 118.8904 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1818: GFLOPs: 288.4053. Time: 116.3731 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1819: GFLOPs: 282.2988. Time: 118.8904 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1820: GFLOPs: 278.8312. Time: 120.3690 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1821: GFLOPs: 278.7801. Time: 120.3910 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1822: GFLOPs: 278.8357. Time: 120.3670 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1823: GFLOPs: 298.7540. Time: 112.3420 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1824: GFLOPs: 298.8330. Time: 112.3123 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1825: GFLOPs: 277.5391. Time: 120.9293 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1826: GFLOPs: 285.4713. Time: 117.5692 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1827: GFLOPs: 277.5595. Time: 120.9204 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1828: GFLOPs: 273.9708. Time: 122.5044 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1829: GFLOPs: 275.0735. Time: 122.0133 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1830: GFLOPs: 277.6218. Time: 120.8933 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1831: GFLOPs: 272.3104. Time: 123.2514 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1832: GFLOPs: 299.0903. Time: 112.2157 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1833: GFLOPs: 277.5691. Time: 120.9163 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1834: GFLOPs: 271.0758. Time: 123.8127 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1835: GFLOPs: 277.5849. Time: 120.9094 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1836: GFLOPs: 219.2599. Time: 153.0723 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1837: GFLOPs: 225.2409. Time: 149.0077 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1838: GFLOPs: 273.8598. Time: 122.5540 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1839: GFLOPs: 218.2783. Time: 153.7607 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1840: GFLOPs: 164.7413. Time: 203.7293 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1841: GFLOPs: 225.2111. Time: 149.0274 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1842: GFLOPs: 234.4123. Time: 143.1777 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1843: GFLOPs: 222.0929. Time: 151.1197 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1844: GFLOPs: 225.2160. Time: 149.0242 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1845: GFLOPs: 218.2382. Time: 153.7889 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1846: GFLOPs: 222.0875. Time: 151.1234 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1847: GFLOPs: 219.1872. Time: 153.1231 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1848: GFLOPs: 222.0917. Time: 151.1206 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1849: GFLOPs: 219.2039. Time: 153.1114 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1850: GFLOPs: 219.2083. Time: 153.1084 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1851: GFLOPs: 27.8309. Time: 1205.9481 us. Best GFLOPs: 388.2341
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1852: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  312: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  311: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  310: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  309: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  308: tvm::transform::Pass::operator()(tvm::IRModule) const
  307: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  306: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  305: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  304: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  303: _ZN3tvm7runtime13PackedFuncObj
  302: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  301: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  300: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  299: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  298: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  297: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  296: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  295: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  294: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  269: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  268: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  267: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  266: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  265: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  264: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  263: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  262: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  261: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  260: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  259: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  258: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  257: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  256: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  255: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  254: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  253: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  252: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  251: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  250: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  249: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  248: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  247: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  246: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  245: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  244: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  243: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  242: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  241: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  240: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  239: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  238: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  237: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  236: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  235: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  234: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  233: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  232: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  231: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  230: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  229: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  228: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  227: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  226: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  225: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  224: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  223: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  222: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  221: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  220: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  219: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  218: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  217: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  216: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  215: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  214: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  213: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  212: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  211: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  210: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  209: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  208: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  207: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  206: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  205: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  204: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  202: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  201: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  200: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  199: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  198: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  197: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  196: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  195: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  194: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  193: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  192: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  191: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  190: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  189: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  188: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  183: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  182: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  181: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  180: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  179: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  178: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  176: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  175: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  173: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  172: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  171: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  170: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  169: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  168: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  167: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  166: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  165: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  164: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  163: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  162: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  161: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  160: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  159: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  158: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  157: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  156: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  154: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  153: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  152: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  151: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  150: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  149: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  148: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  147: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  146: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  145: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  144: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  143: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  142: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  141: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  140: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  139: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  138: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  137: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  136: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  135: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  134: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  133: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  132: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  131: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  130: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  117: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  116: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  115: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  114: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  113: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  112: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  111: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  110: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  109: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  108: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  107: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  106: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  105: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  104: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  82: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  81: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  80: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  79: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  78: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  77: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  76: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  75: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 8, 128, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 22:38:30 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1853: GFLOPs: 8.7745. Time: 3825.0193 us. Best GFLOPs: 388.2341
2024-03-20 22:39:32 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:39:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:39:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1621 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2025 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2432 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2837 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3242 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3645 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4050 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:35 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 22:39:35 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 154 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:37 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 148 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:38 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 148 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:39 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:39:40 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9413  0.9293  0.9232  0.9103  0.9082  0.9057  0.9005  0.8863  0.8748  0.8471  0.8442  0.8411  0.8329  0.8298  0.8043  0.8041
[17 : 32]:	0.7934  0.7443  0.7431  0.7419  0.7399  0.7356  0.7350  0.7341  0.7338  0.7317  0.7316  0.7293  0.7202  0.7178  0.7177  0.7126
[33 : 48]:	0.7121  0.7118  0.7092  0.7082  0.7073  0.7061  0.7045  0.6815  0.6758  0.6459  0.6443  0.6407  0.6381  0.6187  0.6082  0.6028
[49 : 64]:	0.6027  0.6023  0.5998  0.5977  0.5945  0.5941  0.5906  0.5891  0.5835  0.5799  0.5738  0.5712  0.5687  0.5686  0.5660  0.5593
2024-03-20 22:39:40 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:39:40 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1854: GFLOPs: 365.6906. Time: 91.7787 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1855: GFLOPs: 374.2390. Time: 89.6823 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1856: GFLOPs: 368.0775. Time: 91.1836 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1857: GFLOPs: 358.2403. Time: 93.6874 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1858: GFLOPs: 362.5165. Time: 92.5823 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1859: GFLOPs: 350.0214. Time: 95.8874 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1860: GFLOPs: 342.7945. Time: 97.9089 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1861: GFLOPs: 343.0272. Time: 97.8425 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1862: GFLOPs: 339.0344. Time: 98.9948 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1863: GFLOPs: 327.8800. Time: 102.3625 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1864: GFLOPs: 329.5737. Time: 101.8365 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1865: GFLOPs: 327.1614. Time: 102.5874 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1866: GFLOPs: 317.1115. Time: 105.8385 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1867: GFLOPs: 324.9807. Time: 103.2758 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1868: GFLOPs: 305.0627. Time: 110.0188 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1869: GFLOPs: 312.5942. Time: 107.3680 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1870: GFLOPs: 303.5387. Time: 110.5711 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1871: GFLOPs: 299.0964. Time: 112.2134 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1872: GFLOPs: 300.9274. Time: 111.5306 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1873: GFLOPs: 286.0430. Time: 117.3342 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1874: GFLOPs: 300.9123. Time: 111.5362 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1875: GFLOPs: 301.5314. Time: 111.3072 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1876: GFLOPs: 286.6796. Time: 117.0736 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1877: GFLOPs: 287.3833. Time: 116.7870 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1878: GFLOPs: 301.5595. Time: 111.2969 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1879: GFLOPs: 286.6669. Time: 117.0788 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1880: GFLOPs: 286.7522. Time: 117.0440 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1881: GFLOPs: 287.1336. Time: 116.8885 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1882: GFLOPs: 300.2953. Time: 111.7654 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1883: GFLOPs: 287.3980. Time: 116.7810 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1884: GFLOPs: 287.4179. Time: 116.7729 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1885: GFLOPs: 273.2922. Time: 122.8086 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1886: GFLOPs: 283.1146. Time: 118.5479 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1887: GFLOPs: 273.2478. Time: 122.8285 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1888: GFLOPs: 278.7099. Time: 120.4214 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1889: GFLOPs: 278.7149. Time: 120.4192 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1890: GFLOPs: 274.9598. Time: 122.0637 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1891: GFLOPs: 271.9218. Time: 123.4275 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1892: GFLOPs: 271.9184. Time: 123.4290 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1893: GFLOPs: 306.5054. Time: 109.5009 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1894: GFLOPs: 290.5825. Time: 115.5012 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1895: GFLOPs: 206.8209. Time: 162.2787 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1896: GFLOPs: 222.0990. Time: 151.1156 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1897: GFLOPs: 222.1069. Time: 151.1103 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1898: GFLOPs: 222.1073. Time: 151.1100 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1899: GFLOPs: 220.1450. Time: 152.4569 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1900: GFLOPs: 306.5108. Time: 109.4990 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1901: GFLOPs: 164.7665. Time: 203.6981 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1902: GFLOPs: 163.5667. Time: 205.1923 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1903: GFLOPs: 233.5047. Time: 143.7343 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1904: GFLOPs: 232.5028. Time: 144.3537 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1905: GFLOPs: 221.7047. Time: 151.3844 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1906: GFLOPs: 183.4823. Time: 182.9203 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1907: GFLOPs: 222.0829. Time: 151.1265 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1908: GFLOPs: 305.5771. Time: 109.8336 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1909: GFLOPs: 222.0959. Time: 151.1177 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1910: GFLOPs: 306.1386. Time: 109.6321 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1911: GFLOPs: 164.0712. Time: 204.5614 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1912: GFLOPs: 225.2237. Time: 149.0191 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1913: GFLOPs: 221.7540. Time: 151.3507 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1914: GFLOPs: 222.0945. Time: 151.1186 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1915: GFLOPs: 4.3466. Time: 7721.5112 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1916: GFLOPs: 21.9591. Time: 1528.4131 us. Best GFLOPs: 388.2341
2024-03-20 22:40:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1917: GFLOPs: 43.5149. Time: 771.2908 us. Best GFLOPs: 388.2341
2024-03-20 22:42:30 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:42:30 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:42:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 813 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1217 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1621 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2026 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2429 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2835 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3242 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3647 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4054 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4460 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:33 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 22:42:34 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:38 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:42:38 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9274  0.9099  0.9053  0.8888  0.8802  0.8497  0.8495  0.8495  0.8485  0.8234  0.8234  0.8232  0.8218  0.8218  0.8050  0.8050
[17 : 32]:	0.8032  0.7955  0.7903  0.7669  0.7566  0.7478  0.7468  0.7468  0.7427  0.7394  0.7285  0.7285  0.7270  0.7256  0.7208  0.7202
[33 : 48]:	0.7192  0.7075  0.7057  0.7049  0.7049  0.7018  0.7018  0.7013  0.7011  0.6940  0.6925  0.6924  0.6914  0.6741  0.6719  0.6719
[49 : 64]:	0.6535  0.5967  0.5954  0.5869  0.5844  0.5836  0.5835  0.5832  0.5827  0.5817  0.5811  0.5811  0.5805  0.5800  0.5796  0.5791
2024-03-20 22:42:38 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:42:38 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1918: GFLOPs: 366.0681. Time: 91.6841 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1919: GFLOPs: 356.9086. Time: 94.0370 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1920: GFLOPs: 353.3900. Time: 94.9733 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1921: GFLOPs: 348.3411. Time: 96.3499 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1922: GFLOPs: 343.6778. Time: 97.6572 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1923: GFLOPs: 324.4205. Time: 103.4541 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1924: GFLOPs: 127.6198. Time: 262.9893 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1925: GFLOPs: 127.6076. Time: 263.0143 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1926: GFLOPs: 319.1907. Time: 105.1491 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1927: GFLOPs: 325.9487. Time: 102.9690 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1928: GFLOPs: 326.4210. Time: 102.8200 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1929: GFLOPs: 318.7926. Time: 105.2804 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1930: GFLOPs: 321.1212. Time: 104.5170 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1931: GFLOPs: 321.1465. Time: 104.5088 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1932: GFLOPs: 304.6178. Time: 110.1795 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1933: GFLOPs: 304.9124. Time: 110.0730 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1934: GFLOPs: 311.0893. Time: 107.8874 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1935: GFLOPs: 303.6351. Time: 110.5360 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1936: GFLOPs: 296.2642. Time: 113.2861 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1937: GFLOPs: 295.0501. Time: 113.7523 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1938: GFLOPs: 298.0111. Time: 112.6221 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1939: GFLOPs: 296.1954. Time: 113.3124 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1940: GFLOPs: 296.4167. Time: 113.2278 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1941: GFLOPs: 295.7020. Time: 113.5015 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1942: GFLOPs: 300.1145. Time: 111.8327 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1943: GFLOPs: 48.7833. Time: 687.9941 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1944: GFLOPs: 285.8151. Time: 117.4277 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1945: GFLOPs: 284.6734. Time: 117.8987 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1946: GFLOPs: 280.5659. Time: 119.6247 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1947: GFLOPs: 296.2903. Time: 113.2761 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1948: GFLOPs: 285.5161. Time: 117.5507 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1949: GFLOPs: 282.4586. Time: 118.8232 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1950: GFLOPs: 296.3054. Time: 113.2704 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1951: GFLOPs: 276.6940. Time: 121.2987 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1952: GFLOPs: 276.6971. Time: 121.2974 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1953: GFLOPs: 284.8373. Time: 117.8308 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1954: GFLOPs: 284.5950. Time: 117.9312 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1955: GFLOPs: 272.4007. Time: 123.2105 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1956: GFLOPs: 274.6395. Time: 122.2061 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1957: GFLOPs: 271.8835. Time: 123.4449 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1958: GFLOPs: 274.5641. Time: 122.2397 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1959: GFLOPs: 289.9907. Time: 115.7369 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1960: GFLOPs: 289.9852. Time: 115.7391 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1961: GFLOPs: 284.7728. Time: 117.8576 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1962: GFLOPs: 161.3070. Time: 208.0667 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1963: GFLOPs: 274.1746. Time: 122.4133 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1964: GFLOPs: 274.5045. Time: 122.2662 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1965: GFLOPs: 48.8405. Time: 687.1882 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1966: GFLOPs: 274.4800. Time: 122.2771 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1967: GFLOPs: 158.3817. Time: 211.9097 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1968: GFLOPs: 220.2200. Time: 152.4050 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1969: GFLOPs: 235.8528. Time: 142.3033 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1970: GFLOPs: 218.3647. Time: 153.6999 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1971: GFLOPs: 220.2256. Time: 152.4011 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1972: GFLOPs: 220.1556. Time: 152.4496 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1973: GFLOPs: 221.2247. Time: 151.7128 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1974: GFLOPs: 158.3867. Time: 211.9031 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1975: GFLOPs: 235.8242. Time: 142.3205 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1976: GFLOPs: 223.8701. Time: 149.9201 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1977: GFLOPs: 223.9010. Time: 149.8994 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1978: GFLOPs: 236.8428. Time: 141.7084 us. Best GFLOPs: 388.2341
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1979: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(32) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(32) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(32)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(32) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 64, 32, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1980: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 16, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 1, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 22:43:34 [INFO] [task_scheduler.cc:121] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1981: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(1024), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(4))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(4))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 32, 4, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 1, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-03-20 22:48:04 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:48:04 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:48:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1214 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2026 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2431 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 2834 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3240 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 3645 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 4046 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:07 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-03-20 22:48:07 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:08 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 120 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:10 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 137 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:11 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613333bbfbd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133334cf948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133338ed668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613333b33f08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x6133334a54f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334bbbc8)]: 117 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133338edc48)]: 0 failure(s)
2024-03-20 22:48:12 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9322  0.9322  0.9220  0.8993  0.8957  0.8951  0.8815  0.8412  0.8402  0.8378  0.8104  0.8104  0.8096  0.8082  0.8029  0.8015
[17 : 32]:	0.7990  0.7977  0.7630  0.7560  0.7414  0.7316  0.7267  0.7252  0.7224  0.7224  0.7224  0.7220  0.7172  0.7133  0.7083  0.7054
[33 : 48]:	0.7053  0.7043  0.7023  0.7023  0.7017  0.7011  0.7000  0.6811  0.6533  0.6385  0.6279  0.6123  0.5955  0.5953  0.5934  0.5776
[49 : 64]:	0.5687  0.5679  0.5604  0.5604  0.5530  0.5493  0.5465  0.5419  0.5393  0.5391  0.5390  0.5372  0.5372  0.5370  0.5367  0.5322
2024-03-20 22:48:12 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:48:12 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1982: GFLOPs: 368.5297. Time: 91.0717 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1983: GFLOPs: 371.8847. Time: 90.2501 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1984: GFLOPs: 363.2203. Time: 92.4030 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1985: GFLOPs: 349.9745. Time: 95.9002 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1986: GFLOPs: 351.1135. Time: 95.5891 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1987: GFLOPs: 346.6192. Time: 96.8285 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1988: GFLOPs: 343.8364. Time: 97.6122 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1989: GFLOPs: 327.8890. Time: 102.3597 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1990: GFLOPs: 328.5448. Time: 102.1554 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1991: GFLOPs: 326.6198. Time: 102.7575 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1992: GFLOPs: 308.3674. Time: 108.8397 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1993: GFLOPs: 308.3302. Time: 108.8529 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1994: GFLOPs: 309.5695. Time: 108.4171 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1995: GFLOPs: 314.5213. Time: 106.7102 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1996: GFLOPs: 311.8658. Time: 107.6188 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1997: GFLOPs: 307.3333. Time: 109.2059 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1998: GFLOPs: 301.6681. Time: 111.2568 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #1999: GFLOPs: 302.1202. Time: 111.0903 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2000: GFLOPs: 296.2794. Time: 113.2803 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2001: GFLOPs: 299.4320. Time: 112.0876 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2002: GFLOPs: 283.7778. Time: 118.2708 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2003: GFLOPs: 284.5724. Time: 117.9405 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2004: GFLOPs: 286.5818. Time: 117.1136 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2005: GFLOPs: 284.1397. Time: 118.1201 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2006: GFLOPs: 280.1705. Time: 119.7936 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2007: GFLOPs: 280.1279. Time: 119.8118 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2008: GFLOPs: 296.1311. Time: 113.3370 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2009: GFLOPs: 285.5846. Time: 117.5225 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2010: GFLOPs: 300.4505. Time: 111.7077 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2011: GFLOPs: 295.2906. Time: 113.6596 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2012: GFLOPs: 276.9252. Time: 121.1974 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2013: GFLOPs: 273.6666. Time: 122.6406 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2014: GFLOPs: 299.3233. Time: 112.1284 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2015: GFLOPs: 276.9023. Time: 121.2074 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2016: GFLOPs: 270.9310. Time: 123.8789 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2017: GFLOPs: 273.6940. Time: 122.6283 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2018: GFLOPs: 299.4537. Time: 112.0795 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2019: GFLOPs: 272.9986. Time: 122.9406 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2020: GFLOPs: 270.9002. Time: 123.8930 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2021: GFLOPs: 161.8219. Time: 207.4047 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2022: GFLOPs: 199.7418. Time: 168.0300 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2023: GFLOPs: 224.7588. Time: 149.3273 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2024: GFLOPs: 161.9328. Time: 207.2626 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2025: GFLOPs: 161.9483. Time: 207.2429 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2026: GFLOPs: 220.2242. Time: 152.4021 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2027: GFLOPs: 219.6870. Time: 152.7748 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2028: GFLOPs: 162.0615. Time: 207.0981 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2029: GFLOPs: 218.1990. Time: 153.8166 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2030: GFLOPs: 227.3001. Time: 147.6578 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2031: GFLOPs: 204.0251. Time: 164.5024 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2032: GFLOPs: 218.0249. Time: 153.9394 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2033: GFLOPs: 224.6964. Time: 149.3687 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2034: GFLOPs: 161.9142. Time: 207.2865 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2035: GFLOPs: 158.0731. Time: 212.3234 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2036: GFLOPs: 157.4374. Time: 213.1807 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2037: GFLOPs: 202.5775. Time: 165.6779 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2038: GFLOPs: 219.8863. Time: 152.6363 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2039: GFLOPs: 162.6130. Time: 206.3957 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2040: GFLOPs: 158.2058. Time: 212.1454 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2041: GFLOPs: 220.7589. Time: 152.0330 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2042: GFLOPs: 220.8251. Time: 151.9874 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2043: GFLOPs: 2.4416. Time: 13746.3035 us. Best GFLOPs: 388.2341
2024-03-20 22:49:08 [INFO] [task_scheduler.cc:131] [Task #11: fused_nn_dense_add_nn_relu_1] Trial #2044: GFLOPs: 23.3172. Time: 1439.3930 us. Best GFLOPs: 388.2341
