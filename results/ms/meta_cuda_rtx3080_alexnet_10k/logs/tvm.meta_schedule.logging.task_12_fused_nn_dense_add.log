2024-03-20 19:37:47 [INFO] [task_scheduler.cc:160] Initializing Task #12: "fused_nn_dense_add"
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT = T.alloc_buffer((T.int64(1), T.int64(1000)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(1000), T.int64(4096)):
            with T.block("T_matmul_NT"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(p0[v_i0, v_k], p1[v_i1, v_k])
                T.writes(T_matmul_NT[v_i0, v_i1])
                with T.init():
                    T_matmul_NT[v_i0, v_i1] = T.float32(0)
                T_matmul_NT[v_i0, v_i1] = T_matmul_NT[v_i0, v_i1] + p0[v_i0, v_k] * p1[v_i1, v_k]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1000)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(T_matmul_NT[v_ax0, v_ax1], p2[v_ax0, v_ax1])
                T.writes(T_add[v_ax0, v_ax1])
                T_add[v_ax0, v_ax1] = T_matmul_NT[v_ax0, v_ax1] + p2[v_ax0, v_ax1]
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for k_0 in range(T.int64(64)):
                            for ax0_ax1_fused in range(T.int64(64)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(32000)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + ax0_ax1_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(125), T.int64(64), T.int64(1), T.int64(2)):
                                with T.block("T_matmul_NT"):
                                    v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                    v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + i1_3 * T.int64(2) + i1_4)
                                    v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(64) + k_2)
                                    T.reads(p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                    T.writes(T_matmul_NT_local[v_i0, v_i1])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                                    T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(250)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 2, 1, 125, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 1, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 1024})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for k_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_fused in range(T.int64(64)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(32000)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + ax0_ax1_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + ax0_ax1_fused % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(125), T.int64(64), T.int64(1), T.int64(2)):
                                with T.block("T_matmul_NT"):
                                    v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                    v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + i1_3 * T.int64(2) + i1_4)
                                    v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(64) + k_1 * T.int64(64) + k_2)
                                    T.reads(p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                    T.writes(T_matmul_NT_local[v_i0, v_i1])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                                    T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(250)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 2, 1, 125, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 1, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
2024-03-20 19:37:47 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for k_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_fused in range(T.int64(64)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(32000)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + ax0_ax1_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + ax0_ax1_fused % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(125), T.int64(64), T.int64(1), T.int64(2)):
                                with T.block("T_matmul_NT"):
                                    v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                    v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + i1_3 * T.int64(2) + i1_4)
                                    v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(64) + k_1 * T.int64(64) + k_2)
                                    T.reads(p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                    T.writes(T_matmul_NT_local[v_i0, v_i1])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                                    T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(250)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 2, 1, 125, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 1, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
2024-03-20 19:49:31 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 19:49:31 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-03-20 19:49:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 505 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1015 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1520 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2028 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2531 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3033 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3542 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4047 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4546 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:34 [INFO] [evolutionary_search.cc:723] Sampled 62 candidate(s)
2024-03-20 19:49:34 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:37 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:49:37 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0000  0.9996  0.9987  0.9960  0.9959  0.9926  0.9923  0.9922  0.9922  0.9913  0.9892  0.9866  0.9849  0.9844  0.9824  0.9820
[17 : 32]:	0.9809  0.9788  0.9782  0.9780  0.9776  0.9773  0.9764  0.9761  0.9752  0.9751  0.9750  0.9749  0.9741  0.9736  0.9725  0.9709
[33 : 48]:	0.9706  0.9690  0.9684  0.9668  0.9667  0.9660  0.9657  0.9653  0.9644  0.9624  0.9624  0.9599  0.9599  0.9592  0.9590  0.9581
[49 : 64]:	0.9581  0.9579  0.9563  0.9562  0.9535  0.9533  0.9532  0.9524  0.9522  0.9514  0.9512  0.9504  0.9472  0.9456  0.9445  0.9429
2024-03-20 19:49:37 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 19:49:37 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1: GFLOPs: 13.9142. Time: 588.8209 us. Best GFLOPs: 13.9142
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #2: GFLOPs: 17.8290. Time: 459.5317 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #3: GFLOPs: 5.0696. Time: 1616.1162 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #4: GFLOPs: 3.2682. Time: 2506.9023 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #5: GFLOPs: 3.5184. Time: 2328.6236 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #6: GFLOPs: 16.1614. Time: 506.9472 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #7: GFLOPs: 15.4099. Time: 531.6716 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #8: GFLOPs: 6.8125. Time: 1202.6480 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #9: GFLOPs: 3.2607. Time: 2512.6400 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #10: GFLOPs: 10.7225. Time: 764.0916 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #11: GFLOPs: 16.2921. Time: 502.8818 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #12: GFLOPs: 3.3387. Time: 2453.9746 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #13: GFLOPs: 13.5542. Time: 604.4623 us. Best GFLOPs: 17.8290
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #14: GFLOPs: 23.2065. Time: 353.0470 us. Best GFLOPs: 23.2065
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #15: GFLOPs: 4.5458. Time: 1802.3314 us. Best GFLOPs: 23.2065
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #16: GFLOPs: 24.7655. Time: 330.8233 us. Best GFLOPs: 24.7655
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #17: GFLOPs: 18.3506. Time: 446.4697 us. Best GFLOPs: 24.7655
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #18: GFLOPs: 16.4633. Time: 497.6536 us. Best GFLOPs: 24.7655
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #19: GFLOPs: 4.3890. Time: 1866.7140 us. Best GFLOPs: 24.7655
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #20: GFLOPs: 14.4827. Time: 565.7108 us. Best GFLOPs: 24.7655
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #21: GFLOPs: 16.5234. Time: 495.8428 us. Best GFLOPs: 24.7655
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #22: GFLOPs: 55.2735. Time: 148.2266 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #23: GFLOPs: 9.0242. Time: 907.8946 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #24: GFLOPs: 7.4146. Time: 1104.9860 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #25: GFLOPs: 3.3280. Time: 2461.8653 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #26: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  308: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  307: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  306: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  305: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  304: tvm::transform::Pass::operator()(tvm::IRModule) const
  303: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  302: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  301: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  300: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  299: _ZN3tvm7runtime13PackedFuncObj
  298: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  297: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  296: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  295: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  294: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  293: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  292: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  291: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  290: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  269: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  268: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  267: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  266: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  265: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  264: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  263: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  262: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  261: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  260: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  259: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  258: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  257: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  256: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  255: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  254: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  253: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  252: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  251: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  250: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  249: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  248: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  247: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  246: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  245: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  244: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  243: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  242: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  241: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  240: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  239: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  238: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  237: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  236: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  235: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  234: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  233: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  232: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  231: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  230: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  229: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  228: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  227: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  226: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  225: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  224: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  223: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  222: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  221: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  220: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  219: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  218: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  217: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  216: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  215: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  214: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  213: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  212: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  211: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  210: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  209: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  208: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  207: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  206: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  205: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  204: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  202: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  201: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  200: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  199: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  198: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  197: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  196: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  195: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  194: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  193: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  192: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  191: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  183: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  182: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  181: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  180: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  179: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  178: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  176: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  175: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  174: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  173: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  172: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  171: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  170: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  169: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  168: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  167: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  166: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  165: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  164: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  163: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  162: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  161: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  160: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  159: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  158: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  157: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  156: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  155: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  154: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  153: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  152: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  151: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  150: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  149: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  148: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  147: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  146: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  145: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  144: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  143: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  142: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  141: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  140: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  139: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  138: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  137: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  136: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  135: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  134: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  133: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  132: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  131: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  130: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  129: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  128: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  127: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  126: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  125: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  124: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  123: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  122: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  121: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  120: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  119: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  118: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  117: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  116: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  115: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  114: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  113: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  112: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  111: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  110: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  109: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  108: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  107: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  106: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  105: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  104: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  103: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  102: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  101: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  100: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  99: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  98: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  97: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  96: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  95: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  94: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  93: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  92: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  91: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  90: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  89: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  88: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  87: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  86: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  85: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  84: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  83: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  82: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  81: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  80: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  79: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  78: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  77: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  76: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  75: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  74: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  73: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  72: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  71: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  70: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  69: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  68: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  67: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  66: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  65: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  64: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  63: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  62: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  61: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  60: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  59: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  58: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  57: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  56: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  55: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  54: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  53: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  52: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  51: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  49: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  48: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  46: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  43: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  42: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  41: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  38: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  37: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  35: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  32: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  30: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  29: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  28: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  27: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  26: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  23: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  21: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  20: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  19: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  17: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  16: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(500) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 2, 250, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 250, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 250, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #27: GFLOPs: 8.1378. Time: 1006.7866 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #28: GFLOPs: 15.1894. Time: 539.3889 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #29: GFLOPs: 25.1167. Time: 326.1967 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #30: GFLOPs: 3.4651. Time: 2364.4398 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #31: GFLOPs: 9.7786. Time: 837.8539 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #32: GFLOPs: 18.3792. Time: 445.7758 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #33: GFLOPs: 19.5844. Time: 418.3438 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #34: GFLOPs: 4.9117. Time: 1668.0618 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #35: GFLOPs: 6.5986. Time: 1241.6190 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #36: GFLOPs: 19.3087. Time: 424.3157 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #37: GFLOPs: 10.8722. Time: 753.5704 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #38: GFLOPs: 4.8504. Time: 1689.1365 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #39: GFLOPs: 3.0495. Time: 2686.6525 us. Best GFLOPs: 55.2735
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #40: GFLOPs: 78.1526. Time: 104.8334 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #41: GFLOPs: 3.8626. Time: 2121.0880 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #42: GFLOPs: 1.9130. Time: 4282.7946 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #43: GFLOPs: 12.9307. Time: 633.6095 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #44: GFLOPs: 8.4891. Time: 965.1172 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #45: GFLOPs: 24.5532. Time: 333.6839 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #46: GFLOPs: 32.1610. Time: 254.7492 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #47: GFLOPs: 3.0342. Time: 2700.1752 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #48: GFLOPs: 11.6185. Time: 705.1680 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #49: GFLOPs: 28.9832. Time: 282.6814 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #50: GFLOPs: 1.8764. Time: 4366.2915 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #51: GFLOPs: 8.2251. Time: 996.0979 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #52: GFLOPs: 4.3169. Time: 1897.8790 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #53: GFLOPs: 28.6052. Time: 286.4163 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #54: GFLOPs: 3.2275. Time: 2538.5216 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #55: GFLOPs: 3.0977. Time: 2644.8336 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #56: GFLOPs: 4.9913. Time: 1641.4710 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #57: GFLOPs: 6.3656. Time: 1287.0761 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #58: GFLOPs: 11.6785. Time: 701.5474 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #59: GFLOPs: 1.9133. Time: 4282.0266 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #60: GFLOPs: 6.5539. Time: 1250.0864 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #61: GFLOPs: 10.2471. Time: 799.5401 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #62: GFLOPs: 8.9458. Time: 915.8470 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #63: GFLOPs: 17.7885. Time: 460.5783 us. Best GFLOPs: 78.1526
2024-03-20 19:51:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #64: GFLOPs: 3.2891. Time: 2490.9175 us. Best GFLOPs: 78.1526
2024-03-20 19:52:15 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 19:52:15 [INFO] [evolutionary_search.cc:715] Picked top 63 candidate(s) from database
2024-03-20 19:52:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 444 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 888 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1332 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1778 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2223 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2668 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3114 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3549 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3989 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:17 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-20 19:52:18 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 88 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:19 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:22 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 105 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 19:52:22 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.9445  1.9445  1.9429  1.7871  1.6035  1.6035  1.6018  1.5877  1.5877  1.5877  1.5866  1.5732  1.5591  1.4247  1.4234  1.4213
[17 : 32]:	1.4147  1.4126  1.4049  1.4014  1.3906  1.3895  1.3895  1.3895  1.3872  1.3695  1.3689  1.3625  1.3547  1.3547  1.3428  1.3393
[33 : 48]:	1.3285  1.3285  1.3285  1.3285  1.3264  1.3229  1.3202  1.3183  1.3168  1.3144  1.3118  1.3107  1.3100  1.3051  1.3042  1.3036
[49 : 64]:	1.3029  1.3010  1.3010  1.3003  1.2962  1.2941  1.2939  1.2915  1.2914  1.2884  1.2875  1.2871  1.2868  1.2868  1.2868  1.2865
2024-03-20 19:52:22 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 19:52:22 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #65: GFLOPs: 34.7530. Time: 235.7494 us. Best GFLOPs: 78.1526
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #66: GFLOPs: 34.7553. Time: 235.7340 us. Best GFLOPs: 78.1526
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #67: GFLOPs: 34.7129. Time: 236.0215 us. Best GFLOPs: 78.1526
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #68: GFLOPs: 114.3210. Time: 71.6666 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #69: GFLOPs: 105.1473. Time: 77.9192 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #70: GFLOPs: 105.1483. Time: 77.9185 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #71: GFLOPs: 105.1494. Time: 77.9177 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #72: GFLOPs: 102.2743. Time: 80.1081 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #73: GFLOPs: 105.1157. Time: 77.9427 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #74: GFLOPs: 104.5359. Time: 78.3750 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #75: GFLOPs: 102.2783. Time: 80.1050 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #76: GFLOPs: 81.7220. Time: 100.2545 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #77: GFLOPs: 33.2218. Time: 246.6152 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #78: GFLOPs: 61.5546. Time: 133.1013 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #79: GFLOPs: 98.9328. Time: 82.8138 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #80: GFLOPs: 98.9423. Time: 82.8059 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #81: GFLOPs: 61.4543. Time: 133.3185 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #82: GFLOPs: 61.4525. Time: 133.3226 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #83: GFLOPs: 43.6894. Time: 187.5282 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #84: GFLOPs: 59.4654. Time: 137.7775 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #85: GFLOPs: 57.6168. Time: 142.1981 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #86: GFLOPs: 56.0935. Time: 146.0597 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #87: GFLOPs: 56.0988. Time: 146.0460 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #88: GFLOPs: 56.0989. Time: 146.0457 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #89: GFLOPs: 57.6122. Time: 142.2094 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #90: GFLOPs: 61.6654. Time: 132.8621 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #91: GFLOPs: 58.7263. Time: 139.5115 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #92: GFLOPs: 35.1582. Time: 233.0326 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #93: GFLOPs: 58.7337. Time: 139.4941 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #94: GFLOPs: 58.7317. Time: 139.4987 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #95: GFLOPs: 35.2594. Time: 232.3639 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #96: GFLOPs: 34.7035. Time: 236.0860 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #97: GFLOPs: 56.6352. Time: 144.6628 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #98: GFLOPs: 56.6303. Time: 144.6753 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #99: GFLOPs: 56.6361. Time: 144.6605 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #100: GFLOPs: 56.6422. Time: 144.6448 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #101: GFLOPs: 56.6355. Time: 144.6619 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #102: GFLOPs: 34.5409. Time: 237.1970 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #103: GFLOPs: 30.6248. Time: 267.5281 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #104: GFLOPs: 30.6543. Time: 267.2706 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #105: GFLOPs: 28.8295. Time: 284.1880 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #106: GFLOPs: 58.6295. Time: 139.7419 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #107: GFLOPs: 28.8317. Time: 284.1663 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #108: GFLOPs: 30.6510. Time: 267.2993 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #109: GFLOPs: 23.0135. Time: 356.0084 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #110: GFLOPs: 93.1610. Time: 87.9445 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #111: GFLOPs: 28.8285. Time: 284.1980 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #112: GFLOPs: 57.8748. Time: 141.5642 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #113: GFLOPs: 40.9334. Time: 200.1545 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #114: GFLOPs: 21.7084. Time: 377.4116 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #115: GFLOPs: 21.7075. Time: 377.4271 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #116: GFLOPs: 61.2768. Time: 133.7048 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #117: GFLOPs: 31.9233. Time: 256.6463 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #118: GFLOPs: 34.6005. Time: 236.7883 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #119: GFLOPs: 29.2182. Time: 280.4071 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #120: GFLOPs: 50.7999. Time: 161.2800 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #121: GFLOPs: 32.6164. Time: 251.1926 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #122: GFLOPs: 29.2138. Time: 280.4498 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #123: GFLOPs: 29.2181. Time: 280.4083 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #124: GFLOPs: 28.8291. Time: 284.1916 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #125: GFLOPs: 56.0981. Time: 146.0478 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #126: GFLOPs: 29.9985. Time: 273.1138 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #127: GFLOPs: 5.2566. Time: 1558.6068 us. Best GFLOPs: 114.3210
2024-03-20 19:53:08 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #128: GFLOPs: 21.2115. Time: 386.2524 us. Best GFLOPs: 114.3210
2024-03-20 20:05:34 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:05:34 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:05:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1212 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1615 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2020 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2427 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2832 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3237 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3643 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4047 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:36 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-20 20:05:37 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:38 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:40 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 104 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:05:41 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.5389  1.5287  1.5243  1.5243  1.5179  1.5142  1.5130  1.5062  1.5023  1.5019  1.5013  1.5010  1.5001  1.4992  1.4721  1.4489
[17 : 32]:	1.4466  1.4466  1.4460  1.4460  1.4433  1.4360  1.4342  1.4313  1.4213  1.4164  1.3681  1.3680  1.3637  1.3631  1.3608  1.3608
[33 : 48]:	1.3495  1.1595  1.1211  1.0369  1.0348  1.0347  1.0321  1.0321  1.0308  1.0055  1.0020  0.9777  0.9733  0.9727  0.9699  0.9692
[49 : 64]:	0.9684  0.9681  0.9678  0.9678  0.9678  0.9650  0.9644  0.9631  0.9609  0.9594  0.9594  0.9590  0.9513  0.9499  0.9469  0.9469
2024-03-20 20:05:42 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:05:42 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #129: GFLOPs: 72.7060. Time: 112.6867 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #130: GFLOPs: 72.3532. Time: 113.2361 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #131: GFLOPs: 72.3542. Time: 113.2347 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #132: GFLOPs: 72.3491. Time: 113.2426 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #133: GFLOPs: 72.3591. Time: 113.2269 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #134: GFLOPs: 71.8152. Time: 114.0844 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #135: GFLOPs: 71.8311. Time: 114.0593 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #136: GFLOPs: 71.8008. Time: 114.1073 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #137: GFLOPs: 72.9386. Time: 112.3274 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #138: GFLOPs: 71.0266. Time: 115.3511 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #139: GFLOPs: 71.7845. Time: 114.1333 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #140: GFLOPs: 61.9910. Time: 132.1643 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #141: GFLOPs: 71.8074. Time: 114.0969 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #142: GFLOPs: 71.0318. Time: 115.3426 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #143: GFLOPs: 30.2519. Time: 270.8262 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #144: GFLOPs: 53.7300. Time: 152.4846 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #145: GFLOPs: 53.8353. Time: 152.1864 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #146: GFLOPs: 53.8299. Time: 152.2017 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #147: GFLOPs: 53.7242. Time: 152.5010 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #148: GFLOPs: 53.7265. Time: 152.4945 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #149: GFLOPs: 27.4074. Time: 298.9340 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #150: GFLOPs: 27.4083. Time: 298.9242 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #151: GFLOPs: 53.7241. Time: 152.5013 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #152: GFLOPs: 53.7211. Time: 152.5099 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #153: GFLOPs: 53.8391. Time: 152.1755 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #154: GFLOPs: 35.6824. Time: 229.6090 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #155: GFLOPs: 38.4679. Time: 212.9825 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #156: GFLOPs: 27.5049. Time: 297.8744 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #157: GFLOPs: 52.2890. Time: 156.6870 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #158: GFLOPs: 50.8070. Time: 161.2574 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #159: GFLOPs: 38.4714. Time: 212.9635 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #160: GFLOPs: 51.0618. Time: 160.4526 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #161: GFLOPs: 41.4590. Time: 197.6169 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #162: GFLOPs: 32.1987. Time: 254.4511 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #163: GFLOPs: 29.3810. Time: 278.8534 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #164: GFLOPs: 29.8409. Time: 274.5563 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #165: GFLOPs: 29.8101. Time: 274.8400 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #166: GFLOPs: 29.8095. Time: 274.8454 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #167: GFLOPs: 29.8904. Time: 274.1011 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #168: GFLOPs: 29.3462. Time: 279.1844 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #169: GFLOPs: 17.4992. Time: 468.1929 us. Best GFLOPs: 114.3210
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #170: GFLOPs: 130.7471. Time: 62.6630 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #171: GFLOPs: 114.5349. Time: 71.5328 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #172: GFLOPs: 115.6779. Time: 70.8260 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #173: GFLOPs: 130.7304. Time: 62.6710 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #174: GFLOPs: 130.1727. Time: 62.9395 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #175: GFLOPs: 105.1041. Time: 77.9513 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #176: GFLOPs: 114.0790. Time: 71.8186 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #177: GFLOPs: 114.0848. Time: 71.8150 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #178: GFLOPs: 115.2451. Time: 71.0920 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #179: GFLOPs: 111.9007. Time: 73.2167 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #180: GFLOPs: 111.9385. Time: 73.1920 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #181: GFLOPs: 111.9078. Time: 73.2120 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #182: GFLOPs: 111.6703. Time: 73.3678 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #183: GFLOPs: 111.6762. Time: 73.3639 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #184: GFLOPs: 130.1668. Time: 62.9423 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #185: GFLOPs: 105.0906. Time: 77.9613 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #186: GFLOPs: 105.1483. Time: 77.9185 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #187: GFLOPs: 105.0953. Time: 77.9579 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #188: GFLOPs: 111.8760. Time: 73.2329 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #189: GFLOPs: 20.2542. Time: 404.5082 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #190: GFLOPs: 25.6662. Time: 319.2141 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #191: GFLOPs: 12.2221. Time: 670.3418 us. Best GFLOPs: 130.7471
2024-03-20 20:06:30 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #192: GFLOPs: 53.1300. Time: 154.2065 us. Best GFLOPs: 130.7471
2024-03-20 20:18:05 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:18:05 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:18:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 814 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1219 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1628 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2036 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2441 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2843 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3249 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3650 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4052 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4456 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:08 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-03-20 20:18:09 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 140 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:10 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 162 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:11 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:12 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:18:13 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0059  1.0042  1.0023  1.0006  0.9989  0.9933  0.9916  0.9899  0.9852  0.9816  0.9726  0.9691  0.9677  0.9669  0.9669  0.9669
[17 : 32]:	0.9666  0.9659  0.9655  0.9653  0.9652  0.9645  0.9645  0.9636  0.9629  0.9629  0.9625  0.9619  0.9619  0.9500  0.9185  0.9168
[33 : 48]:	0.9151  0.9116  0.9095  0.9079  0.9078  0.9067  0.9067  0.9061  0.8908  0.8888  0.8859  0.8852  0.8841  0.8841  0.8838  0.8835
[49 : 64]:	0.8830  0.8828  0.8826  0.8822  0.8822  0.8821  0.8821  0.8816  0.8815  0.8809  0.8807  0.8801  0.8800  0.8791  0.8790  0.8790
2024-03-20 20:18:13 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:18:13 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #193: GFLOPs: 130.3093. Time: 62.8735 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #194: GFLOPs: 130.3107. Time: 62.8728 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #195: GFLOPs: 121.0365. Time: 67.6903 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #196: GFLOPs: 121.0417. Time: 67.6874 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #197: GFLOPs: 121.0473. Time: 67.6843 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #198: GFLOPs: 130.3315. Time: 62.8628 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #199: GFLOPs: 130.3234. Time: 62.8667 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #200: GFLOPs: 130.3175. Time: 62.8695 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #201: GFLOPs: 130.3091. Time: 62.8736 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #202: GFLOPs: 121.0621. Time: 67.6760 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #203: GFLOPs: 130.3325. Time: 62.8623 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #204: GFLOPs: 118.8386. Time: 68.9422 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #205: GFLOPs: 118.2575. Time: 69.2810 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #206: GFLOPs: 130.3275. Time: 62.8647 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #207: GFLOPs: 129.8794. Time: 63.0816 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #208: GFLOPs: 117.6714. Time: 69.6261 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #209: GFLOPs: 117.8201. Time: 69.5382 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #210: GFLOPs: 117.8262. Time: 69.5346 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #211: GFLOPs: 129.8786. Time: 63.0820 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #212: GFLOPs: 120.6397. Time: 67.9130 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #213: GFLOPs: 129.8829. Time: 63.0799 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #214: GFLOPs: 129.8811. Time: 63.0808 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #215: GFLOPs: 129.8853. Time: 63.0787 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #216: GFLOPs: 120.6350. Time: 67.9156 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #217: GFLOPs: 120.6265. Time: 67.9204 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #218: GFLOPs: 120.6323. Time: 67.9171 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #219: GFLOPs: 120.6335. Time: 67.9165 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #220: GFLOPs: 120.6401. Time: 67.9128 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #221: GFLOPs: 120.6374. Time: 67.9142 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #222: GFLOPs: 118.4296. Time: 69.1804 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #223: GFLOPs: 115.0484. Time: 71.2135 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #224: GFLOPs: 115.0335. Time: 71.2227 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #225: GFLOPs: 115.0292. Time: 71.2254 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #226: GFLOPs: 115.5793. Time: 70.8864 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #227: GFLOPs: 113.6202. Time: 72.1086 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #228: GFLOPs: 118.1778. Time: 69.3277 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #229: GFLOPs: 113.6129. Time: 72.1133 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #230: GFLOPs: 115.5894. Time: 70.8802 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #231: GFLOPs: 113.6120. Time: 72.1139 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #232: GFLOPs: 113.6097. Time: 72.1153 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #233: GFLOPs: 117.2530. Time: 69.8745 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #234: GFLOPs: 113.6096. Time: 72.1154 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #235: GFLOPs: 117.2504. Time: 69.8761 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #236: GFLOPs: 113.1669. Time: 72.3975 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #237: GFLOPs: 105.5072. Time: 77.6535 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #238: GFLOPs: 113.6169. Time: 72.1108 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #239: GFLOPs: 113.3063. Time: 72.3084 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #240: GFLOPs: 113.1778. Time: 72.3905 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #241: GFLOPs: 110.8097. Time: 73.9375 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #242: GFLOPs: 113.8307. Time: 71.9753 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #243: GFLOPs: 105.3280. Time: 77.7856 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #244: GFLOPs: 113.8468. Time: 71.9651 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #245: GFLOPs: 113.8526. Time: 71.9615 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #246: GFLOPs: 113.3043. Time: 72.3097 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #247: GFLOPs: 113.3048. Time: 72.3094 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #248: GFLOPs: 105.5097. Time: 77.6516 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #249: GFLOPs: 115.0253. Time: 71.2278 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #250: GFLOPs: 105.0379. Time: 78.0004 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #251: GFLOPs: 113.6078. Time: 72.1165 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #252: GFLOPs: 105.3227. Time: 77.7895 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #253: GFLOPs: 105.3389. Time: 77.7775 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #254: GFLOPs: 17.4922. Time: 468.3795 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #255: GFLOPs: 11.2433. Time: 728.7022 us. Best GFLOPs: 130.7471
2024-03-20 20:19:00 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #256: GFLOPs: 3.0455. Time: 2690.2046 us. Best GFLOPs: 130.7471
2024-03-20 20:22:06 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:22:06 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:22:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1215 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1617 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2020 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2425 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2826 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3232 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3632 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:08 [INFO] [evolutionary_search.cc:723] Sampled 58 candidate(s)
2024-03-20 20:22:09 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:11 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 186 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:12 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 190 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:13 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:22:14 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9796  0.9743  0.9736  0.9736  0.9591  0.9584  0.9584  0.9582  0.9558  0.9558  0.9558  0.9542  0.9443  0.9435  0.9435  0.9273
[17 : 32]:	0.9250  0.9250  0.9250  0.9232  0.9230  0.9226  0.9221  0.9219  0.9209  0.9203  0.9203  0.9189  0.9183  0.8957  0.8857  0.8850
[33 : 48]:	0.8841  0.8841  0.8841  0.8834  0.8817  0.8766  0.8759  0.8735  0.8707  0.8707  0.8705  0.8705  0.8693  0.8693  0.8686  0.8686
[49 : 64]:	0.8686  0.8686  0.8685  0.8685  0.8683  0.8683  0.8677  0.8677  0.8677  0.8674  0.8674  0.8659  0.8659  0.8659  0.8659  0.8656
2024-03-20 20:22:14 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:22:14 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #257: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #258: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(128))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #259: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(128))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #260: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(128))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #261: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #262: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 16, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #263: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #264: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 64, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #265: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 16, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #266: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #267: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #268: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #269: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #270: GFLOPs: 121.0486. Time: 67.6836 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #271: GFLOPs: 121.0482. Time: 67.6838 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #272: GFLOPs: 118.8081. Time: 68.9599 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #273: GFLOPs: 118.8052. Time: 68.9616 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #274: GFLOPs: 118.8029. Time: 68.9630 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #275: GFLOPs: 118.7938. Time: 68.9682 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #276: GFLOPs: 118.2647. Time: 69.2768 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #277: GFLOPs: 118.0382. Time: 69.4097 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #278: GFLOPs: 120.8738. Time: 67.7815 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #279: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #280: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #281: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(128))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #282: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 16, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #283: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 8, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #284: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(128))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #285: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #286: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #287: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 64, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #288: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #289: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #290: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #291: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #292: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 16, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #293: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(128))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #294: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(128))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 64, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #295: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(128))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 32, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #296: GFLOPs: 115.9791. Time: 70.6421 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #297: GFLOPs: 117.7801. Time: 69.5618 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #298: GFLOPs: 117.7696. Time: 69.5680 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #299: GFLOPs: 117.7710. Time: 69.5672 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #300: GFLOPs: 120.4760. Time: 68.0053 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #301: GFLOPs: 115.5099. Time: 70.9290 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #302: GFLOPs: 115.5010. Time: 70.9344 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #303: GFLOPs: 115.4861. Time: 70.9436 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #304: GFLOPs: 115.4747. Time: 70.9506 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #305: GFLOPs: 115.4958. Time: 70.9377 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #306: GFLOPs: 115.4974. Time: 70.9367 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #307: GFLOPs: 117.7415. Time: 69.5846 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #308: GFLOPs: 117.7441. Time: 69.5831 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #309: GFLOPs: 130.1568. Time: 62.9472 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #310: GFLOPs: 117.7561. Time: 69.5760 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #311: GFLOPs: 115.9867. Time: 70.6374 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #312: GFLOPs: 116.4220. Time: 70.3733 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #313: GFLOPs: 115.9882. Time: 70.6365 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #314: GFLOPs: 115.9904. Time: 70.6351 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #315: GFLOPs: 120.8653. Time: 67.7862 us. Best GFLOPs: 130.7471
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #316: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #317: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #318: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(500) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + (ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_1_i1_1_fused * T.int64(250) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 2, 250, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[512, 2, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 250, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69 = sch.split(loop=l67, factors=[None, 250], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b71)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #319: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(5), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_1_i1_1_fused * T.int64(200) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(40)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_1_i1_1_fused * T.int64(200) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_1_i1_1_fused * T.int64(200) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 5, 50, 1, 4])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70 = sch.split(loop=l68, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:22:45 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #320: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 250, 1, 4])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 250], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 250, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:37:49 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:37:49 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:37:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 803 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1612 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2017 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2420 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2826 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3231 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3636 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:52 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-03-20 20:37:53 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 183 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:54 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:55 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 205 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:57 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 193 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:37:57 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9516  0.9190  0.9129  0.9118  0.9099  0.9090  0.9090  0.9071  0.9064  0.9028  0.9006  0.9000  0.9000  0.9000  0.8950  0.8950
[17 : 32]:	0.8903  0.8903  0.8896  0.8896  0.8896  0.8896  0.8896  0.8810  0.8537  0.8531  0.8531  0.8527  0.8527  0.8520  0.8520  0.8520
[33 : 48]:	0.8482  0.8482  0.8471  0.8471  0.8462  0.8462  0.8451  0.8416  0.8416  0.8411  0.8411  0.8404  0.8404  0.8404  0.8404  0.8404
[49 : 64]:	0.8323  0.8316  0.8316  0.8312  0.8306  0.8306  0.8306  0.8303  0.8280  0.8269  0.8269  0.8268  0.8264  0.8263  0.8263  0.8263
2024-03-20 20:37:57 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:37:57 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #321: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #322: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #323: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(128))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #324: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #325: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #326: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #327: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 16, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #328: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #329: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 4, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #330: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(128) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(128) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 128, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #331: GFLOPs: 117.7490. Time: 69.5802 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #332: GFLOPs: 118.4453. Time: 69.1712 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #333: GFLOPs: 118.4371. Time: 69.1760 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #334: GFLOPs: 118.4342. Time: 69.1777 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #335: GFLOPs: 114.2579. Time: 71.7062 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #336: GFLOPs: 114.2579. Time: 71.7062 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #337: GFLOPs: 118.4415. Time: 69.1734 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #338: GFLOPs: 121.3095. Time: 67.5380 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #339: GFLOPs: 118.4363. Time: 69.1764 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #340: GFLOPs: 118.4287. Time: 69.1809 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #341: GFLOPs: 118.4331. Time: 69.1783 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #342: GFLOPs: 118.4294. Time: 69.1804 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #343: GFLOPs: 118.4369. Time: 69.1761 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #344: GFLOPs: 115.4871. Time: 70.9430 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #345: GFLOPs: 113.3043. Time: 72.3097 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #346: GFLOPs: 113.3026. Time: 72.3108 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #347: GFLOPs: 113.3151. Time: 72.3028 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #348: GFLOPs: 113.1852. Time: 72.3858 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #349: GFLOPs: 110.2441. Time: 74.3169 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #350: GFLOPs: 113.1708. Time: 72.3950 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #351: GFLOPs: 113.1749. Time: 72.3924 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #352: GFLOPs: 113.1895. Time: 72.3830 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #353: GFLOPs: 110.7707. Time: 73.9636 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #354: GFLOPs: 110.7786. Time: 73.9584 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #355: GFLOPs: 105.6228. Time: 77.5685 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #356: GFLOPs: 105.6185. Time: 77.5716 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #357: GFLOPs: 110.7736. Time: 73.9617 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #358: GFLOPs: 110.7727. Time: 73.9623 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #359: GFLOPs: 105.6260. Time: 77.5661 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #360: GFLOPs: 105.4687. Time: 77.6818 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #361: GFLOPs: 105.4687. Time: 77.6818 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #362: GFLOPs: 110.1964. Time: 74.3491 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #363: GFLOPs: 113.4088. Time: 72.2431 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #364: GFLOPs: 113.4157. Time: 72.2387 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #365: GFLOPs: 113.4096. Time: 72.2425 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #366: GFLOPs: 113.4154. Time: 72.2389 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #367: GFLOPs: 113.3958. Time: 72.2514 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #368: GFLOPs: 113.4051. Time: 72.2454 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #369: GFLOPs: 110.7770. Time: 73.9594 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #370: GFLOPs: 110.7761. Time: 73.9600 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #371: GFLOPs: 110.7825. Time: 73.9557 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #372: GFLOPs: 105.6341. Time: 77.5602 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #373: GFLOPs: 105.6463. Time: 77.5512 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #374: GFLOPs: 105.6414. Time: 77.5548 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #375: GFLOPs: 105.6497. Time: 77.5487 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #376: GFLOPs: 55.7598. Time: 146.9338 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #377: GFLOPs: 105.4890. Time: 77.6669 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #378: GFLOPs: 110.7761. Time: 73.9600 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #379: GFLOPs: 110.8021. Time: 73.9426 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #380: GFLOPs: 105.1500. Time: 77.9173 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #381: GFLOPs: 55.7515. Time: 146.9557 us. Best GFLOPs: 130.7471
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #382: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(200) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(200) + ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(200) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(200) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[5, 1, 50, 2, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #383: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_2_i1_2_fused * T.int64(10) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(160)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(5), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_2_i1_2_fused * T.int64(10) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(10)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(500) + i0_2_i1_2_fused * T.int64(10) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 1, 50, 5, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 8, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69 = sch.split(loop=l67, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b71)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 20:38:39 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #384: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_1_i1_1_fused * T.int64(500) + i0_2_i1_2_fused * T.int64(10) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(20)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1)
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_1_i1_1_fused * T.int64(500) + i0_2_i1_2_fused * T.int64(10) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(10)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_1_i1_1_fused * T.int64(500) + i0_2_i1_2_fused * T.int64(10) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 2, 50, 5, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69 = sch.split(loop=l67, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-03-20 20:50:25 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 20:50:26 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 20:50:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1616 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2018 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2420 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2826 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3230 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:28 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 20:50:28 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 184 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:30 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 172 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:31 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 149 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:32 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 20:50:33 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9052  0.8993  0.8926  0.8751  0.8712  0.8616  0.8571  0.8571  0.8552  0.8552  0.8552  0.8552  0.8550  0.8545  0.8545  0.8545
[17 : 32]:	0.8545  0.8545  0.8545  0.8545  0.8545  0.8524  0.8524  0.8524  0.8524  0.8524  0.8524  0.8524  0.8524  0.8524  0.8524  0.8524
[33 : 48]:	0.8524  0.8524  0.8524  0.8524  0.8524  0.8509  0.8501  0.8501  0.8501  0.8501  0.8481  0.8480  0.8480  0.8477  0.8466  0.8466
[49 : 64]:	0.8442  0.8422  0.8422  0.8420  0.8415  0.8415  0.8415  0.8415  0.8396  0.8393  0.8393  0.8393  0.8393  0.8393  0.8393  0.8393
2024-03-20 20:50:33 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 20:50:33 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #385: GFLOPs: 118.6535. Time: 69.0498 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #386: GFLOPs: 115.8956. Time: 70.6929 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #387: GFLOPs: 115.9577. Time: 70.6551 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #388: GFLOPs: 115.3804. Time: 71.0086 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #389: GFLOPs: 95.3505. Time: 85.9251 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #390: GFLOPs: 95.3571. Time: 85.9191 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #391: GFLOPs: 60.3870. Time: 135.6748 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #392: GFLOPs: 59.1283. Time: 138.5631 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #393: GFLOPs: 72.8037. Time: 112.5354 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #394: GFLOPs: 72.8129. Time: 112.5213 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #395: GFLOPs: 80.1089. Time: 102.2732 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #396: GFLOPs: 80.1080. Time: 102.2744 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #397: GFLOPs: 95.3512. Time: 85.9245 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #398: GFLOPs: 72.8069. Time: 112.5306 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #399: GFLOPs: 80.1125. Time: 102.2686 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #400: GFLOPs: 80.1127. Time: 102.2684 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #401: GFLOPs: 80.1053. Time: 102.2779 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #402: GFLOPs: 72.8049. Time: 112.5337 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #403: GFLOPs: 72.7977. Time: 112.5448 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #404: GFLOPs: 80.1073. Time: 102.2754 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #405: GFLOPs: 72.8013. Time: 112.5393 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #406: GFLOPs: 80.1072. Time: 102.2755 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #407: GFLOPs: 72.8012. Time: 112.5393 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #408: GFLOPs: 80.1050. Time: 102.2782 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #409: GFLOPs: 80.1107. Time: 102.2710 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #410: GFLOPs: 80.1081. Time: 102.2743 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #411: GFLOPs: 80.1096. Time: 102.2723 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #412: GFLOPs: 72.7993. Time: 112.5423 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #413: GFLOPs: 72.8064. Time: 112.5313 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #414: GFLOPs: 80.1071. Time: 102.2756 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #415: GFLOPs: 72.8042. Time: 112.5348 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #416: GFLOPs: 80.1083. Time: 102.2741 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #417: GFLOPs: 72.8013. Time: 112.5391 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #418: GFLOPs: 72.7994. Time: 112.5421 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #419: GFLOPs: 80.1061. Time: 102.2769 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #420: GFLOPs: 72.7968. Time: 112.5461 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #421: GFLOPs: 72.7961. Time: 112.5472 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #422: GFLOPs: 95.2191. Time: 86.0436 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #423: GFLOPs: 83.3927. Time: 98.2460 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #424: GFLOPs: 83.3889. Time: 98.2504 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #425: GFLOPs: 83.3813. Time: 98.2594 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #426: GFLOPs: 83.3876. Time: 98.2520 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #427: GFLOPs: 95.3381. Time: 85.9363 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #428: GFLOPs: 95.3529. Time: 85.9229 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #429: GFLOPs: 95.3527. Time: 85.9231 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #430: GFLOPs: 83.3879. Time: 98.2517 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #431: GFLOPs: 83.3818. Time: 98.2588 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #432: GFLOPs: 83.3935. Time: 98.2450 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #433: GFLOPs: 83.3884. Time: 98.2510 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #434: GFLOPs: 75.3086. Time: 108.7923 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #435: GFLOPs: 75.3101. Time: 108.7901 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #436: GFLOPs: 95.2066. Time: 86.0549 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #437: GFLOPs: 75.3063. Time: 108.7956 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #438: GFLOPs: 75.3073. Time: 108.7943 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #439: GFLOPs: 75.3110. Time: 108.7889 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #440: GFLOPs: 75.3072. Time: 108.7944 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #441: GFLOPs: 83.3961. Time: 98.2420 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #442: GFLOPs: 75.3082. Time: 108.7929 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #443: GFLOPs: 75.3048. Time: 108.7979 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #444: GFLOPs: 75.3055. Time: 108.7969 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #445: GFLOPs: 75.3034. Time: 108.7998 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #446: GFLOPs: 25.5533. Time: 320.6243 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #447: GFLOPs: 2.7806. Time: 2946.5298 us. Best GFLOPs: 130.7471
2024-03-20 20:51:24 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #448: GFLOPs: 52.8489. Time: 155.0269 us. Best GFLOPs: 130.7471
2024-03-20 21:00:41 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:00:41 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:00:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 811 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1215 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1622 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2028 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2433 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2833 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3240 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3644 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4050 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:44 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 21:00:45 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 224 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:46 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:48 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 180 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:49 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 172 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:00:50 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3388  1.3366  1.3172  1.3086  1.2843  1.2828  1.2786  1.2602  1.2587  1.2223  1.1895  1.1838  1.1638  0.8800  0.8494  0.8419
[17 : 32]:	0.8419  0.8389  0.8389  0.8373  0.8373  0.8319  0.8284  0.8284  0.8283  0.8283  0.8283  0.8257  0.8257  0.8257  0.8201  0.8200
[33 : 48]:	0.8200  0.8200  0.8198  0.8197  0.8197  0.8197  0.8194  0.8188  0.8159  0.8159  0.8142  0.8134  0.8134  0.8134  0.8132  0.8132
[49 : 64]:	0.8113  0.8108  0.8098  0.8098  0.8098  0.8098  0.8098  0.8077  0.8077  0.8077  0.8051  0.8051  0.7996  0.7996  0.7996  0.7994
2024-03-20 21:00:50 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:00:50 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #449: GFLOPs: 21.2772. Time: 385.0594 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #450: GFLOPs: 23.8984. Time: 342.8265 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #451: GFLOPs: 23.8062. Time: 344.1535 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #452: GFLOPs: 23.8061. Time: 344.1557 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #453: GFLOPs: 23.8955. Time: 342.8677 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #454: GFLOPs: 23.8966. Time: 342.8526 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #455: GFLOPs: 23.8049. Time: 344.1732 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #456: GFLOPs: 23.8097. Time: 344.1033 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #457: GFLOPs: 23.8063. Time: 344.1532 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #458: GFLOPs: 23.8590. Time: 343.3929 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #459: GFLOPs: 23.9087. Time: 342.6786 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #460: GFLOPs: 24.0657. Time: 340.4433 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #461: GFLOPs: 23.8613. Time: 343.3588 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #462: GFLOPs: 112.4887. Time: 72.8340 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #463: GFLOPs: 12.2727. Time: 667.5797 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #464: GFLOPs: 106.2495. Time: 77.1109 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #465: GFLOPs: 106.2454. Time: 77.1139 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #466: GFLOPs: 106.2473. Time: 77.1125 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #467: GFLOPs: 106.2652. Time: 77.0995 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #468: GFLOPs: 106.3917. Time: 77.0079 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #469: GFLOPs: 106.3880. Time: 77.0106 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #470: GFLOPs: 111.9628. Time: 73.1761 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #471: GFLOPs: 111.9523. Time: 73.1830 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #472: GFLOPs: 111.7307. Time: 73.3281 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #473: GFLOPs: 105.8718. Time: 77.3861 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #474: GFLOPs: 105.8790. Time: 77.3808 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #475: GFLOPs: 105.8588. Time: 77.3955 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #476: GFLOPs: 111.7249. Time: 73.3319 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #477: GFLOPs: 111.7212. Time: 73.3343 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #478: GFLOPs: 111.7185. Time: 73.3361 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #479: GFLOPs: 105.9620. Time: 77.3202 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #480: GFLOPs: 105.8674. Time: 77.3893 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #481: GFLOPs: 105.8642. Time: 77.3916 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #482: GFLOPs: 105.8752. Time: 77.3836 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #483: GFLOPs: 105.9769. Time: 77.3093 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #484: GFLOPs: 105.8830. Time: 77.3778 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #485: GFLOPs: 105.8912. Time: 77.3718 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #486: GFLOPs: 105.8715. Time: 77.3862 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #487: GFLOPs: 111.7263. Time: 73.3310 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #488: GFLOPs: 105.8077. Time: 77.4330 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #489: GFLOPs: 105.8124. Time: 77.4295 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #490: GFLOPs: 105.8329. Time: 77.4145 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #491: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #492: GFLOPs: 105.2544. Time: 77.8400 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #493: GFLOPs: 105.2639. Time: 77.8330 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #494: GFLOPs: 105.3096. Time: 77.7992 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #495: GFLOPs: 105.9942. Time: 77.2967 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #496: GFLOPs: 106.0026. Time: 77.2906 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #497: GFLOPs: 105.1488. Time: 77.9181 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #498: GFLOPs: 105.1007. Time: 77.9538 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #499: GFLOPs: 105.0911. Time: 77.9609 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #500: GFLOPs: 105.1116. Time: 77.9457 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #501: GFLOPs: 105.1142. Time: 77.9438 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #502: GFLOPs: 105.1136. Time: 77.9443 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #503: GFLOPs: 105.1143. Time: 77.9437 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #504: GFLOPs: 105.1257. Time: 77.9353 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #505: GFLOPs: 105.1199. Time: 77.9396 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #506: GFLOPs: 105.1237. Time: 77.9368 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #507: GFLOPs: 105.1284. Time: 77.9333 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #508: GFLOPs: 105.1225. Time: 77.9376 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #509: GFLOPs: 56.0108. Time: 146.2753 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #510: GFLOPs: 8.2376. Time: 994.5879 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #511: GFLOPs: 8.4559. Time: 968.9089 us. Best GFLOPs: 130.7471
2024-03-20 21:01:36 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #512: GFLOPs: 5.4182. Time: 1512.1385 us. Best GFLOPs: 130.7471
2024-03-20 21:10:30 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:10:30 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:10:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 811 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1621 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2026 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2432 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2839 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3246 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3651 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4052 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4458 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:33 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-20 21:10:34 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 199 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 189 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:37 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:38 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:10:39 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8295  0.8264  0.8244  0.8241  0.8231  0.8231  0.8225  0.8225  0.8202  0.8202  0.8198  0.8195  0.8176  0.8176  0.8167  0.8167
[17 : 32]:	0.8167  0.8167  0.8153  0.8153  0.8124  0.8110  0.8103  0.8060  0.8050  0.8050  0.8032  0.7972  0.7962  0.7962  0.7884  0.7884
[33 : 48]:	0.7873  0.7873  0.7849  0.7846  0.7846  0.7831  0.7775  0.7350  0.7330  0.7252  0.7252  0.7154  0.7128  0.7108  0.7105  0.7105
[49 : 64]:	0.7105  0.7105  0.7105  0.7088  0.7085  0.7085  0.7085  0.7085  0.7085  0.7065  0.7052  0.7049  0.7049  0.7049  0.7049  0.7049
2024-03-20 21:10:39 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:10:39 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #513: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #514: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 2, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #515: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #516: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #517: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 1, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #518: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #519: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 16, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #520: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #521: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #522: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 2, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #523: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 32, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #524: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 4, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #525: GFLOPs: 106.2749. Time: 77.0925 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #526: GFLOPs: 106.2842. Time: 77.0857 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #527: GFLOPs: 106.0671. Time: 77.2436 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #528: GFLOPs: 106.0785. Time: 77.2352 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #529: GFLOPs: 106.0785. Time: 77.2352 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #530: GFLOPs: 105.7599. Time: 77.4679 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #531: GFLOPs: 106.2804. Time: 77.0885 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #532: GFLOPs: 106.2726. Time: 77.0942 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #533: GFLOPs: 106.0799. Time: 77.2342 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #534: GFLOPs: 105.7062. Time: 77.5073 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #535: GFLOPs: 104.6086. Time: 78.3205 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #536: GFLOPs: 104.6084. Time: 78.3206 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #537: GFLOPs: 104.6024. Time: 78.3252 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #538: GFLOPs: 104.6025. Time: 78.3251 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #539: GFLOPs: 104.4973. Time: 78.4039 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #540: GFLOPs: 104.4946. Time: 78.4060 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #541: GFLOPs: 104.4933. Time: 78.4069 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #542: GFLOPs: 104.4862. Time: 78.4122 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #543: GFLOPs: 104.6313. Time: 78.3035 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #544: GFLOPs: 104.6346. Time: 78.3011 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #545: GFLOPs: 104.6360. Time: 78.3000 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #546: GFLOPs: 104.2513. Time: 78.5890 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #547: GFLOPs: 104.3299. Time: 78.5297 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #548: GFLOPs: 104.3394. Time: 78.5226 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #549: GFLOPs: 104.3376. Time: 78.5239 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #550: GFLOPs: 104.6326. Time: 78.3025 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #551: GFLOPs: 104.6359. Time: 78.3001 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #552: GFLOPs: 60.7391. Time: 134.8884 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #553: GFLOPs: 94.2211. Time: 86.9550 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #554: GFLOPs: 61.8444. Time: 132.4776 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #555: GFLOPs: 60.7329. Time: 134.9021 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #556: GFLOPs: 61.9145. Time: 132.3276 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #557: GFLOPs: 94.2187. Time: 86.9572 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #558: GFLOPs: 55.8705. Time: 146.6428 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #559: GFLOPs: 55.8891. Time: 146.5938 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #560: GFLOPs: 55.8884. Time: 146.5957 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #561: GFLOPs: 55.8851. Time: 146.6044 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #562: GFLOPs: 55.9388. Time: 146.4637 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #563: GFLOPs: 55.9595. Time: 146.4095 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #564: GFLOPs: 55.8836. Time: 146.6084 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #565: GFLOPs: 55.9427. Time: 146.4535 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #566: GFLOPs: 55.8843. Time: 146.6064 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #567: GFLOPs: 55.8857. Time: 146.6028 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #568: GFLOPs: 55.8862. Time: 146.6016 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #569: GFLOPs: 55.9545. Time: 146.4225 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #570: GFLOPs: 55.8835. Time: 146.6085 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #571: GFLOPs: 55.6652. Time: 147.1836 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #572: GFLOPs: 55.7330. Time: 147.0045 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #573: GFLOPs: 55.6390. Time: 147.2527 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #574: GFLOPs: 3.2707. Time: 2504.9601 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #575: GFLOPs: 4.9047. Time: 1670.4512 us. Best GFLOPs: 130.7471
2024-03-20 21:11:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #576: GFLOPs: 13.9243. Time: 588.3964 us. Best GFLOPs: 130.7471
2024-03-20 21:18:56 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:18:56 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:18:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1218 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1624 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2029 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2432 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2836 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3243 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3649 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4057 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4461 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4865 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:18:59 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-03-20 21:19:00 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 191 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:19:01 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:19:03 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 164 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:19:04 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 174 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:19:05 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9548  0.9469  0.9298  0.9296  0.9296  0.9231  0.9142  0.9132  0.9043  0.8792  0.8726  0.8693  0.8627  0.8304  0.8250  0.8237
[17 : 32]:	0.8215  0.8191  0.8132  0.8110  0.8110  0.8101  0.8086  0.8034  0.8034  0.8034  0.8018  0.8018  0.8018  0.8018  0.7988  0.7988
[33 : 48]:	0.7941  0.7872  0.7862  0.7853  0.7806  0.7708  0.7708  0.7698  0.7692  0.7589  0.7589  0.7435  0.7282  0.7277  0.7140  0.7029
[49 : 64]:	0.6972  0.6955  0.6921  0.6871  0.6837  0.6813  0.6796  0.6792  0.6792  0.6763  0.6758  0.6758  0.6755  0.6752  0.6752  0.6738
2024-03-20 21:19:05 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:19:05 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #577: GFLOPs: 12.5916. Time: 650.6697 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #578: GFLOPs: 3.2641. Time: 2510.0504 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #579: GFLOPs: 12.5907. Time: 650.7187 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #580: GFLOPs: 47.4764. Time: 172.5698 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #581: GFLOPs: 47.4714. Time: 172.5880 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #582: GFLOPs: 11.5844. Time: 707.2435 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #583: GFLOPs: 9.6637. Time: 847.8112 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #584: GFLOPs: 12.5922. Time: 650.6390 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #585: GFLOPs: 9.8470. Time: 832.0264 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #586: GFLOPs: 9.4351. Time: 868.3520 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #587: GFLOPs: 9.6645. Time: 847.7418 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #588: GFLOPs: 9.8480. Time: 831.9419 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #589: GFLOPs: 9.8490. Time: 831.8604 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #590: GFLOPs: 111.6874. Time: 73.3565 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #591: GFLOPs: 111.6905. Time: 73.3545 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #592: GFLOPs: 105.9518. Time: 77.3276 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #593: GFLOPs: 105.9594. Time: 77.3221 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #594: GFLOPs: 106.0608. Time: 77.2482 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #595: GFLOPs: 105.1436. Time: 77.9220 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #596: GFLOPs: 105.4213. Time: 77.7168 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #597: GFLOPs: 105.4144. Time: 77.7218 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #598: GFLOPs: 105.1453. Time: 77.9207 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #599: GFLOPs: 105.8404. Time: 77.4090 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #600: GFLOPs: 105.1480. Time: 77.9187 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #601: GFLOPs: 105.1481. Time: 77.9186 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #602: GFLOPs: 105.1489. Time: 77.9181 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #603: GFLOPs: 105.1494. Time: 77.9177 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #604: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #605: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #606: GFLOPs: 105.1488. Time: 77.9181 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #607: GFLOPs: 105.1482. Time: 77.9186 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #608: GFLOPs: 105.1480. Time: 77.9187 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #609: GFLOPs: 9.2567. Time: 885.0894 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #610: GFLOPs: 105.1462. Time: 77.9201 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #611: GFLOPs: 3.4737. Time: 2358.6053 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #612: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #613: GFLOPs: 105.1481. Time: 77.9187 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #614: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #615: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #616: GFLOPs: 8.4549. Time: 969.0289 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #617: GFLOPs: 49.0173. Time: 167.1451 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #618: GFLOPs: 8.4547. Time: 969.0486 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #619: GFLOPs: 8.4549. Time: 969.0289 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #620: GFLOPs: 95.3230. Time: 85.9499 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #621: GFLOPs: 5.6655. Time: 1446.1221 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #622: GFLOPs: 94.9281. Time: 86.3074 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #623: GFLOPs: 63.4106. Time: 129.2055 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #624: GFLOPs: 95.3120. Time: 85.9598 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #625: GFLOPs: 58.1619. Time: 140.8655 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #626: GFLOPs: 58.1541. Time: 140.8844 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #627: GFLOPs: 58.1607. Time: 140.8683 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #628: GFLOPs: 94.9429. Time: 86.2940 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #629: GFLOPs: 5.8059. Time: 1411.1437 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #630: GFLOPs: 58.5009. Time: 140.0492 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #631: GFLOPs: 59.9110. Time: 136.7528 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #632: GFLOPs: 83.2186. Time: 98.4515 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #633: GFLOPs: 83.2165. Time: 98.4540 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #634: GFLOPs: 59.9238. Time: 136.7236 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #635: GFLOPs: 83.2204. Time: 98.4495 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #636: GFLOPs: 83.2245. Time: 98.4446 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #637: GFLOPs: 81.3138. Time: 100.7579 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #638: GFLOPs: 62.3194. Time: 131.4679 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #639: GFLOPs: 8.8354. Time: 927.2889 us. Best GFLOPs: 130.7471
2024-03-20 21:19:48 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #640: GFLOPs: 5.8460. Time: 1401.4720 us. Best GFLOPs: 130.7471
2024-03-20 21:29:52 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:29:52 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:29:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 812 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1620 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2027 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2430 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2832 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3239 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3649 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4058 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4464 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4870 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:55 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 21:29:56 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 169 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:58 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 199 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:29:59 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 185 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:30:00 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 199 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:30:01 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8311  0.8302  0.8211  0.8178  0.8083  0.7977  0.7969  0.7969  0.7969  0.7963  0.7963  0.7950  0.7913  0.7913  0.7876  0.7216
[17 : 32]:	0.7184  0.7157  0.7103  0.7097  0.7038  0.7038  0.7017  0.7016  0.6753  0.6732  0.6726  0.6693  0.6693  0.6673  0.6667  0.6667
[33 : 48]:	0.6647  0.6640  0.6640  0.6640  0.6639  0.6639  0.6616  0.6616  0.6610  0.6610  0.6610  0.6604  0.6604  0.6583  0.6583  0.6583
[49 : 64]:	0.6574  0.6569  0.6552  0.6535  0.6535  0.6535  0.6530  0.6516  0.6501  0.6501  0.6499  0.6499  0.6476  0.6447  0.6447  0.6447
2024-03-20 21:30:01 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:30:01 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #641: GFLOPs: 111.6318. Time: 73.3931 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #642: GFLOPs: 106.4361. Time: 76.9758 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #643: GFLOPs: 111.7842. Time: 73.2930 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #644: GFLOPs: 106.4310. Time: 76.9795 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #645: GFLOPs: 106.1618. Time: 77.1747 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #646: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #647: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #648: GFLOPs: 105.1488. Time: 77.9181 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #649: GFLOPs: 105.1491. Time: 77.9179 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #650: GFLOPs: 105.1496. Time: 77.9176 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #651: GFLOPs: 105.1488. Time: 77.9181 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #652: GFLOPs: 105.1488. Time: 77.9181 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #653: GFLOPs: 105.1136. Time: 77.9443 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #654: GFLOPs: 105.1036. Time: 77.9516 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #655: GFLOPs: 105.1486. Time: 77.9183 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #656: GFLOPs: 1.8974. Time: 4317.9947 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #657: GFLOPs: 95.5882. Time: 85.7114 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #658: GFLOPs: 95.9443. Time: 85.3933 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #659: GFLOPs: 95.4841. Time: 85.8048 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #660: GFLOPs: 95.5853. Time: 85.7140 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #661: GFLOPs: 95.9394. Time: 85.3977 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #662: GFLOPs: 95.9394. Time: 85.3976 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #663: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #664: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #665: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #666: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(32) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[128, 8, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #667: GFLOPs: 82.9847. Time: 98.7290 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #668: GFLOPs: 95.0714. Time: 86.1773 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #669: GFLOPs: 53.8416. Time: 152.1687 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #670: GFLOPs: 82.9802. Time: 98.7344 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #671: GFLOPs: 82.9726. Time: 98.7435 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #672: GFLOPs: 82.9710. Time: 98.7454 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #673: GFLOPs: 94.6674. Time: 86.5451 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #674: GFLOPs: 54.0545. Time: 151.5692 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #675: GFLOPs: 54.0504. Time: 151.5808 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #676: GFLOPs: 54.0506. Time: 151.5802 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #677: GFLOPs: 53.8473. Time: 152.1524 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #678: GFLOPs: 53.8473. Time: 152.1524 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #679: GFLOPs: 83.1157. Time: 98.5734 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #680: GFLOPs: 83.1276. Time: 98.5594 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #681: GFLOPs: 83.1194. Time: 98.5690 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #682: GFLOPs: 83.1260. Time: 98.5613 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #683: GFLOPs: 83.1231. Time: 98.5647 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #684: GFLOPs: 83.1292. Time: 98.5575 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #685: GFLOPs: 83.0857. Time: 98.6090 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #686: GFLOPs: 82.9826. Time: 98.7316 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #687: GFLOPs: 82.9808. Time: 98.7337 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #688: GFLOPs: 82.9800. Time: 98.7347 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #689: GFLOPs: 95.0860. Time: 86.1641 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #690: GFLOPs: 82.9685. Time: 98.7483 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #691: GFLOPs: 82.9659. Time: 98.7514 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #692: GFLOPs: 82.9810. Time: 98.7334 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #693: GFLOPs: 82.9735. Time: 98.7423 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #694: GFLOPs: 82.9826. Time: 98.7315 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #695: GFLOPs: 82.9822. Time: 98.7320 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #696: GFLOPs: 82.9727. Time: 98.7433 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #697: GFLOPs: 82.9759. Time: 98.7395 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #698: GFLOPs: 82.9709. Time: 98.7454 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #699: GFLOPs: 82.9710. Time: 98.7454 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #700: GFLOPs: 82.9675. Time: 98.7495 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #701: GFLOPs: 81.2903. Time: 100.7869 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #702: GFLOPs: 8.9974. Time: 910.5967 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #703: GFLOPs: 8.3887. Time: 976.6655 us. Best GFLOPs: 130.7471
2024-03-20 21:30:47 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #704: GFLOPs: 39.5811. Time: 206.9928 us. Best GFLOPs: 130.7471
2024-03-20 21:34:16 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:34:16 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:34:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1610 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2010 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2419 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2827 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3230 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:18 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 21:34:19 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 200 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 180 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:23 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 172 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:34:23 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8198  0.8198  0.8179  0.8064  0.8000  0.7899  0.7899  0.7819  0.7696  0.7313  0.7293  0.7276  0.7264  0.7253  0.7222  0.7204
[17 : 32]:	0.7203  0.7164  0.7014  0.6440  0.6437  0.6437  0.6435  0.6427  0.6422  0.6410  0.6409  0.6409  0.6396  0.6391  0.6346  0.6333
[33 : 48]:	0.6333  0.6329  0.6315  0.6276  0.6276  0.6254  0.6254  0.6254  0.6254  0.6204  0.6204  0.6165  0.6165  0.6165  0.6165  0.6165
[49 : 64]:	0.6165  0.6165  0.6127  0.6103  0.6085  0.6028  0.6028  0.6023  0.6022  0.6020  0.6018  0.6018  0.6018  0.6011  0.6000  0.6000
2024-03-20 21:34:23 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:34:23 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #705: GFLOPs: 106.1430. Time: 77.1883 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #706: GFLOPs: 106.0042. Time: 77.2894 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #707: GFLOPs: 106.0069. Time: 77.2874 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #708: GFLOPs: 105.1483. Time: 77.9185 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #709: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #710: GFLOPs: 105.1486. Time: 77.9183 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #711: GFLOPs: 105.1488. Time: 77.9181 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #712: GFLOPs: 105.1491. Time: 77.9179 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #713: GFLOPs: 49.9300. Time: 164.0897 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #714: GFLOPs: 94.4204. Time: 86.7715 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #715: GFLOPs: 94.4032. Time: 86.7873 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #716: GFLOPs: 94.4626. Time: 86.7327 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #717: GFLOPs: 94.8407. Time: 86.3870 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #718: GFLOPs: 94.4113. Time: 86.7798 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #719: GFLOPs: 94.8231. Time: 86.4030 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #720: GFLOPs: 94.4542. Time: 86.7404 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #721: GFLOPs: 94.8457. Time: 86.3824 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #722: GFLOPs: 94.4536. Time: 86.7410 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #723: GFLOPs: 94.8480. Time: 86.3803 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #724: GFLOPs: 82.3354. Time: 99.5076 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #725: GFLOPs: 82.3217. Time: 99.5242 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #726: GFLOPs: 82.3312. Time: 99.5127 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #727: GFLOPs: 58.9514. Time: 138.9789 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #728: GFLOPs: 82.3369. Time: 99.5058 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #729: GFLOPs: 82.3254. Time: 99.5198 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #730: GFLOPs: 58.9431. Time: 138.9985 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #731: GFLOPs: 82.3222. Time: 99.5235 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #732: GFLOPs: 82.3325. Time: 99.5111 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #733: GFLOPs: 82.3423. Time: 99.4993 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #734: GFLOPs: 58.9406. Time: 139.0045 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #735: GFLOPs: 82.3266. Time: 99.5183 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #736: GFLOPs: 82.3223. Time: 99.5235 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #737: GFLOPs: 82.3311. Time: 99.5128 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #738: GFLOPs: 82.3275. Time: 99.5171 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #739: GFLOPs: 82.3295. Time: 99.5148 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #740: GFLOPs: 82.3285. Time: 99.5159 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #741: GFLOPs: 82.3292. Time: 99.5152 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #742: GFLOPs: 82.5165. Time: 99.2893 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #743: GFLOPs: 82.9792. Time: 98.7356 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #744: GFLOPs: 82.5219. Time: 99.2828 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #745: GFLOPs: 82.5117. Time: 99.2950 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #746: GFLOPs: 82.5107. Time: 99.2963 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #747: GFLOPs: 82.4965. Time: 99.3133 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #748: GFLOPs: 34.4725. Time: 237.6675 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #749: GFLOPs: 34.1510. Time: 239.9051 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #750: GFLOPs: 34.4960. Time: 237.5060 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #751: GFLOPs: 34.4688. Time: 237.6935 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #752: GFLOPs: 34.4709. Time: 237.6784 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #753: GFLOPs: 34.4685. Time: 237.6953 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #754: GFLOPs: 34.4677. Time: 237.7007 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #755: GFLOPs: 94.5501. Time: 86.6524 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #756: GFLOPs: 80.5397. Time: 101.7262 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #757: GFLOPs: 80.5368. Time: 101.7299 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #758: GFLOPs: 70.7324. Time: 115.8309 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #759: GFLOPs: 70.7327. Time: 115.8305 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #760: GFLOPs: 70.7363. Time: 115.8245 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #761: GFLOPs: 57.7574. Time: 141.8520 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #762: GFLOPs: 58.9473. Time: 138.9885 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #763: GFLOPs: 80.5263. Time: 101.7432 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #764: GFLOPs: 80.5353. Time: 101.7318 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #765: GFLOPs: 80.5283. Time: 101.7407 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #766: GFLOPs: 5.3513. Time: 1531.0351 us. Best GFLOPs: 130.7471
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #767: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  312: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  311: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  310: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  309: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  308: tvm::transform::Pass::operator()(tvm::IRModule) const
  307: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  306: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  305: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  304: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  303: _ZN3tvm7runtime13PackedFuncObj
  302: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  301: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  300: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  299: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  298: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  297: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  296: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  295: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  294: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  269: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  268: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  267: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  266: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  265: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  264: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  263: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  262: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  261: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  260: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  259: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  258: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  257: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  256: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  255: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  254: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  253: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  252: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  251: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  250: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  249: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  248: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  247: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  246: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  245: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  244: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  243: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  242: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  241: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  240: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  239: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  238: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  237: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  236: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  235: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  234: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  233: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  232: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  231: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  230: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  229: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  228: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  227: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  226: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  225: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  224: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  223: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  222: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  221: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  220: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  219: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  218: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  217: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  216: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  215: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  214: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  213: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  212: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  211: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  210: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  209: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  208: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  207: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  206: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  205: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  204: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  202: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  201: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  200: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  199: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  198: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  197: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  196: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  195: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  194: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  193: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  192: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  191: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  190: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  189: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  188: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  183: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  182: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  181: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  180: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  179: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  178: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  176: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  175: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  173: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  172: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  171: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  170: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  169: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  168: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  167: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  166: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  165: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  164: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  163: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  162: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  161: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  160: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  159: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  158: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  157: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  156: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  154: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  153: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  152: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  151: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  150: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  149: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  148: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  147: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  146: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  145: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  144: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  143: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  142: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  141: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  140: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  139: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  138: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  137: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  136: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  135: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  134: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  133: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  132: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  131: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  130: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  117: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  116: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  115: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  114: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  113: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  112: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  111: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  110: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  109: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  108: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  107: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  106: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  105: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  104: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  82: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  81: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  80: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  79: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  78: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  77: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  76: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  75: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(20), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_1_i1_1_fused * T.int64(50) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_1_i1_1_fused * T.int64(50) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_1_i1_1_fused * T.int64(50) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 20, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-03-20 21:35:11 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #768: GFLOPs: 23.6840. Time: 345.9300 us. Best GFLOPs: 130.7471
2024-03-20 21:41:11 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:41:11 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:41:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 400 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1208 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1613 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2019 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2421 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2820 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:13 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 21:41:14 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 192 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:15 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 175 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:16 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 179 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:18 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 169 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:41:18 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8338  0.8333  0.8233  0.8233  0.8228  0.8228  0.8188  0.8108  0.8001  0.7989  0.7782  0.7311  0.7266  0.7248  0.7096  0.7089
[17 : 32]:	0.6993  0.6833  0.6774  0.6390  0.6371  0.6346  0.6341  0.6328  0.6328  0.6325  0.6245  0.6218  0.6218  0.6193  0.6168  0.6070
[33 : 48]:	0.6070  0.6070  0.6070  0.6066  0.6066  0.6066  0.6066  0.6044  0.6044  0.6044  0.6019  0.6019  0.6014  0.5979  0.5979  0.5976
[49 : 64]:	0.5976  0.5965  0.5961  0.5916  0.5916  0.5913  0.5913  0.5913  0.5706  0.5703  0.5703  0.5703  0.5701  0.5597  0.5589  0.5589
2024-03-20 21:41:18 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:41:18 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #769: GFLOPs: 110.2156. Time: 74.3361 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #770: GFLOPs: 112.4427. Time: 72.8638 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #771: GFLOPs: 106.3873. Time: 77.0111 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #772: GFLOPs: 106.3865. Time: 77.0116 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #773: GFLOPs: 106.3838. Time: 77.0136 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #774: GFLOPs: 106.3957. Time: 77.0050 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #775: GFLOPs: 106.2022. Time: 77.1453 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #776: GFLOPs: 105.1490. Time: 77.9180 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #777: GFLOPs: 105.1487. Time: 77.9182 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #778: GFLOPs: 105.1494. Time: 77.9177 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #779: GFLOPs: 122.9081. Time: 66.6596 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #780: GFLOPs: 95.5101. Time: 85.7815 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #781: GFLOPs: 95.4985. Time: 85.7919 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #782: GFLOPs: 95.6021. Time: 85.6990 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #783: GFLOPs: 95.9028. Time: 85.4302 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #784: GFLOPs: 95.5084. Time: 85.7830 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #785: GFLOPs: 95.5862. Time: 85.7132 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #786: GFLOPs: 99.2069. Time: 82.5850 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #787: GFLOPs: 95.5238. Time: 85.7692 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #788: GFLOPs: 61.9372. Time: 132.2792 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #789: GFLOPs: 83.2778. Time: 98.3816 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #790: GFLOPs: 83.2706. Time: 98.3901 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #791: GFLOPs: 83.2719. Time: 98.3885 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #792: GFLOPs: 83.2799. Time: 98.3790 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #793: GFLOPs: 83.2760. Time: 98.3837 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #794: GFLOPs: 83.2719. Time: 98.3885 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #795: GFLOPs: 83.2737. Time: 98.3863 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #796: GFLOPs: 83.3372. Time: 98.3115 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #797: GFLOPs: 83.2669. Time: 98.3945 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #798: GFLOPs: 83.2741. Time: 98.3860 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #799: GFLOPs: 83.3872. Time: 98.2525 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #800: GFLOPs: 82.2357. Time: 99.6283 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #801: GFLOPs: 82.2344. Time: 99.6299 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #802: GFLOPs: 82.2322. Time: 99.6324 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #803: GFLOPs: 82.2346. Time: 99.6296 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #804: GFLOPs: 82.2307. Time: 99.6344 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #805: GFLOPs: 82.2275. Time: 99.6382 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #806: GFLOPs: 82.2307. Time: 99.6343 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #807: GFLOPs: 82.2281. Time: 99.6375 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #808: GFLOPs: 81.8979. Time: 100.0392 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #809: GFLOPs: 81.8958. Time: 100.0418 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #810: GFLOPs: 81.8942. Time: 100.0437 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #811: GFLOPs: 81.9026. Time: 100.0335 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #812: GFLOPs: 81.8905. Time: 100.0482 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #813: GFLOPs: 81.8978. Time: 100.0393 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #814: GFLOPs: 80.6540. Time: 101.5821 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #815: GFLOPs: 80.6512. Time: 101.5856 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #816: GFLOPs: 80.6515. Time: 101.5852 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #817: GFLOPs: 80.6493. Time: 101.5880 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #818: GFLOPs: 78.9666. Time: 103.7527 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #819: GFLOPs: 78.9641. Time: 103.7561 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #820: GFLOPs: 71.4382. Time: 114.6865 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #821: GFLOPs: 71.4359. Time: 114.6902 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #822: GFLOPs: 71.4359. Time: 114.6902 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #823: GFLOPs: 71.4365. Time: 114.6893 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #824: GFLOPs: 71.4346. Time: 114.6923 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #825: GFLOPs: 75.3404. Time: 108.7464 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #826: GFLOPs: 75.3447. Time: 108.7402 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #827: GFLOPs: 75.3380. Time: 108.7499 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #828: GFLOPs: 75.3396. Time: 108.7475 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #829: GFLOPs: 63.2091. Time: 129.6174 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #830: GFLOPs: 3.2630. Time: 2510.8736 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #831: GFLOPs: 4.8987. Time: 1672.4992 us. Best GFLOPs: 130.7471
2024-03-20 21:42:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #832: GFLOPs: 4.7666. Time: 1718.8273 us. Best GFLOPs: 130.7471
2024-03-20 21:48:08 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:48:08 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:48:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1214 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2025 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2433 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2835 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3242 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3644 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4050 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:10 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 21:48:11 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 203 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:12 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 185 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:14 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 154 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:15 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 197 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:48:16 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9124  0.9108  0.9069  0.8182  0.8171  0.8112  0.8091  0.7968  0.7880  0.7833  0.7466  0.7463  0.7348  0.7348  0.7348  0.7265
[17 : 32]:	0.7265  0.7249  0.7177  0.7030  0.7030  0.6847  0.6694  0.6690  0.6348  0.6344  0.6268  0.6268  0.6251  0.6251  0.6164  0.6164
[33 : 48]:	0.6164  0.6164  0.6161  0.6124  0.6124  0.6124  0.6121  0.6121  0.6107  0.6091  0.6030  0.5927  0.5861  0.5861  0.5855  0.5848
[49 : 64]:	0.5848  0.5815  0.5812  0.5801  0.5674  0.5631  0.5595  0.5581  0.5397  0.5370  0.5358  0.5317  0.5304  0.5284  0.5282  0.5282
2024-03-20 21:48:16 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:48:16 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #833: GFLOPs: 130.0180. Time: 63.0143 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #834: GFLOPs: 125.8757. Time: 65.0880 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #835: GFLOPs: 130.3655. Time: 62.8464 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #836: GFLOPs: 126.1378. Time: 64.9528 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #837: GFLOPs: 106.3503. Time: 77.0378 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #838: GFLOPs: 105.1492. Time: 77.9179 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #839: GFLOPs: 126.1108. Time: 64.9667 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #840: GFLOPs: 105.1486. Time: 77.9183 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #841: GFLOPs: 116.0857. Time: 70.5771 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #842: GFLOPs: 114.8736. Time: 71.3219 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #843: GFLOPs: 113.0886. Time: 72.4477 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #844: GFLOPs: 94.9944. Time: 86.2472 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #845: GFLOPs: 109.1508. Time: 75.0613 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #846: GFLOPs: 109.1546. Time: 75.0587 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #847: GFLOPs: 109.1502. Time: 75.0617 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #848: GFLOPs: 105.1485. Time: 77.9184 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #849: GFLOPs: 105.1484. Time: 77.9184 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #850: GFLOPs: 95.1106. Time: 86.1419 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #851: GFLOPs: 94.9914. Time: 86.2499 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #852: GFLOPs: 103.0218. Time: 79.5269 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #853: GFLOPs: 103.0129. Time: 79.5337 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #854: GFLOPs: 117.0524. Time: 69.9943 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #855: GFLOPs: 94.7342. Time: 86.4841 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #856: GFLOPs: 95.0907. Time: 86.1598 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #857: GFLOPs: 83.9322. Time: 97.6145 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #858: GFLOPs: 83.9348. Time: 97.6115 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #859: GFLOPs: 82.1656. Time: 99.7133 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #860: GFLOPs: 82.1620. Time: 99.7177 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #861: GFLOPs: 83.9428. Time: 97.6022 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #862: GFLOPs: 83.9383. Time: 97.6074 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #863: GFLOPs: 80.6184. Time: 101.6270 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #864: GFLOPs: 80.6205. Time: 101.6242 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #865: GFLOPs: 80.6226. Time: 101.6217 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #866: GFLOPs: 80.6211. Time: 101.6236 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #867: GFLOPs: 82.1584. Time: 99.7220 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #868: GFLOPs: 78.9233. Time: 103.8097 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #869: GFLOPs: 78.9254. Time: 103.8069 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #870: GFLOPs: 78.9227. Time: 103.8104 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #871: GFLOPs: 82.1606. Time: 99.7193 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #872: GFLOPs: 82.1573. Time: 99.7233 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #873: GFLOPs: 82.1527. Time: 99.7289 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #874: GFLOPs: 107.3967. Time: 76.2872 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #875: GFLOPs: 34.4955. Time: 237.5090 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #876: GFLOPs: 82.1520. Time: 99.7298 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #877: GFLOPs: 72.2808. Time: 113.3495 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #878: GFLOPs: 72.2670. Time: 113.3713 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #879: GFLOPs: 96.5749. Time: 84.8357 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #880: GFLOPs: 72.2734. Time: 113.3612 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #881: GFLOPs: 72.2785. Time: 113.3532 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #882: GFLOPs: 72.2773. Time: 113.3551 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #883: GFLOPs: 69.7690. Time: 117.4304 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #884: GFLOPs: 72.2744. Time: 113.3597 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #885: GFLOPs: 60.3937. Time: 135.6598 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #886: GFLOPs: 60.3961. Time: 135.6544 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #887: GFLOPs: 72.9754. Time: 112.2707 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #888: GFLOPs: 60.8995. Time: 134.5332 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #889: GFLOPs: 60.9018. Time: 134.5280 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #890: GFLOPs: 60.3999. Time: 135.6459 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #891: GFLOPs: 60.4015. Time: 135.6424 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #892: GFLOPs: 60.4095. Time: 135.6243 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #893: GFLOPs: 60.0813. Time: 136.3651 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #894: GFLOPs: 3.3820. Time: 2422.5402 us. Best GFLOPs: 130.7471
2024-03-20 21:49:03 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #895: GFLOPs: 3.3122. Time: 2473.5595 us. Best GFLOPs: 130.7471
2024-03-20 21:53:23 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 21:53:24 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 21:53:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 812 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1218 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1623 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2025 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2430 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2835 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3240 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3640 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:26 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 21:53:27 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:28 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 205 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 171 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:31 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 158 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 21:53:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9823  0.9798  0.9765  0.9726  0.9537  0.9503  0.9425  0.9367  0.8824  0.8824  0.8791  0.8754  0.8661  0.8640  0.8638  0.8517
[17 : 32]:	0.8481  0.8423  0.8417  0.8372  0.8274  0.8214  0.8210  0.8208  0.8199  0.8181  0.8163  0.8159  0.8065  0.8032  0.7998  0.7998
[33 : 48]:	0.7982  0.7982  0.7969  0.7960  0.7960  0.7952  0.7913  0.7910  0.7898  0.7409  0.7254  0.7217  0.7169  0.7013  0.7013  0.6988
[49 : 64]:	0.6988  0.6988  0.6954  0.6916  0.6864  0.6860  0.6476  0.6420  0.6405  0.6356  0.6348  0.6348  0.6344  0.6342  0.6293  0.6268
2024-03-20 21:53:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 21:53:32 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #896: GFLOPs: 126.1385. Time: 64.9524 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #897: GFLOPs: 129.6383. Time: 63.1989 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #898: GFLOPs: 129.4008. Time: 63.3149 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #899: GFLOPs: 125.9583. Time: 65.0453 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #900: GFLOPs: 123.6613. Time: 66.2536 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #901: GFLOPs: 122.1877. Time: 67.0526 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #902: GFLOPs: 125.0628. Time: 65.5111 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #903: GFLOPs: 125.0718. Time: 65.5064 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #904: GFLOPs: 115.0931. Time: 71.1859 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #905: GFLOPs: 115.0941. Time: 71.1852 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #906: GFLOPs: 114.3572. Time: 71.6440 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #907: GFLOPs: 115.8185. Time: 70.7400 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #908: GFLOPs: 115.7124. Time: 70.8049 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #909: GFLOPs: 124.5802. Time: 65.7649 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #910: GFLOPs: 122.9505. Time: 66.6366 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #911: GFLOPs: 111.8738. Time: 73.2343 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #912: GFLOPs: 108.4385. Time: 75.5544 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #913: GFLOPs: 108.4351. Time: 75.5567 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #914: GFLOPs: 108.4369. Time: 75.5555 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #915: GFLOPs: 108.4447. Time: 75.5500 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #916: GFLOPs: 89.5037. Time: 91.5381 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #917: GFLOPs: 108.4390. Time: 75.5540 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #918: GFLOPs: 108.4390. Time: 75.5540 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #919: GFLOPs: 124.1404. Time: 65.9979 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #920: GFLOPs: 89.5003. Time: 91.5416 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #921: GFLOPs: 103.5746. Time: 79.1024 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #922: GFLOPs: 108.4399. Time: 75.5533 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #923: GFLOPs: 108.4390. Time: 75.5540 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #924: GFLOPs: 104.6025. Time: 78.3251 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #925: GFLOPs: 96.5389. Time: 84.8673 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #926: GFLOPs: 104.5032. Time: 78.3995 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #927: GFLOPs: 104.4932. Time: 78.4070 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #928: GFLOPs: 121.8869. Time: 67.2180 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #929: GFLOPs: 121.8894. Time: 67.2166 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #930: GFLOPs: 104.4954. Time: 78.4054 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #931: GFLOPs: 104.4914. Time: 78.4084 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #932: GFLOPs: 104.4936. Time: 78.4067 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #933: GFLOPs: 89.5484. Time: 91.4924 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #934: GFLOPs: 123.0445. Time: 66.5857 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #935: GFLOPs: 123.0429. Time: 66.5865 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #936: GFLOPs: 104.2682. Time: 78.5762 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #937: GFLOPs: 76.7632. Time: 106.7308 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #938: GFLOPs: 94.2039. Time: 86.9709 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #939: GFLOPs: 97.7392. Time: 83.8251 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #940: GFLOPs: 94.6243. Time: 86.5845 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #941: GFLOPs: 83.1812. Time: 98.4958 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #942: GFLOPs: 83.1802. Time: 98.4969 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #943: GFLOPs: 92.6252. Time: 88.4532 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #944: GFLOPs: 92.6199. Time: 88.4583 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #945: GFLOPs: 92.6216. Time: 88.4567 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #946: GFLOPs: 83.1786. Time: 98.4988 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #947: GFLOPs: 84.5917. Time: 96.8535 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #948: GFLOPs: 65.6841. Time: 124.7334 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #949: GFLOPs: 65.6911. Time: 124.7202 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #950: GFLOPs: 81.8593. Time: 100.0864 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #951: GFLOPs: 55.9955. Time: 146.3153 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #952: GFLOPs: 82.7911. Time: 98.9599 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #953: GFLOPs: 82.8008. Time: 98.9484 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #954: GFLOPs: 56.0250. Time: 146.2384 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #955: GFLOPs: 55.9816. Time: 146.3516 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #956: GFLOPs: 82.7987. Time: 98.9508 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #957: GFLOPs: 10.6832. Time: 766.9056 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #958: GFLOPs: 3.4601. Time: 2367.8452 us. Best GFLOPs: 130.7471
2024-03-20 21:54:19 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #959: GFLOPs: 31.3629. Time: 261.2319 us. Best GFLOPs: 130.7471
2024-03-20 22:03:09 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:03:09 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:03:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1219 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1626 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2033 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2440 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2843 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3248 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3651 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4056 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4460 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:12 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 22:03:13 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 175 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:14 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 191 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:15 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:17 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 187 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:03:17 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9824  0.9823  0.9817  0.9816  0.9816  0.9816  0.9796  0.9796  0.9782  0.9780  0.9779  0.9778  0.9517  0.9517  0.9517  0.9504
[17 : 32]:	0.9504  0.9502  0.9502  0.9490  0.9489  0.9395  0.9394  0.9394  0.9293  0.9293  0.9292  0.9292  0.9292  0.9292  0.9292  0.9253
[33 : 48]:	0.9251  0.9250  0.9250  0.9250  0.9250  0.9228  0.9227  0.9227  0.9227  0.9021  0.8985  0.8984  0.8984  0.8984  0.8967  0.8966
[49 : 64]:	0.8966  0.8961  0.8959  0.8959  0.8959  0.8939  0.8923  0.8923  0.8922  0.8921  0.8921  0.8921  0.8786  0.8731  0.8723  0.8651
2024-03-20 22:03:18 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:03:18 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #960: GFLOPs: 129.7536. Time: 63.1428 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #961: GFLOPs: 129.7659. Time: 63.1368 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #962: GFLOPs: 130.0762. Time: 62.9861 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #963: GFLOPs: 130.0729. Time: 62.9878 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #964: GFLOPs: 130.0757. Time: 62.9864 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #965: GFLOPs: 130.0733. Time: 62.9875 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #966: GFLOPs: 125.8258. Time: 65.1139 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #967: GFLOPs: 125.8205. Time: 65.1166 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #968: GFLOPs: 125.8158. Time: 65.1190 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #969: GFLOPs: 125.8206. Time: 65.1165 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #970: GFLOPs: 129.7671. Time: 63.1362 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #971: GFLOPs: 129.7709. Time: 63.1344 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #972: GFLOPs: 125.8245. Time: 65.1145 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #973: GFLOPs: 122.9939. Time: 66.6131 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #974: GFLOPs: 125.8261. Time: 65.1137 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #975: GFLOPs: 127.2664. Time: 64.3768 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #976: GFLOPs: 127.2682. Time: 64.3759 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #977: GFLOPs: 127.2611. Time: 64.3794 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #978: GFLOPs: 127.2688. Time: 64.3756 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #979: GFLOPs: 125.5593. Time: 65.2520 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #980: GFLOPs: 125.5624. Time: 65.2504 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #981: GFLOPs: 122.9435. Time: 66.6404 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #982: GFLOPs: 123.2646. Time: 66.4668 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #983: GFLOPs: 122.9419. Time: 66.6412 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #984: GFLOPs: 124.7061. Time: 65.6985 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #985: GFLOPs: 124.6954. Time: 65.7041 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #986: GFLOPs: 124.7018. Time: 65.7007 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #987: GFLOPs: 124.6883. Time: 65.7079 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #988: GFLOPs: 125.9180. Time: 65.0662 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #989: GFLOPs: 124.0050. Time: 66.0699 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #990: GFLOPs: 125.9183. Time: 65.0660 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #991: GFLOPs: 126.3927. Time: 64.8218 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #992: GFLOPs: 121.7718. Time: 67.2816 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #993: GFLOPs: 121.7791. Time: 67.2775 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #994: GFLOPs: 125.4646. Time: 65.3013 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #995: GFLOPs: 125.5224. Time: 65.2712 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #996: GFLOPs: 121.8035. Time: 67.2641 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #997: GFLOPs: 126.1096. Time: 64.9673 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #998: GFLOPs: 126.1088. Time: 64.9677 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #999: GFLOPs: 126.1199. Time: 64.9620 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1000: GFLOPs: 126.1150. Time: 64.9645 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1001: GFLOPs: 123.1608. Time: 66.5228 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1002: GFLOPs: 122.9569. Time: 66.6331 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1003: GFLOPs: 129.1692. Time: 63.4284 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1004: GFLOPs: 122.9594. Time: 66.6318 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1005: GFLOPs: 122.9671. Time: 66.6276 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1006: GFLOPs: 115.6077. Time: 70.8690 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1007: GFLOPs: 115.6102. Time: 70.8674 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1008: GFLOPs: 115.6018. Time: 70.8726 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1009: GFLOPs: 115.3942. Time: 71.0001 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1010: GFLOPs: 115.3759. Time: 71.0114 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1011: GFLOPs: 115.3957. Time: 70.9992 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1012: GFLOPs: 115.3859. Time: 71.0052 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1013: GFLOPs: 116.2186. Time: 70.4965 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1014: GFLOPs: 116.2043. Time: 70.5052 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1015: GFLOPs: 116.1830. Time: 70.5180 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1016: GFLOPs: 115.6082. Time: 70.8687 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1017: GFLOPs: 115.6020. Time: 70.8725 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1018: GFLOPs: 115.5961. Time: 70.8761 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1019: GFLOPs: 115.6064. Time: 70.8698 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1020: GFLOPs: 123.4609. Time: 66.3611 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1021: GFLOPs: 31.5924. Time: 259.3342 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1022: GFLOPs: 15.1790. Time: 539.7581 us. Best GFLOPs: 130.7471
2024-03-20 22:04:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1023: GFLOPs: 8.4530. Time: 969.2455 us. Best GFLOPs: 130.7471
2024-03-20 22:08:44 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:08:44 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:08:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 805 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1612 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2021 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2426 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2833 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3236 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3641 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4043 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:46 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2024-03-20 22:08:47 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 202 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:48 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 178 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:50 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 149 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:51 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 195 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:08:52 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9823  0.9808  0.9808  0.9808  0.9808  0.9808  0.9773  0.9754  0.9743  0.9743  0.9739  0.9739  0.9739  0.9739  0.9693  0.9687
[17 : 32]:	0.9687  0.9687  0.9682  0.9682  0.9658  0.9653  0.9642  0.9642  0.9636  0.9636  0.9634  0.9631  0.9631  0.9631  0.9628  0.9623
[33 : 48]:	0.9558  0.9558  0.9558  0.9530  0.9489  0.9460  0.9460  0.9438  0.9438  0.9409  0.9382  0.9382  0.9376  0.9376  0.9374  0.9374
[49 : 64]:	0.9371  0.9371  0.9347  0.9347  0.9345  0.9342  0.9339  0.9339  0.9332  0.9326  0.9326  0.9326  0.9324  0.9324  0.9321  0.9321
2024-03-20 22:08:52 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:08:52 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1024: GFLOPs: 129.3155. Time: 63.3567 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1025: GFLOPs: 129.3874. Time: 63.3215 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1026: GFLOPs: 129.3882. Time: 63.3211 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1027: GFLOPs: 129.3832. Time: 63.3235 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1028: GFLOPs: 129.3940. Time: 63.3182 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1029: GFLOPs: 129.3901. Time: 63.3201 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1030: GFLOPs: 129.1451. Time: 63.4403 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1031: GFLOPs: 125.6801. Time: 65.1893 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1032: GFLOPs: 129.1491. Time: 63.4383 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1033: GFLOPs: 129.1443. Time: 63.4407 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1034: GFLOPs: 125.6828. Time: 65.1879 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1035: GFLOPs: 125.6847. Time: 65.1869 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1036: GFLOPs: 125.6882. Time: 65.1851 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1037: GFLOPs: 125.6866. Time: 65.1860 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1038: GFLOPs: 127.0691. Time: 64.4767 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1039: GFLOPs: 127.0718. Time: 64.4754 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1040: GFLOPs: 127.0693. Time: 64.4766 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1041: GFLOPs: 127.0756. Time: 64.4734 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1042: GFLOPs: 127.0807. Time: 64.4709 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1043: GFLOPs: 127.0720. Time: 64.4753 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1044: GFLOPs: 125.5103. Time: 65.2775 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1045: GFLOPs: 125.5330. Time: 65.2657 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1046: GFLOPs: 124.7174. Time: 65.6925 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1047: GFLOPs: 124.7653. Time: 65.6673 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1048: GFLOPs: 124.7606. Time: 65.6698 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1049: GFLOPs: 124.7683. Time: 65.6657 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1050: GFLOPs: 125.5114. Time: 65.2769 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1051: GFLOPs: 124.7350. Time: 65.6833 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1052: GFLOPs: 124.4578. Time: 65.8295 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1053: GFLOPs: 124.5388. Time: 65.7867 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1054: GFLOPs: 125.2341. Time: 65.4215 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1055: GFLOPs: 125.2383. Time: 65.4193 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1056: GFLOPs: 124.8630. Time: 65.6159 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1057: GFLOPs: 124.8613. Time: 65.6168 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1058: GFLOPs: 124.8636. Time: 65.6156 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1059: GFLOPs: 124.1339. Time: 66.0013 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1060: GFLOPs: 125.0803. Time: 65.5019 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1061: GFLOPs: 122.2169. Time: 67.0366 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1062: GFLOPs: 122.2263. Time: 67.0314 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1063: GFLOPs: 124.1300. Time: 66.0034 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1064: GFLOPs: 124.1369. Time: 65.9997 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1065: GFLOPs: 124.1353. Time: 66.0006 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1066: GFLOPs: 122.9513. Time: 66.6361 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1067: GFLOPs: 122.9417. Time: 66.6414 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1068: GFLOPs: 122.9445. Time: 66.6398 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1069: GFLOPs: 122.9467. Time: 66.6386 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1070: GFLOPs: 121.8767. Time: 67.2237 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1071: GFLOPs: 121.8767. Time: 67.2237 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1072: GFLOPs: 122.9411. Time: 66.6416 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1073: GFLOPs: 122.9457. Time: 66.6392 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1074: GFLOPs: 118.5130. Time: 69.1317 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1075: GFLOPs: 118.5114. Time: 69.1326 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1076: GFLOPs: 121.9065. Time: 67.2072 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1077: GFLOPs: 118.5307. Time: 69.1213 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1078: GFLOPs: 124.4835. Time: 65.8159 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1079: GFLOPs: 122.6659. Time: 66.7912 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1080: GFLOPs: 123.2441. Time: 66.4778 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1081: GFLOPs: 123.2448. Time: 66.4774 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1082: GFLOPs: 123.2424. Time: 66.4787 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1083: GFLOPs: 123.2455. Time: 66.4771 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1084: GFLOPs: 118.5334. Time: 69.1198 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1085: GFLOPs: 9.0511. Time: 905.1964 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1086: GFLOPs: 28.6773. Time: 285.6960 us. Best GFLOPs: 130.7471
2024-03-20 22:09:41 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1087: GFLOPs: 11.9898. Time: 683.3284 us. Best GFLOPs: 130.7471
2024-03-20 22:12:35 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:12:35 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:12:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 408 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 816 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1221 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1626 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2030 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2436 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2838 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3239 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3643 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4047 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:38 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-20 22:12:39 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:40 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 168 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:41 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 177 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:43 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 183 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:12:43 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9770  0.9754  0.9738  0.9738  0.9738  0.9738  0.9713  0.9713  0.9713  0.9639  0.9637  0.9637  0.9637  0.9637  0.9564  0.9562
[17 : 32]:	0.9562  0.9562  0.9562  0.9489  0.9489  0.9438  0.9438  0.9410  0.9379  0.9379  0.9379  0.9370  0.9316  0.9316  0.9316  0.9316
[33 : 48]:	0.9306  0.9304  0.9304  0.9304  0.9289  0.9239  0.9239  0.9239  0.9239  0.9231  0.9231  0.9229  0.9229  0.9229  0.9229  0.9229
[49 : 64]:	0.9164  0.9164  0.9164  0.9164  0.9164  0.9137  0.9089  0.9089  0.9089  0.9089  0.9089  0.9089  0.9061  0.8976  0.8976  0.8976
2024-03-20 22:12:44 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:12:44 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1088: GFLOPs: 123.5756. Time: 66.2995 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1089: GFLOPs: 125.4999. Time: 65.2829 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1090: GFLOPs: 129.4534. Time: 63.2892 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1091: GFLOPs: 129.4541. Time: 63.2888 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1092: GFLOPs: 129.4608. Time: 63.2856 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1093: GFLOPs: 129.4613. Time: 63.2853 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1094: GFLOPs: 127.1992. Time: 64.4108 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1095: GFLOPs: 127.2341. Time: 64.3931 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1096: GFLOPs: 127.2136. Time: 64.4035 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1097: GFLOPs: 125.4491. Time: 65.3093 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1098: GFLOPs: 125.4278. Time: 65.3204 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1099: GFLOPs: 125.4428. Time: 65.3126 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1100: GFLOPs: 125.4501. Time: 65.3088 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1101: GFLOPs: 125.4517. Time: 65.3080 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1102: GFLOPs: 123.7180. Time: 66.2232 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1103: GFLOPs: 123.8232. Time: 66.1669 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1104: GFLOPs: 123.8513. Time: 66.1519 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1105: GFLOPs: 123.8210. Time: 66.1681 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1106: GFLOPs: 123.8637. Time: 66.1453 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1107: GFLOPs: 124.7005. Time: 65.7014 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1108: GFLOPs: 124.6987. Time: 65.7024 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1109: GFLOPs: 122.4508. Time: 66.9085 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1110: GFLOPs: 122.4461. Time: 66.9111 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1111: GFLOPs: 122.4445. Time: 66.9119 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1112: GFLOPs: 123.7280. Time: 66.2178 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1113: GFLOPs: 123.7238. Time: 66.2201 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1114: GFLOPs: 123.7278. Time: 66.2180 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1115: GFLOPs: 123.6003. Time: 66.2863 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1116: GFLOPs: 123.2121. Time: 66.4951 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1117: GFLOPs: 123.2131. Time: 66.4946 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1118: GFLOPs: 123.2121. Time: 66.4951 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1119: GFLOPs: 123.2123. Time: 66.4950 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1120: GFLOPs: 124.0122. Time: 66.0661 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1121: GFLOPs: 124.0066. Time: 66.0691 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1122: GFLOPs: 123.9996. Time: 66.0728 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1123: GFLOPs: 124.0032. Time: 66.0709 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1124: GFLOPs: 123.2154. Time: 66.4933 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1125: GFLOPs: 126.6567. Time: 64.6867 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1126: GFLOPs: 126.6560. Time: 64.6870 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1127: GFLOPs: 126.6555. Time: 64.6873 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1128: GFLOPs: 126.6567. Time: 64.6867 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1129: GFLOPs: 118.2675. Time: 69.2752 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1130: GFLOPs: 118.2880. Time: 69.2632 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1131: GFLOPs: 118.2605. Time: 69.2793 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1132: GFLOPs: 118.2767. Time: 69.2698 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1133: GFLOPs: 118.2976. Time: 69.2575 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1134: GFLOPs: 118.2533. Time: 69.2835 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1135: GFLOPs: 118.2614. Time: 69.2787 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1136: GFLOPs: 127.1353. Time: 64.4432 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1137: GFLOPs: 127.1165. Time: 64.4527 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1138: GFLOPs: 127.1267. Time: 64.4475 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1139: GFLOPs: 127.1191. Time: 64.4514 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1140: GFLOPs: 127.1301. Time: 64.4458 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1141: GFLOPs: 127.1239. Time: 64.4490 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1142: GFLOPs: 128.1395. Time: 63.9381 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1143: GFLOPs: 128.1396. Time: 63.9381 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1144: GFLOPs: 128.1228. Time: 63.9465 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1145: GFLOPs: 128.1197. Time: 63.9480 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1146: GFLOPs: 128.1167. Time: 63.9495 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1147: GFLOPs: 128.1520. Time: 63.9319 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1148: GFLOPs: 128.1365. Time: 63.9397 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1149: GFLOPs: 9.3825. Time: 873.2227 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1150: GFLOPs: 51.8077. Time: 158.1425 us. Best GFLOPs: 130.7471
2024-03-20 22:13:33 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1151: GFLOPs: 9.6687. Time: 847.3761 us. Best GFLOPs: 130.7471
2024-03-20 22:19:59 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:19:59 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:20:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 814 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1220 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1623 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2027 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2432 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2832 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3239 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3639 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:02 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-03-20 22:20:03 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 184 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:04 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 176 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:05 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 154 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:06 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 177 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:20:07 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9797  0.9735  0.9718  0.9718  0.9718  0.9663  0.9470  0.9470  0.9470  0.9470  0.9470  0.9470  0.9470  0.9470  0.9470  0.9470
[17 : 32]:	0.9465  0.9465  0.9465  0.9465  0.9465  0.9465  0.9465  0.9445  0.9445  0.9445  0.9445  0.9445  0.9425  0.9425  0.9425  0.9425
[33 : 48]:	0.9425  0.9373  0.9373  0.9341  0.9337  0.9337  0.9319  0.9319  0.9319  0.9319  0.9319  0.9313  0.9313  0.9313  0.9313  0.9313
[49 : 64]:	0.9313  0.9188  0.9188  0.9188  0.9188  0.9187  0.9187  0.9005  0.9005  0.9005  0.9005  0.8991  0.8991  0.8991  0.8991  0.8943
2024-03-20 22:20:07 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:20:07 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1152: GFLOPs: 130.0571. Time: 62.9954 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1153: GFLOPs: 126.3726. Time: 64.8321 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1154: GFLOPs: 127.3794. Time: 64.3196 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1155: GFLOPs: 127.2556. Time: 64.3823 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1156: GFLOPs: 126.8325. Time: 64.5970 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1157: GFLOPs: 125.7914. Time: 65.1316 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1158: GFLOPs: 84.0487. Time: 97.4792 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1159: GFLOPs: 84.0530. Time: 97.4742 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1160: GFLOPs: 84.0193. Time: 97.5133 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1161: GFLOPs: 84.0563. Time: 97.4704 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1162: GFLOPs: 84.1038. Time: 97.4154 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1163: GFLOPs: 84.0579. Time: 97.4685 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1164: GFLOPs: 84.0214. Time: 97.5109 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1165: GFLOPs: 84.0537. Time: 97.4734 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1166: GFLOPs: 84.1367. Time: 97.3773 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1167: GFLOPs: 84.0219. Time: 97.5103 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1168: GFLOPs: 84.0529. Time: 97.4743 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1169: GFLOPs: 84.0057. Time: 97.5291 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1170: GFLOPs: 84.0905. Time: 97.4308 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1171: GFLOPs: 84.0084. Time: 97.5260 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1172: GFLOPs: 84.0548. Time: 97.4721 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1173: GFLOPs: 84.0066. Time: 97.5281 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1174: GFLOPs: 84.1393. Time: 97.3742 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1175: GFLOPs: 84.1148. Time: 97.4026 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1176: GFLOPs: 84.0275. Time: 97.5038 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1177: GFLOPs: 84.0213. Time: 97.5110 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1178: GFLOPs: 84.0197. Time: 97.5129 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1179: GFLOPs: 84.1118. Time: 97.4061 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1180: GFLOPs: 83.9026. Time: 97.6489 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1181: GFLOPs: 83.8958. Time: 97.6569 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1182: GFLOPs: 83.9777. Time: 97.5616 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1183: GFLOPs: 83.8993. Time: 97.6528 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1184: GFLOPs: 83.8999. Time: 97.6521 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1185: GFLOPs: 123.9153. Time: 66.1177 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1186: GFLOPs: 123.9178. Time: 66.1164 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1187: GFLOPs: 124.2202. Time: 65.9555 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1188: GFLOPs: 124.2171. Time: 65.9571 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1189: GFLOPs: 124.2130. Time: 65.9593 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1190: GFLOPs: 57.3160. Time: 142.9444 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1191: GFLOPs: 57.3178. Time: 142.9398 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1192: GFLOPs: 57.3565. Time: 142.8434 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1193: GFLOPs: 57.3210. Time: 142.9318 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1194: GFLOPs: 57.3547. Time: 142.8480 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1195: GFLOPs: 57.2845. Time: 143.0229 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1196: GFLOPs: 57.2856. Time: 143.0202 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1197: GFLOPs: 57.2787. Time: 143.0375 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1198: GFLOPs: 57.2851. Time: 143.0216 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1199: GFLOPs: 57.3159. Time: 142.9447 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1200: GFLOPs: 57.3163. Time: 142.9437 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1201: GFLOPs: 57.3730. Time: 142.8024 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1202: GFLOPs: 57.4128. Time: 142.7034 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1203: GFLOPs: 57.3787. Time: 142.7882 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1204: GFLOPs: 57.3730. Time: 142.8024 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1205: GFLOPs: 118.5034. Time: 69.1373 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1206: GFLOPs: 118.5032. Time: 69.1373 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1207: GFLOPs: 115.9949. Time: 70.6324 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1208: GFLOPs: 115.9943. Time: 70.6328 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1209: GFLOPs: 115.9898. Time: 70.6355 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1210: GFLOPs: 115.9734. Time: 70.6455 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1211: GFLOPs: 114.0768. Time: 71.8200 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1212: GFLOPs: 114.0883. Time: 71.8128 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1213: GFLOPs: 8.3916. Time: 976.3293 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1214: GFLOPs: 23.1318. Time: 354.1872 us. Best GFLOPs: 130.7471
2024-03-20 22:20:55 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1215: GFLOPs: 15.0230. Time: 545.3635 us. Best GFLOPs: 130.7471
2024-03-20 22:24:32 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:24:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:24:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1215 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2023 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2427 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2834 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3240 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3643 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4049 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:35 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-03-20 22:24:36 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:37 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:38 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 171 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:40 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 184 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:24:40 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9727  0.9478  0.9401  0.9314  0.8950  0.8950  0.8950  0.8931  0.8931  0.8931  0.8931  0.8931  0.8931  0.8931  0.8931  0.8931
[17 : 32]:	0.8907  0.8907  0.8553  0.8553  0.8553  0.8553  0.8553  0.8551  0.8551  0.8551  0.8544  0.8544  0.8544  0.8544  0.8461  0.8461
[33 : 48]:	0.8458  0.8458  0.8458  0.8458  0.8387  0.8387  0.8387  0.8382  0.8379  0.8379  0.8379  0.8379  0.8377  0.8321  0.8300  0.8300
[49 : 64]:	0.8300  0.8300  0.8285  0.8285  0.8143  0.8143  0.8137  0.8013  0.7998  0.7998  0.7998  0.7962  0.7953  0.7953  0.7953  0.7952
2024-03-20 22:24:40 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:24:40 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1216: GFLOPs: 127.3443. Time: 64.3374 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1217: GFLOPs: 126.6722. Time: 64.6788 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1218: GFLOPs: 122.3443. Time: 66.9667 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1219: GFLOPs: 123.7906. Time: 66.1843 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1220: GFLOPs: 115.7679. Time: 70.7709 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1221: GFLOPs: 115.7252. Time: 70.7970 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1222: GFLOPs: 115.3876. Time: 71.0042 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1223: GFLOPs: 116.1865. Time: 70.5160 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1224: GFLOPs: 116.1874. Time: 70.5154 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1225: GFLOPs: 116.1767. Time: 70.5219 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1226: GFLOPs: 116.1764. Time: 70.5220 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1227: GFLOPs: 116.1836. Time: 70.5177 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1228: GFLOPs: 116.1938. Time: 70.5115 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1229: GFLOPs: 116.1887. Time: 70.5146 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1230: GFLOPs: 116.1715. Time: 70.5251 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1231: GFLOPs: 116.1926. Time: 70.5122 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1232: GFLOPs: 115.5904. Time: 70.8796 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1233: GFLOPs: 115.6095. Time: 70.8679 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1234: GFLOPs: 108.4302. Time: 75.5601 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1235: GFLOPs: 108.4320. Time: 75.5589 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1236: GFLOPs: 108.4315. Time: 75.5592 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1237: GFLOPs: 108.4327. Time: 75.5584 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1238: GFLOPs: 108.4340. Time: 75.5575 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1239: GFLOPs: 112.3600. Time: 72.9174 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1240: GFLOPs: 112.3557. Time: 72.9202 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1241: GFLOPs: 112.3588. Time: 72.9182 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1242: GFLOPs: 108.4389. Time: 75.5541 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1243: GFLOPs: 108.4378. Time: 75.5548 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1244: GFLOPs: 108.4391. Time: 75.5540 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1245: GFLOPs: 108.4384. Time: 75.5544 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1246: GFLOPs: 108.4395. Time: 75.5537 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1247: GFLOPs: 108.4383. Time: 75.5545 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1248: GFLOPs: 108.4312. Time: 75.5594 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1249: GFLOPs: 108.4306. Time: 75.5598 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1250: GFLOPs: 108.4323. Time: 75.5587 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1251: GFLOPs: 108.4320. Time: 75.5589 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1252: GFLOPs: 108.4320. Time: 75.5589 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1253: GFLOPs: 108.4326. Time: 75.5585 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1254: GFLOPs: 108.4302. Time: 75.5601 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1255: GFLOPs: 112.3624. Time: 72.9158 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1256: GFLOPs: 108.4390. Time: 75.5540 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1257: GFLOPs: 108.4391. Time: 75.5539 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1258: GFLOPs: 108.4397. Time: 75.5535 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1259: GFLOPs: 108.4385. Time: 75.5544 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1260: GFLOPs: 98.7064. Time: 83.0037 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1261: GFLOPs: 103.6598. Time: 79.0374 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1262: GFLOPs: 112.3624. Time: 72.9158 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1263: GFLOPs: 112.3704. Time: 72.9107 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1264: GFLOPs: 112.3663. Time: 72.9133 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1265: GFLOPs: 112.3668. Time: 72.9130 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1266: GFLOPs: 98.5607. Time: 83.1264 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1267: GFLOPs: 112.3674. Time: 72.9126 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1268: GFLOPs: 89.5228. Time: 91.5186 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1269: GFLOPs: 89.5221. Time: 91.5193 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1270: GFLOPs: 89.4606. Time: 91.5822 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1271: GFLOPs: 116.0926. Time: 70.5730 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1272: GFLOPs: 114.4693. Time: 71.5738 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1273: GFLOPs: 114.4723. Time: 71.5719 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1274: GFLOPs: 114.1156. Time: 71.7956 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1275: GFLOPs: 89.0037. Time: 92.0524 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1276: GFLOPs: 89.4480. Time: 91.5952 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1277: GFLOPs: 16.8768. Time: 485.4605 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1278: GFLOPs: 4.1648. Time: 1967.2244 us. Best GFLOPs: 130.7471
2024-03-20 22:25:28 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1279: GFLOPs: 8.4016. Time: 975.1761 us. Best GFLOPs: 130.7471
2024-03-20 22:31:41 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:31:41 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:31:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1614 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2018 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2424 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2829 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3232 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3639 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:44 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-03-20 22:31:45 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 206 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:46 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 192 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:47 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 182 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:49 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:31:49 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9423  0.9054  0.8872  0.8872  0.8581  0.8581  0.8562  0.8562  0.8562  0.8562  0.8419  0.8419  0.8419  0.8419  0.8419  0.8379
[17 : 32]:	0.8379  0.8379  0.8355  0.8355  0.8355  0.8315  0.8315  0.8300  0.8297  0.8127  0.8041  0.8041  0.8041  0.8020  0.8020  0.7659
[33 : 48]:	0.7659  0.7595  0.7595  0.7595  0.7595  0.7552  0.7552  0.7552  0.7552  0.7552  0.7552  0.7552  0.7552  0.7552  0.7552  0.7552
[49 : 64]:	0.7552  0.7552  0.7552  0.7540  0.7540  0.7448  0.7448  0.7441  0.7441  0.7415  0.7411  0.7411  0.7411  0.7411  0.7411  0.7411
2024-03-20 22:31:49 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:31:49 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1280: GFLOPs: 123.7195. Time: 66.2224 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1281: GFLOPs: 119.8667. Time: 68.3509 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1282: GFLOPs: 116.5977. Time: 70.2673 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1283: GFLOPs: 116.2951. Time: 70.4501 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1284: GFLOPs: 116.6863. Time: 70.2139 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1285: GFLOPs: 113.5500. Time: 72.1532 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1286: GFLOPs: 114.4929. Time: 71.5590 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1287: GFLOPs: 114.1866. Time: 71.7510 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1288: GFLOPs: 114.1915. Time: 71.7479 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1289: GFLOPs: 114.6389. Time: 71.4679 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1290: GFLOPs: 112.5853. Time: 72.7715 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1291: GFLOPs: 112.5897. Time: 72.7687 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1292: GFLOPs: 112.5918. Time: 72.7673 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1293: GFLOPs: 112.5941. Time: 72.7658 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1294: GFLOPs: 112.5918. Time: 72.7673 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1295: GFLOPs: 112.5914. Time: 72.7675 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1296: GFLOPs: 112.5961. Time: 72.7645 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1297: GFLOPs: 112.5956. Time: 72.7648 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1298: GFLOPs: 108.8789. Time: 75.2488 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1299: GFLOPs: 108.8785. Time: 75.2490 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1300: GFLOPs: 108.8778. Time: 75.2495 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1301: GFLOPs: 108.8834. Time: 75.2456 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1302: GFLOPs: 108.8821. Time: 75.2465 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1303: GFLOPs: 108.8624. Time: 75.2601 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1304: GFLOPs: 108.8558. Time: 75.2647 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1305: GFLOPs: 108.8666. Time: 75.2573 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1306: GFLOPs: 108.8836. Time: 75.2455 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1307: GFLOPs: 108.8826. Time: 75.2462 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1308: GFLOPs: 108.8788. Time: 75.2488 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1309: GFLOPs: 112.5926. Time: 72.7668 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1310: GFLOPs: 112.6122. Time: 72.7541 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1311: GFLOPs: 90.3431. Time: 90.6877 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1312: GFLOPs: 90.3408. Time: 90.6899 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1313: GFLOPs: 90.3229. Time: 90.7079 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1314: GFLOPs: 90.3483. Time: 90.6824 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1315: GFLOPs: 90.3506. Time: 90.6800 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1316: GFLOPs: 90.3390. Time: 90.6917 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1317: GFLOPs: 91.9174. Time: 89.1344 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1318: GFLOPs: 91.9091. Time: 89.1424 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1319: GFLOPs: 91.9105. Time: 89.1411 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1320: GFLOPs: 91.9038. Time: 89.1475 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1321: GFLOPs: 91.9133. Time: 89.1384 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1322: GFLOPs: 91.9066. Time: 89.1448 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1323: GFLOPs: 91.9033. Time: 89.1481 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1324: GFLOPs: 91.9173. Time: 89.1344 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1325: GFLOPs: 91.9039. Time: 89.1475 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1326: GFLOPs: 91.9062. Time: 89.1452 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1327: GFLOPs: 91.9064. Time: 89.1451 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1328: GFLOPs: 91.9121. Time: 89.1396 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1329: GFLOPs: 91.9064. Time: 89.1451 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1330: GFLOPs: 91.9086. Time: 89.1429 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1331: GFLOPs: 89.7960. Time: 91.2402 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1332: GFLOPs: 89.8080. Time: 91.2279 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1333: GFLOPs: 87.8676. Time: 93.2426 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1334: GFLOPs: 87.8640. Time: 93.2464 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1335: GFLOPs: 90.3340. Time: 90.6968 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1336: GFLOPs: 90.3425. Time: 90.6882 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1337: GFLOPs: 90.3437. Time: 90.6870 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1338: GFLOPs: 95.1477. Time: 86.1082 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1339: GFLOPs: 95.1467. Time: 86.1091 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1340: GFLOPs: 95.1390. Time: 86.1161 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1341: GFLOPs: 21.1526. Time: 387.3286 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1342: GFLOPs: 3.3817. Time: 2422.7816 us. Best GFLOPs: 130.7471
2024-03-20 22:32:42 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1343: GFLOPs: 2.7227. Time: 3009.1746 us. Best GFLOPs: 130.7471
2024-03-20 22:38:30 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:38:30 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:38:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1217 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2021 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2425 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2833 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3234 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3640 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:33 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-20 22:38:33 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 160 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 158 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:37 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:38:38 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9314  0.8777  0.8766  0.8726  0.8465  0.8463  0.8463  0.8400  0.8399  0.8397  0.8361  0.8336  0.8319  0.8319  0.8273  0.8060
[17 : 32]:	0.8053  0.8011  0.7393  0.7392  0.7392  0.7392  0.7330  0.7330  0.7327  0.7327  0.7326  0.7305  0.7303  0.7274  0.7242  0.7239
[33 : 48]:	0.7202  0.7190  0.7157  0.7138  0.7055  0.7044  0.7044  0.7044  0.7044  0.7044  0.7044  0.7044  0.7044  0.7044  0.7028  0.6980
[49 : 64]:	0.6980  0.6980  0.6980  0.6980  0.6980  0.6980  0.6980  0.6980  0.6980  0.6980  0.6980  0.6858  0.6858  0.6856  0.6856  0.6856
2024-03-20 22:38:38 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:38:38 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1344: GFLOPs: 120.1590. Time: 68.1847 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1345: GFLOPs: 114.8566. Time: 71.3324 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1346: GFLOPs: 114.2106. Time: 71.7359 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1347: GFLOPs: 116.4888. Time: 70.3329 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1348: GFLOPs: 109.1418. Time: 75.0675 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1349: GFLOPs: 112.9741. Time: 72.5210 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1350: GFLOPs: 112.9698. Time: 72.5238 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1351: GFLOPs: 112.9762. Time: 72.5197 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1352: GFLOPs: 109.7602. Time: 74.6446 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1353: GFLOPs: 108.9224. Time: 75.2187 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1354: GFLOPs: 108.9214. Time: 75.2194 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1355: GFLOPs: 109.7566. Time: 74.6470 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1356: GFLOPs: 112.9744. Time: 72.5209 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1357: GFLOPs: 112.9747. Time: 72.5207 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1358: GFLOPs: 108.8799. Time: 75.2480 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1359: GFLOPs: 98.9123. Time: 82.8309 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1360: GFLOPs: 108.2503. Time: 75.6857 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1361: GFLOPs: 104.0228. Time: 78.7616 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1362: GFLOPs: 104.3859. Time: 78.4876 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1363: GFLOPs: 90.0409. Time: 90.9920 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1364: GFLOPs: 90.0443. Time: 90.9885 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1365: GFLOPs: 90.0360. Time: 90.9969 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1366: GFLOPs: 77.3950. Time: 105.8596 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1367: GFLOPs: 77.3865. Time: 105.8712 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1368: GFLOPs: 89.9550. Time: 91.0789 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1369: GFLOPs: 89.9556. Time: 91.0783 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1370: GFLOPs: 89.9521. Time: 91.0818 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1371: GFLOPs: 77.4009. Time: 105.8514 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1372: GFLOPs: 77.3878. Time: 105.8693 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1373: GFLOPs: 90.0404. Time: 90.9925 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1374: GFLOPs: 97.4000. Time: 84.1171 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1375: GFLOPs: 90.1308. Time: 90.9013 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1376: GFLOPs: 89.4175. Time: 91.6264 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1377: GFLOPs: 96.9828. Time: 84.4789 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1378: GFLOPs: 89.1455. Time: 91.9059 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1379: GFLOPs: 89.1606. Time: 91.8904 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1380: GFLOPs: 96.9987. Time: 84.4650 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1381: GFLOPs: 94.6128. Time: 86.5950 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1382: GFLOPs: 94.6057. Time: 86.6015 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1383: GFLOPs: 94.6126. Time: 86.5952 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1384: GFLOPs: 94.6111. Time: 86.5966 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1385: GFLOPs: 94.6004. Time: 86.6064 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1386: GFLOPs: 94.6126. Time: 86.5952 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1387: GFLOPs: 94.6148. Time: 86.5932 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1388: GFLOPs: 94.6159. Time: 86.5922 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1389: GFLOPs: 94.6072. Time: 86.6002 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1390: GFLOPs: 76.7570. Time: 106.7394 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1391: GFLOPs: 92.1970. Time: 88.8641 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1392: GFLOPs: 92.1928. Time: 88.8681 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1393: GFLOPs: 92.1890. Time: 88.8718 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1394: GFLOPs: 92.1870. Time: 88.8737 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1395: GFLOPs: 92.1895. Time: 88.8713 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1396: GFLOPs: 92.1825. Time: 88.8781 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1397: GFLOPs: 92.1888. Time: 88.8720 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1398: GFLOPs: 92.1999. Time: 88.8613 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1399: GFLOPs: 92.1961. Time: 88.8649 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1400: GFLOPs: 92.1787. Time: 88.8817 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1401: GFLOPs: 92.1887. Time: 88.8721 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1402: GFLOPs: 92.1776. Time: 88.8828 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1403: GFLOPs: 84.7595. Time: 96.6618 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1404: GFLOPs: 84.7628. Time: 96.6579 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1405: GFLOPs: 33.6062. Time: 243.7941 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1406: GFLOPs: 9.3931. Time: 872.2329 us. Best GFLOPs: 130.7471
2024-03-20 22:39:32 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1407: GFLOPs: 9.7047. Time: 844.2321 us. Best GFLOPs: 130.7471
2024-03-20 22:45:10 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-20 22:45:11 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-20 22:45:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1218 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 1622 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2029 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2437 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 2842 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3250 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 3657 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4064 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4468 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 4874 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 5279 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:14 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-03-20 22:45:15 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 200 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:16 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 213 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:17 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 168 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:19 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x613334c8f808)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x6133339e9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x6133334cf0b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x613334d9b248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x613334c653b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x6133334a8d38)]: 185 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x6133339e63c8)]: 0 failure(s)
2024-03-20 22:45:19 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8347  0.8329  0.8238  0.8082  0.7205  0.7185  0.7185  0.7156  0.7119  0.7119  0.7098  0.7098  0.7077  0.7074  0.7074  0.7073
[17 : 32]:	0.7044  0.7044  0.7044  0.7044  0.7041  0.7032  0.7032  0.7020  0.7020  0.7003  0.7003  0.7003  0.7003  0.6997  0.6961  0.6956
[33 : 48]:	0.6948  0.6945  0.6945  0.6945  0.6945  0.6945  0.6943  0.6941  0.6941  0.6914  0.6914  0.6914  0.6914  0.6910  0.6910  0.6910
[49 : 64]:	0.6910  0.6884  0.6884  0.6878  0.6878  0.6878  0.6877  0.6791  0.6786  0.6786  0.6786  0.6786  0.6782  0.6782  0.6782  0.6782
2024-03-20 22:45:20 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-20 22:45:20 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #1408: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #1409: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 2, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #1410: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #1411: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 4, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #1412: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 1, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:121] [Task #12: fused_nn_dense_add] Trial #1413: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i0_0_i1_0_fused * T.int64(40) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1414: GFLOPs: 89.3327. Time: 91.7134 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1415: GFLOPs: 93.7732. Time: 87.3704 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1416: GFLOPs: 89.5943. Time: 91.4455 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1417: GFLOPs: 89.6037. Time: 91.4359 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1418: GFLOPs: 89.3381. Time: 91.7078 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1419: GFLOPs: 89.3473. Time: 91.6984 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1420: GFLOPs: 98.1394. Time: 83.4833 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1421: GFLOPs: 91.5286. Time: 89.5130 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1422: GFLOPs: 91.5240. Time: 89.5175 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1423: GFLOPs: 98.1413. Time: 83.4817 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1424: GFLOPs: 87.2410. Time: 93.9122 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1425: GFLOPs: 87.2452. Time: 93.9077 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1426: GFLOPs: 87.2517. Time: 93.9008 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1427: GFLOPs: 87.2405. Time: 93.9128 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1428: GFLOPs: 87.2481. Time: 93.9046 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1429: GFLOPs: 87.2472. Time: 93.9056 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1430: GFLOPs: 87.2295. Time: 93.9246 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1431: GFLOPs: 87.2431. Time: 93.9100 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1432: GFLOPs: 87.2564. Time: 93.8957 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1433: GFLOPs: 87.2405. Time: 93.9128 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1434: GFLOPs: 87.2468. Time: 93.9061 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1435: GFLOPs: 87.2592. Time: 93.8927 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1436: GFLOPs: 87.2448. Time: 93.9082 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1437: GFLOPs: 98.9601. Time: 82.7910 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1438: GFLOPs: 89.1919. Time: 91.8582 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1439: GFLOPs: 98.5513. Time: 83.1344 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1440: GFLOPs: 89.1391. Time: 91.9125 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1441: GFLOPs: 89.0775. Time: 91.9761 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1442: GFLOPs: 87.6165. Time: 93.5098 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1443: GFLOPs: 87.6342. Time: 93.4909 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1444: GFLOPs: 87.6292. Time: 93.4963 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1445: GFLOPs: 87.6362. Time: 93.4888 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1446: GFLOPs: 98.9614. Time: 82.7899 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1447: GFLOPs: 87.6269. Time: 93.4987 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1448: GFLOPs: 87.6291. Time: 93.4964 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1449: GFLOPs: 87.8636. Time: 93.2468 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1450: GFLOPs: 87.8705. Time: 93.2394 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1451: GFLOPs: 87.8743. Time: 93.2355 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1452: GFLOPs: 87.8709. Time: 93.2391 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1453: GFLOPs: 87.8653. Time: 93.2450 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1454: GFLOPs: 87.8689. Time: 93.2411 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1455: GFLOPs: 87.8749. Time: 93.2348 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1456: GFLOPs: 87.8732. Time: 93.2367 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1457: GFLOPs: 87.6207. Time: 93.5053 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1458: GFLOPs: 87.6223. Time: 93.5036 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1459: GFLOPs: 87.8779. Time: 93.2317 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1460: GFLOPs: 87.8694. Time: 93.2407 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1461: GFLOPs: 87.8835. Time: 93.2257 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1462: GFLOPs: 98.5418. Time: 83.1424 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1463: GFLOPs: 87.8616. Time: 93.2489 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1464: GFLOPs: 92.7618. Time: 88.3230 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1465: GFLOPs: 92.7599. Time: 88.3248 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1466: GFLOPs: 92.7629. Time: 88.3220 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1467: GFLOPs: 87.8742. Time: 93.2356 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1468: GFLOPs: 92.7671. Time: 88.3179 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1469: GFLOPs: 13.9302. Time: 588.1484 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1470: GFLOPs: 3.2572. Time: 2515.3784 us. Best GFLOPs: 130.7471
2024-03-20 22:46:06 [INFO] [task_scheduler.cc:131] [Task #12: fused_nn_dense_add] Trial #1471: GFLOPs: 32.8891. Time: 249.1101 us. Best GFLOPs: 130.7471
