2024-03-21 07:30:26 [INFO] [task_scheduler.cc:160] Initializing Task #161: "fused_nn_max_pool2d"
2024-03-21 07:30:26 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(96), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(96), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(114), T.int64(114)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(96), T.int64(114), T.int64(114)):
            with T.block("pad_temp"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(p0[v_ax0, v_ax1, v_ax2 - T.int64(1), v_ax3 - T.int64(1)])
                T.writes(pad_temp[v_ax0, v_ax1, v_ax2, v_ax3])
                pad_temp[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1) <= v_ax2 and v_ax2 < T.int64(113) and T.int64(1) <= v_ax3 and v_ax3 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 - T.int64(1), v_ax3 - T.int64(1)], T.float32(-3.4028234663852886e+38))
        for ax0, ax1, ax2, ax3, rv0, rv1 in T.grid(T.int64(1), T.int64(96), T.int64(56), T.int64(56), T.int64(3), T.int64(3)):
            with T.block("pool_max"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_rv0, v_rv1 = T.axis.remap("SSSSRR", [ax0, ax1, ax2, ax3, rv0, rv1])
                T.reads(pad_temp[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1])
                T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                T.block_attr({"schedule_rule": "meta_schedule.pool_max"})
                with T.init():
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], pad_temp[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1])
2024-03-21 07:30:26 [INFO] [task_scheduler.cc:164] Total 2 design space(s) generated
2024-03-21 07:30:26 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(96), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(96), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
                for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                    for ax0_ax1_ax2_ax3_fused_0, rv0, rv1 in T.grid(T.int64(2), T.int64(3), T.int64(3)):
                        with T.block("pool_max"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_ax3_fused_0 * T.int64(262144) + ax0_ax1_ax2_ax3_fused_1 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3136))
                            v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(262144) + ax0_ax1_ax2_ax3_fused_1 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3136) // T.int64(56))
                            v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(262144) + ax0_ax1_ax2_ax3_fused_1 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_2) % T.int64(56))
                            v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                            T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(1024) + ax0_ax1_ax2_ax3_fused_2 < T.int64(301056))
                            T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                            T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
l11, l12, l13 = sch.split(loop=l10, factors=[None, 256, 1024], preserve_unit_iters=True)
sch.reorder(l12, l13, l11)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
2024-03-21 07:30:26 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(96), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(96), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(301056), thread="blockIdx.x"):
                for rv0_rv1_fused_0 in range(T.int64(1)):
                    for rv0_rv1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                        with T.block("pool_max"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                            v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                            v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                            v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) // T.int64(3))
                            v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) % T.int64(3))
                            T.where(rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1 < T.int64(9))
                            T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                            T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
2024-03-21 09:01:27 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-21 09:01:27 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-03-21 09:01:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x61c8f57541f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x61c8ee8bd448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x61c8efd3c388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x61c8f23ba798)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x61c8f26b4db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x61c8efd450f8)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x61c8efd3c288)]: 0 failure(s)
2024-03-21 09:01:28 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-03-21 09:01:29 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x61c8f57541f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x61c8ee8bd448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x61c8efd3c388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x61c8f23ba798)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x61c8f26b4db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x61c8efd450f8)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x61c8efd3c288)]: 0 failure(s)
2024-03-21 09:01:29 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x61c8f57541f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x61c8ee8bd448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x61c8efd3c388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x61c8f23ba798)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x61c8f26b4db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x61c8efd450f8)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x61c8efd3c288)]: 0 failure(s)
2024-03-21 09:01:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x61c8f57541f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x61c8ee8bd448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x61c8efd3c388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x61c8f23ba798)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x61c8f26b4db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x61c8efd450f8)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x61c8efd3c288)]: 0 failure(s)
2024-03-21 09:01:30 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x61c8f57541f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x61c8ee8bd448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x61c8efd3c388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x61c8f23ba798)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x61c8f26b4db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x61c8efd450f8)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x61c8efd3c288)]: 0 failure(s)
2024-03-21 09:01:30 [INFO] [evolutionary_search.cc:649] Scores of the best 45 candidates:
[1 : 16]:	0.9759  0.9743  0.9206  0.9170  0.8266  0.8243  0.8132  0.7958  0.7939  0.7792  0.7335  0.7127  0.7114  0.7014  0.6956  0.6929
[17 : 32]:	0.6563  0.6481  0.6282  0.6017  0.6010  0.5625  0.5504  0.5236  0.5221  0.4915  0.4596  0.4299  0.3979  0.3939  0.3395  0.3272
[33 : 45]:	0.3189  0.2591  0.2539  0.2021  0.1722  0.1719  0.1382  0.1312  0.1054  0.0738  0.0309  0.0129  0.0102
2024-03-21 09:01:30 [INFO] [evolutionary_search.cc:727] Got 45 candidate(s) with evolutionary search
2024-03-21 09:01:30 [INFO] [evolutionary_search.cc:730] Sending 45 candidates(s) for measurement
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:121] [Task #161: fused_nn_max_pool2d] Trial #1: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(96), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(96), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(301056), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #2: GFLOPs: 15.0641. Time: 179.8652 us. Best GFLOPs: 15.0641
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #3: GFLOPs: 5.7166. Time: 473.9715 us. Best GFLOPs: 15.0641
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #4: GFLOPs: 15.1792. Time: 178.5016 us. Best GFLOPs: 15.1792
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #5: GFLOPs: 14.5578. Time: 186.1210 us. Best GFLOPs: 15.1792
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #6: GFLOPs: 15.2101. Time: 178.1386 us. Best GFLOPs: 15.2101
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #7: GFLOPs: 11.7576. Time: 230.4466 us. Best GFLOPs: 15.2101
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #8: GFLOPs: 435.1830. Time: 6.2261 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #9: GFLOPs: 15.2108. Time: 178.1305 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #10: GFLOPs: 15.2002. Time: 178.2544 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #11: GFLOPs: 15.2137. Time: 178.0964 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #12: GFLOPs: 15.2001. Time: 178.2555 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #13: GFLOPs: 15.2003. Time: 178.2532 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #14: GFLOPs: 15.2131. Time: 178.1035 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #15: GFLOPs: 11.9614. Time: 226.5206 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #16: GFLOPs: 2.7479. Time: 986.0317 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #17: GFLOPs: 15.1996. Time: 178.2621 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #18: GFLOPs: 2.7490. Time: 985.6298 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #19: GFLOPs: 15.1841. Time: 178.4433 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #20: GFLOPs: 11.7313. Time: 230.9628 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #21: GFLOPs: 5.6901. Time: 476.1782 us. Best GFLOPs: 435.1830
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #22: GFLOPs: 452.8081. Time: 5.9838 us. Best GFLOPs: 452.8081
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #23: GFLOPs: 2.7473. Time: 986.2309 us. Best GFLOPs: 452.8081
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #24: GFLOPs: 15.2136. Time: 178.0978 us. Best GFLOPs: 452.8081
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #25: GFLOPs: 15.1887. Time: 178.3897 us. Best GFLOPs: 452.8081
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #26: GFLOPs: 394.1414. Time: 6.8744 us. Best GFLOPs: 452.8081
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #27: GFLOPs: 15.2133. Time: 178.1010 us. Best GFLOPs: 452.8081
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #28: GFLOPs: 11.7369. Time: 230.8535 us. Best GFLOPs: 452.8081
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #29: GFLOPs: 15.0661. Time: 179.8411 us. Best GFLOPs: 452.8081
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #30: GFLOPs: 15.0714. Time: 179.7777 us. Best GFLOPs: 452.8081
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #31: GFLOPs: 452.9628. Time: 5.9817 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #32: GFLOPs: 5.6564. Time: 479.0158 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #33: GFLOPs: 15.0696. Time: 179.7991 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #34: GFLOPs: 15.1887. Time: 178.3893 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #35: GFLOPs: 452.6981. Time: 5.9852 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #36: GFLOPs: 5.6597. Time: 478.7371 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #37: GFLOPs: 15.0952. Time: 179.4938 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #38: GFLOPs: 15.2125. Time: 178.1109 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #39: GFLOPs: 2.7480. Time: 985.9815 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #40: GFLOPs: 15.2001. Time: 178.2556 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #41: GFLOPs: 5.6596. Time: 478.7486 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #42: GFLOPs: 11.6658. Time: 232.2599 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #43: GFLOPs: 11.6727. Time: 232.1234 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #44: GFLOPs: 15.0659. Time: 179.8437 us. Best GFLOPs: 452.9628
2024-03-21 09:12:14 [INFO] [task_scheduler.cc:131] [Task #161: fused_nn_max_pool2d] Trial #45: GFLOPs: 2.7355. Time: 990.4992 us. Best GFLOPs: 452.9628
