2024-03-22 04:15:25 [INFO] [task_scheduler.cc:160] Initializing Task #17: "fused_nn_conv2d_add_nn_relu_8"
2024-03-22 04:15:25 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)))
        conv2d_nchw = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1024), T.int64(14), T.int64(14)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = p0[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(256), T.int64(14), T.int64(14), T.int64(1024), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1[v_ff, v_rc, v_ry, v_rx])
                T.writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(14), T.int64(14)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(14), T.int64(14)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2024-03-22 04:15:25 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-03-22 04:15:25 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(28), thread="threadIdx.x"):
                        for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(1), T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(896)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + ax0_ax1_ax2_ax3_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ff_3 * T.int64(64) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(32) + rc_1 + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ax1)
                                v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 64])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 32, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
2024-03-22 04:15:25 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(28), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(896)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + ax0_ax1_ax2_ax3_fused // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + ax0_ax1_ax2_ax3_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + ax0_ax1_ax2_ax3_fused % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ff_3 * T.int64(64) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + rc_1 + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ax1)
                                v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 64])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 32, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
2024-03-22 04:15:25 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(28), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(896)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + ax0_ax1_ax2_ax3_fused // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + ax0_ax1_ax2_ax3_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + ax0_ax1_ax2_ax3_fused % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ff_3 * T.int64(64) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + rc_1 + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ax1)
                                v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 64])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 32, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
2024-03-22 04:36:55 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 04:36:55 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-03-22 04:36:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 484 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:36:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 971 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:36:57 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-22 04:36:58 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 83 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:36:59 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:37:01 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 71 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:37:02 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 76 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:37:02 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9995  0.9993  0.9990  0.9978  0.9974  0.9968  0.9967  0.9965  0.9964  0.9961  0.9959  0.9957  0.9948  0.9945  0.9942  0.9933
[17 : 32]:	0.9928  0.9916  0.9915  0.9907  0.9903  0.9888  0.9881  0.9875  0.9867  0.9855  0.9853  0.9835  0.9831  0.9823  0.9823  0.9820
[33 : 48]:	0.9786  0.9781  0.9773  0.9765  0.9761  0.9755  0.9752  0.9748  0.9743  0.9740  0.9723  0.9718  0.9716  0.9695  0.9689  0.9679
[49 : 64]:	0.9677  0.9673  0.9670  0.9668  0.9668  0.9656  0.9652  0.9640  0.9628  0.9628  0.9625  0.9617  0.9613  0.9603  0.9602  0.9601
2024-03-22 04:37:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 04:37:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(14), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), yy_3_init * T.int64(14) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(56)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(196) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(14), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), yy_3 * T.int64(14) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(14), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 4, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108 = sch.split(loop=l106, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113 = sch.get_loops(block=b87)
l114, l115 = sch.split(loop=l113, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l115, thread_axis="threadIdx.x")
b116 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b116, ann_key="meta_schedule.unroll_explicit")
b117, b118, b119, b120 = sch.get_child_blocks(b116)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b117)
l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b118)
l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l151, l152, l153, l154, l155, l156, l157 = sch.get_loops(block=b120)
b158 = sch.get_block(name="conv2d_nchw", func_name="main")
l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176 = sch.get_loops(block=b158)
b177 = sch.decompose_reduction(block=b158, loop=l162)
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #2: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(7) + xx_3_init * T.int64(7) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(196))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(7) + xx_3 * T.int64(7) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b120)
l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b121)
l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b122)
b162 = sch.get_block(name="conv2d_nchw", func_name="main")
l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b162)
b181 = sch.decompose_reduction(block=b162, loop=l166)
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #3: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(392), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(392), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1) // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1) % T.int64(196) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(392), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 4, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108 = sch.split(loop=l106, factors=[None, 392], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113 = sch.get_loops(block=b87)
l114, l115 = sch.split(loop=l113, factors=[None, 392], preserve_unit_iters=True)
sch.bind(loop=l115, thread_axis="threadIdx.x")
b116 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b116, ann_key="meta_schedule.unroll_explicit")
b117, b118, b119, b120 = sch.get_child_blocks(b116)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b117)
l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b118)
l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l151, l152, l153, l154, l155, l156, l157 = sch.get_loops(block=b120)
b158 = sch.get_block(name="conv2d_nchw", func_name="main")
l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176 = sch.get_loops(block=b158)
b177 = sch.decompose_reduction(block=b158, loop=l162)
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #4: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(16) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1024))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(16) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(8) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #5: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(7), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(16) + ff_3_init * T.int64(16) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(2))
                                    v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4096))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(16) + ff_3 * T.int64(16) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #6: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(196) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(392))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(14)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #7: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(7), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(224))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(7), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #8: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(7), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(56))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #9: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(28), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(32) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(14)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 32, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #10: GFLOPs: 1386.8140. Time: 74.1706 us. Best GFLOPs: 1386.8140
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #11: GFLOPs: 2044.4855. Time: 50.3113 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #12: GFLOPs: 761.3548. Time: 135.1023 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #13: GFLOPs: 34.8539. Time: 2951.1981 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #14: GFLOPs: 70.6716. Time: 1455.4750 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #15: GFLOPs: 262.4965. Time: 391.8559 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #16: GFLOPs: 12.0825. Time: 8513.1943 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #17: GFLOPs: 131.0081. Time: 785.1483 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #18: GFLOPs: 171.5268. Time: 599.6777 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #19: GFLOPs: 144.2174. Time: 713.2342 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #20: GFLOPs: 127.9201. Time: 804.1022 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #21: GFLOPs: 54.4259. Time: 1889.9248 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #22: GFLOPs: 534.7338. Time: 192.3589 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #23: GFLOPs: 323.1177. Time: 318.3385 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #24: GFLOPs: 75.7866. Time: 1357.2428 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #25: GFLOPs: 93.4562. Time: 1100.6309 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #26: GFLOPs: 636.3099. Time: 161.6520 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #27: GFLOPs: 211.5256. Time: 486.2807 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #28: GFLOPs: 529.1450. Time: 194.3906 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #29: GFLOPs: 278.4778. Time: 369.3681 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #30: GFLOPs: 258.9378. Time: 397.2414 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #31: GFLOPs: 202.4087. Time: 508.1837 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #32: GFLOPs: 334.9886. Time: 307.0576 us. Best GFLOPs: 2044.4855
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #33: GFLOPs: 3017.2063. Time: 34.0914 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #34: GFLOPs: 668.1690. Time: 153.9443 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #35: GFLOPs: 456.7132. Time: 225.2197 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #36: GFLOPs: 25.2959. Time: 4066.3040 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #37: GFLOPs: 159.6979. Time: 644.0960 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #38: GFLOPs: 8.6630. Time: 11873.6216 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #39: GFLOPs: 641.0530. Time: 160.4560 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #40: GFLOPs: 16.9747. Time: 6059.6704 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #41: GFLOPs: 391.2776. Time: 262.8845 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #42: GFLOPs: 743.4806. Time: 138.3503 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #43: GFLOPs: 716.3375. Time: 143.5926 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #44: GFLOPs: 591.4465. Time: 173.9140 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #45: GFLOPs: 222.4433. Time: 462.4136 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #46: GFLOPs: 112.7732. Time: 912.1030 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #47: GFLOPs: 723.2576. Time: 142.2188 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #48: GFLOPs: 23.4887. Time: 4379.1583 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #49: GFLOPs: 1606.2985. Time: 64.0359 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #50: GFLOPs: 21.6534. Time: 4750.3360 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #51: GFLOPs: 93.3636. Time: 1101.7227 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #52: GFLOPs: 406.1201. Time: 253.2768 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #53: GFLOPs: 23.7848. Time: 4324.6507 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #54: GFLOPs: 324.4661. Time: 317.0155 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #55: GFLOPs: 174.6224. Time: 589.0470 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #56: GFLOPs: 114.4136. Time: 899.0262 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #57: GFLOPs: 608.3382. Time: 169.0849 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #58: GFLOPs: 384.4603. Time: 267.5460 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #59: GFLOPs: 21.5817. Time: 4766.1150 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #60: GFLOPs: 11.8540. Time: 8677.2906 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #61: GFLOPs: 12.5389. Time: 8203.3427 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #62: GFLOPs: 376.0759. Time: 273.5108 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #63: GFLOPs: 1661.3483. Time: 61.9140 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #64: GFLOPs: 1020.4277. Time: 100.8017 us. Best GFLOPs: 3017.2063
2024-03-22 04:47:18 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 04:47:19 [INFO] [evolutionary_search.cc:715] Picked top 55 candidate(s) from database
2024-03-22 04:47:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 428 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:47:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 868 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:47:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1299 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:47:20 [INFO] [evolutionary_search.cc:723] Sampled 72 candidate(s)
2024-03-22 04:47:22 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:47:24 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 83 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:47:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 99 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:47:28 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 85 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:47:29 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1430  1.1360  1.1205  1.1020  1.0995  1.0995  1.0939  1.0919  1.0919  1.0866  1.0866  1.0861  1.0851  1.0847  1.0829  1.0802
[17 : 32]:	1.0796  1.0796  1.0789  1.0789  1.0785  1.0773  1.0732  1.0716  1.0694  1.0691  1.0690  1.0687  1.0680  1.0661  1.0657  1.0654
[33 : 48]:	1.0646  1.0639  1.0603  1.0557  1.0488  1.0469  1.0447  1.0372  1.0362  1.0355  1.0355  1.0345  1.0283  1.0211  1.0193  1.0167
[49 : 64]:	1.0158  1.0146  1.0124  1.0112  1.0092  1.0063  0.9936  0.9898  0.9823  0.9823  0.9697  0.9697  0.9662  0.9608  0.9581  0.9581
2024-03-22 04:47:29 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 04:47:29 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #65: GFLOPs: 3442.2692. Time: 29.8817 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #66: GFLOPs: 3001.3866. Time: 34.2711 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #67: GFLOPs: 2994.3805. Time: 34.3513 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #68: GFLOPs: 1506.1976. Time: 68.2917 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #69: GFLOPs: 1596.3841. Time: 64.4336 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #70: GFLOPs: 1596.3341. Time: 64.4356 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #71: GFLOPs: 1631.1725. Time: 63.0594 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #72: GFLOPs: 1670.5225. Time: 61.5740 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #73: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  336: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  335: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  334: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  333: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  332: tvm::transform::Pass::operator()(tvm::IRModule) const
  331: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  330: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  329: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  328: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  327: _ZN3tvm7runtime13PackedFuncObj
  326: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  325: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  324: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  323: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  322: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  321: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  320: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  319: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  318: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  317: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  316: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  315: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  314: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  313: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  312: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  311: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  310: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  309: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  308: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  307: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  306: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  305: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  304: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  303: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  302: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  301: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  300: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  299: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  298: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  260: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  259: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  258: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  257: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  256: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  255: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  254: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  253: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  252: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  251: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  250: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  249: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  248: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  244: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  243: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  242: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  241: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  237: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  236: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  235: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  234: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  233: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  232: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  231: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  230: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  229: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  228: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  227: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  226: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  222: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  221: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  220: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  219: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  218: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  217: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  216: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  215: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  214: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  213: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  212: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  211: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  210: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  209: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  208: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  207: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  206: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  204: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  203: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  202: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  200: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  199: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  198: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  197: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  191: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  181: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  180: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  179: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  178: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  155: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  154: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  153: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  152: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  151: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  150: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  149: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  148: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  117: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  116: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  106: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  105: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  82: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  81: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  77: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  76: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b120)
l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l137, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l137, ann_key="pragma_unroll_explicit", ann_val=1)
l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b122)
b162 = sch.get_block(name="conv2d_nchw", func_name="main")
l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b162)
b181 = sch.decompose_reduction(block=b162, loop=l166)
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #74: GFLOPs: 1658.6258. Time: 62.0157 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #75: GFLOPs: 1631.8852. Time: 63.0319 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #76: GFLOPs: 1657.9434. Time: 62.0412 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #77: GFLOPs: 1658.4339. Time: 62.0229 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #78: GFLOPs: 2383.6838. Time: 43.1520 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #79: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  336: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  335: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  334: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  333: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  332: tvm::transform::Pass::operator()(tvm::IRModule) const
  331: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  330: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  329: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  328: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  327: _ZN3tvm7runtime13PackedFuncObj
  326: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  325: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  324: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  323: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  322: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  321: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  320: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  319: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  318: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  317: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  316: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  315: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  314: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  313: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  312: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  311: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  310: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  309: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  308: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  307: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  306: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  305: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  304: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  303: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  302: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  301: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  300: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  299: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  298: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  260: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  259: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  258: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  257: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  256: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  255: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  254: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  253: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  252: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  251: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  250: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  249: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  248: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  244: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  243: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  242: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  241: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  237: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  236: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  235: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  234: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  233: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  232: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  231: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  230: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  229: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  228: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  227: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  226: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  222: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  221: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  220: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  219: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  218: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  217: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  216: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  215: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  214: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  213: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  212: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  211: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  210: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  209: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  208: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  207: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  206: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  204: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  203: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  202: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  200: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  199: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  198: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  197: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  191: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  181: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  180: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  179: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  178: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  155: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  154: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  153: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  152: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  151: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  150: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  149: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  148: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  117: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  116: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  106: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  105: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  82: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  81: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  77: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  76: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b120)
l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l137, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l137, ann_key="pragma_unroll_explicit", ann_val=1)
l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b122)
b162 = sch.get_block(name="conv2d_nchw", func_name="main")
l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b162)
b181 = sch.decompose_reduction(block=b162, loop=l166)
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #80: GFLOPs: 3204.9910. Time: 32.0939 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #81: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  336: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  335: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  334: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  333: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  332: tvm::transform::Pass::operator()(tvm::IRModule) const
  331: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  330: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  329: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  328: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  327: _ZN3tvm7runtime13PackedFuncObj
  326: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  325: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  324: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  323: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  322: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  321: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  320: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  319: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  318: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  317: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  316: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  315: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  314: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  313: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  312: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  311: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  310: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  309: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  308: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  307: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  306: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  305: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  304: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  303: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  302: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  301: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  300: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  299: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  298: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  260: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  259: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  258: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  257: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  256: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  255: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  254: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  253: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  252: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  251: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  250: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  249: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  248: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  244: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  243: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  242: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  241: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  237: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  236: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  235: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  234: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  233: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  232: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  231: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  230: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  229: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  228: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  227: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  226: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  222: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  221: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  220: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  219: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  218: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  217: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  216: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  215: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  214: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  213: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  212: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  211: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  210: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  209: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  208: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  207: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  206: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  204: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  203: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  202: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  200: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  199: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  198: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  197: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  191: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  181: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  180: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  179: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  178: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  155: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  154: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  153: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  152: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  151: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  150: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  149: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  148: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  117: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  116: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  106: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  105: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  82: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  81: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  77: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  76: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b120)
l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l137, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l137, ann_key="pragma_unroll_explicit", ann_val=1)
l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b122)
b162 = sch.get_block(name="conv2d_nchw", func_name="main")
l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b162)
b181 = sch.decompose_reduction(block=b162, loop=l166)
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #82: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  336: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  335: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  334: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  333: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  332: tvm::transform::Pass::operator()(tvm::IRModule) const
  331: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  330: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  329: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  328: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  327: _ZN3tvm7runtime13PackedFuncObj
  326: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  325: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  324: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  323: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  322: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  321: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  320: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  319: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  318: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  317: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  316: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  315: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  314: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  313: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  312: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  311: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  310: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  309: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  308: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  307: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  306: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  305: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  304: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  303: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  302: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  301: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  300: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  299: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  298: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  260: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  259: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  258: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  257: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  256: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  255: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  254: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  253: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  252: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  251: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  250: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  249: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  248: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  244: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  243: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  242: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  241: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  237: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  236: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  235: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  234: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  233: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  232: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  231: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  230: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  229: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  228: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  227: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  226: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  222: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  221: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  220: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  219: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  218: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  217: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  216: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  215: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  214: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  213: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  212: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  211: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  210: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  209: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  208: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  207: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  206: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  204: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  203: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  202: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  200: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  199: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  198: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  197: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  191: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  181: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  180: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  179: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  178: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  155: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  154: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  153: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  152: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  151: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  150: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  149: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  148: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  117: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  116: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  106: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  105: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  82: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  81: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  77: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  76: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b120)
l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l137, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l137, ann_key="pragma_unroll_explicit", ann_val=1)
l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b122)
b162 = sch.get_block(name="conv2d_nchw", func_name="main")
l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b162)
b181 = sch.decompose_reduction(block=b162, loop=l166)
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #83: GFLOPs: 1648.4834. Time: 62.3972 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #84: GFLOPs: 1660.1806. Time: 61.9576 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #85: GFLOPs: 1521.3253. Time: 67.6126 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #86: GFLOPs: 1650.1398. Time: 62.3346 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #87: GFLOPs: 1579.2695. Time: 65.1319 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #88: GFLOPs: 1574.6446. Time: 65.3232 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #89: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  336: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  335: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  334: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  333: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  332: tvm::transform::Pass::operator()(tvm::IRModule) const
  331: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  330: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  329: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  328: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  327: _ZN3tvm7runtime13PackedFuncObj
  326: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  325: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  324: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  323: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  322: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  321: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  320: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  319: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  318: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  317: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  316: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  315: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  314: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  313: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  312: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  311: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  310: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  309: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  308: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  307: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  306: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  305: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  304: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  303: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  302: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  301: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  300: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  299: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  298: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  260: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  259: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  258: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  257: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  256: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  255: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  254: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  253: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  252: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  251: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  250: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  249: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  248: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  244: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  243: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  242: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  241: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  237: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  236: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  235: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  234: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  233: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  232: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  231: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  230: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  229: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  228: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  227: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  226: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  222: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  221: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  220: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  219: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  218: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  217: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  216: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  215: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  214: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  213: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  212: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  211: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  210: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  209: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  208: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  207: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  206: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  204: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  203: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  202: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  200: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  199: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  198: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  197: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  191: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  181: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  180: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  179: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  178: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  155: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  154: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  153: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  152: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  151: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  150: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  149: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  148: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  117: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  116: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  106: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  105: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  82: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  81: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  77: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  76: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #90: GFLOPs: 2385.5675. Time: 43.1180 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #91: GFLOPs: 1604.8817. Time: 64.0924 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #92: GFLOPs: 1512.5072. Time: 68.0068 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #93: GFLOPs: 2332.8136. Time: 44.0930 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #94: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  336: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  335: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  334: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  333: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  332: tvm::transform::Pass::operator()(tvm::IRModule) const
  331: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  330: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  329: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  328: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  327: _ZN3tvm7runtime13PackedFuncObj
  326: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  325: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  324: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  323: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  322: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  321: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  320: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  319: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  318: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  317: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  316: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  315: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  314: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  313: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  312: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  311: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  310: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  309: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  308: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  307: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  306: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  305: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  304: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  303: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  302: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  301: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  300: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  299: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  298: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  260: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  259: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  258: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  257: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  256: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  255: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  254: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  253: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  252: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  251: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  250: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  249: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  248: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  244: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  243: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  242: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  241: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  237: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  236: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  235: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  234: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  233: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  232: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  231: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  230: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  229: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  228: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  227: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  226: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  222: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  221: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  220: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  219: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  218: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  217: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  216: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  215: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  214: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  213: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  212: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  211: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  210: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  209: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  208: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  207: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  206: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  204: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  203: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  202: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  200: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  199: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  198: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  197: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  191: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  181: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  180: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  179: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  178: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  155: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  154: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  153: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  152: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  151: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  150: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  149: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  148: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  117: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  116: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  106: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  105: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  82: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  81: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  77: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  76: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(32) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #95: GFLOPs: 2375.2874. Time: 43.3046 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #96: GFLOPs: 1503.8593. Time: 68.3979 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #97: GFLOPs: 3206.3116. Time: 32.0807 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #98: GFLOPs: 1515.8369. Time: 67.8574 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #99: GFLOPs: 2106.9842. Time: 48.8190 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #100: GFLOPs: 2107.1203. Time: 48.8158 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #101: GFLOPs: 1872.7087. Time: 54.9262 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #102: GFLOPs: 1355.5034. Time: 75.8838 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #103: GFLOPs: 2078.7817. Time: 49.4813 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #104: GFLOPs: 2077.9091. Time: 49.5021 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #105: GFLOPs: 2980.3300. Time: 34.5132 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #106: GFLOPs: 2980.3303. Time: 34.5132 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #107: GFLOPs: 3047.4759. Time: 33.7528 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #108: GFLOPs: 1638.5622. Time: 62.7750 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #109: GFLOPs: 1330.0954. Time: 77.3334 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #110: GFLOPs: 1516.4437. Time: 67.8303 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #111: GFLOPs: 961.8409. Time: 106.9416 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #112: GFLOPs: 2977.4964. Time: 34.5461 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #113: GFLOPs: 1634.4517. Time: 62.9329 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #114: GFLOPs: 3124.9512. Time: 32.9160 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #115: GFLOPs: 2975.2662. Time: 34.5720 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #116: GFLOPs: 1516.5636. Time: 67.8249 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #117: GFLOPs: 1524.4557. Time: 67.4738 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #118: GFLOPs: 3038.9177. Time: 33.8478 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #119: GFLOPs: 3101.6998. Time: 33.1627 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #120: GFLOPs: 980.6562. Time: 104.8898 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #121: GFLOPs: 2393.0956. Time: 42.9823 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #122: GFLOPs: 2393.1450. Time: 42.9814 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #123: GFLOPs: 3037.3059. Time: 33.8658 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #124: GFLOPs: 2984.0167. Time: 34.4706 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #125: GFLOPs: 1625.7104. Time: 63.2713 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #126: GFLOPs: 766.7644. Time: 134.1492 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #127: GFLOPs: 68.0939. Time: 1510.5733 us. Best GFLOPs: 3442.2692
2024-03-22 04:48:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #128: GFLOPs: 408.0867. Time: 252.0562 us. Best GFLOPs: 3442.2692
2024-03-22 04:51:34 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 04:51:34 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 04:51:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 389 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:51:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 769 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:51:36 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-03-22 04:51:37 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:51:39 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 110 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:51:41 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:51:44 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 77 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:51:45 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1942  1.1645  1.1449  1.1060  1.0844  1.0830  1.0668  1.0657  1.0519  1.0484  1.0228  1.0177  1.0177  1.0071  1.0062  1.0040
[17 : 32]:	1.0017  0.9998  0.9989  0.9926  0.9910  0.9910  0.9901  0.9874  0.9872  0.9872  0.9850  0.9850  0.9792  0.9761  0.9736  0.9724
[33 : 48]:	0.9687  0.9647  0.9623  0.9607  0.9579  0.9555  0.9555  0.9517  0.9455  0.9386  0.9386  0.9369  0.9360  0.9358  0.9348  0.9327
[49 : 64]:	0.9308  0.9305  0.9305  0.9284  0.9275  0.9268  0.9257  0.9257  0.9197  0.9169  0.9169  0.9106  0.9098  0.9088  0.9071  0.9071
2024-03-22 04:51:45 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 04:51:45 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #129: GFLOPs: 1005.5443. Time: 102.2936 us. Best GFLOPs: 3442.2692
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #130: GFLOPs: 1467.2667. Time: 70.1037 us. Best GFLOPs: 3442.2692
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #131: GFLOPs: 1067.8159. Time: 96.3282 us. Best GFLOPs: 3442.2692
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #132: GFLOPs: 791.2790. Time: 129.9931 us. Best GFLOPs: 3442.2692
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #133: GFLOPs: 2862.2158. Time: 35.9375 us. Best GFLOPs: 3442.2692
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #134: GFLOPs: 813.2201. Time: 126.4858 us. Best GFLOPs: 3442.2692
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #135: GFLOPs: 1304.5035. Time: 78.8505 us. Best GFLOPs: 3442.2692
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #136: GFLOPs: 1341.0706. Time: 76.7005 us. Best GFLOPs: 3442.2692
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #137: GFLOPs: 1269.0608. Time: 81.0527 us. Best GFLOPs: 3442.2692
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #138: GFLOPs: 4178.5679. Time: 24.6163 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #139: GFLOPs: 2247.2285. Time: 45.7723 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #140: GFLOPs: 2452.8113. Time: 41.9359 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #141: GFLOPs: 2560.1702. Time: 40.1773 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #142: GFLOPs: 1533.1038. Time: 67.0932 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #143: GFLOPs: 2519.1286. Time: 40.8319 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #144: GFLOPs: 2558.4857. Time: 40.2038 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #145: GFLOPs: 1331.1950. Time: 77.2695 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #146: GFLOPs: 1460.8285. Time: 70.4126 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #147: GFLOPs: 1370.1241. Time: 75.0741 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #148: GFLOPs: 2453.8650. Time: 41.9179 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #149: GFLOPs: 2547.9443. Time: 40.3701 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #150: GFLOPs: 2480.5538. Time: 41.4669 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #151: GFLOPs: 2568.0780. Time: 40.0536 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #152: GFLOPs: 2518.8013. Time: 40.8372 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #153: GFLOPs: 2425.9519. Time: 42.4002 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #154: GFLOPs: 2415.9150. Time: 42.5763 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #155: GFLOPs: 2526.6130. Time: 40.7109 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #156: GFLOPs: 2535.8021. Time: 40.5634 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #157: GFLOPs: 3426.9862. Time: 30.0149 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #158: GFLOPs: 1333.7541. Time: 77.1213 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #159: GFLOPs: 2378.3357. Time: 43.2491 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #160: GFLOPs: 2522.2776. Time: 40.7809 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #161: GFLOPs: 2154.9666. Time: 47.7320 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #162: GFLOPs: 3257.5174. Time: 31.5764 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #163: GFLOPs: 3515.2827. Time: 29.2610 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #164: GFLOPs: 2461.9843. Time: 41.7796 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #165: GFLOPs: 2246.5844. Time: 45.7854 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #166: GFLOPs: 3360.0706. Time: 30.6127 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #167: GFLOPs: 3221.5190. Time: 31.9293 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #168: GFLOPs: 1625.6770. Time: 63.2726 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #169: GFLOPs: 2417.9409. Time: 42.5407 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #170: GFLOPs: 3505.5087. Time: 29.3426 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #171: GFLOPs: 3376.6194. Time: 30.4627 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #172: GFLOPs: 1317.3327. Time: 78.0826 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #173: GFLOPs: 1383.4502. Time: 74.3509 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #174: GFLOPs: 2505.8209. Time: 41.0487 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #175: GFLOPs: 3149.4192. Time: 32.6602 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #176: GFLOPs: 3487.2517. Time: 29.4962 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #177: GFLOPs: 1780.5452. Time: 57.7693 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #178: GFLOPs: 2489.9586. Time: 41.3102 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #179: GFLOPs: 2533.2360. Time: 40.6045 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #180: GFLOPs: 1310.0432. Time: 78.5171 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #181: GFLOPs: 2126.1025. Time: 48.3800 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #182: GFLOPs: 2519.6831. Time: 40.8229 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #183: GFLOPs: 1302.9599. Time: 78.9440 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #184: GFLOPs: 1303.0186. Time: 78.9404 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #185: GFLOPs: 2207.3198. Time: 46.5999 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #186: GFLOPs: 2914.0628. Time: 35.2981 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #187: GFLOPs: 2476.6347. Time: 41.5325 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #188: GFLOPs: 2256.0856. Time: 45.5926 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #189: GFLOPs: 1044.3111. Time: 98.4963 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #190: GFLOPs: 391.4415. Time: 262.7744 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #191: GFLOPs: 1514.8959. Time: 67.8996 us. Best GFLOPs: 4178.5679
2024-03-22 04:52:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #192: GFLOPs: 723.4133. Time: 142.1882 us. Best GFLOPs: 4178.5679
2024-03-22 04:56:04 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 04:56:04 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 04:56:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 393 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:56:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 776 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:56:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:56:06 [INFO] [evolutionary_search.cc:723] Sampled 71 candidate(s)
2024-03-22 04:56:07 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 108 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:56:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 118 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:56:11 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 86 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:56:13 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:56:14 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.5041  1.4185  1.2904  1.1907  1.1606  1.1385  1.1250  1.0916  1.0911  1.0664  1.0631  1.0351  1.0184  1.0184  1.0127  1.0022
[17 : 32]:	0.9855  0.9766  0.9760  0.9750  0.9712  0.9712  0.9710  0.9706  0.9706  0.9637  0.9618  0.9613  0.9593  0.9593  0.9560  0.9552
[33 : 48]:	0.9539  0.9539  0.9531  0.9514  0.9510  0.9471  0.9358  0.9264  0.9248  0.9242  0.9235  0.9207  0.9202  0.9165  0.9153  0.9130
[49 : 64]:	0.9114  0.9099  0.9061  0.9054  0.9037  0.8984  0.8972  0.8921  0.8895  0.8815  0.8814  0.8804  0.8773  0.8724  0.8704  0.8636
2024-03-22 04:56:15 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 04:56:15 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #193: GFLOPs: 856.7691. Time: 120.0566 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #194: GFLOPs: 888.9214. Time: 115.7142 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #195: GFLOPs: 705.6145. Time: 145.7748 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #196: GFLOPs: 2738.0300. Time: 37.5674 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #197: GFLOPs: 2641.9163. Time: 38.9342 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #198: GFLOPs: 2263.2853. Time: 45.4476 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #199: GFLOPs: 2490.2953. Time: 41.3047 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #200: GFLOPs: 2088.7805. Time: 49.2444 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #201: GFLOPs: 2128.2579. Time: 48.3310 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #202: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(28) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(28) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(28) // T.int64(2))
                                    v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(28) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(28) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(28) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(28) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(28) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #203: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(32) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 32, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #204: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(32) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #205: GFLOPs: 738.6598. Time: 139.2533 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #206: GFLOPs: 738.6220. Time: 139.2604 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #207: GFLOPs: 325.9961. Time: 315.5277 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #208: GFLOPs: 18.1999. Time: 5651.7368 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #209: GFLOPs: 1084.1708. Time: 94.8751 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #210: GFLOPs: 1071.0493. Time: 96.0374 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #211: GFLOPs: 661.2729. Time: 155.5497 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #212: GFLOPs: 907.9434. Time: 113.2899 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #213: GFLOPs: 1060.2211. Time: 97.0183 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #214: GFLOPs: 1093.0293. Time: 94.1062 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #215: GFLOPs: 2256.1547. Time: 45.5912 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #216: GFLOPs: 663.0520. Time: 155.1323 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #217: GFLOPs: 663.0469. Time: 155.1335 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #218: GFLOPs: 592.9140. Time: 173.4835 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #219: GFLOPs: 3390.5914. Time: 30.3371 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #220: GFLOPs: 682.6810. Time: 150.6718 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #221: GFLOPs: 682.7178. Time: 150.6637 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #222: GFLOPs: 683.2810. Time: 150.5395 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #223: GFLOPs: 635.3181. Time: 161.9044 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #224: GFLOPs: 3355.3744. Time: 30.6555 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #225: GFLOPs: 681.7196. Time: 150.8843 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #226: GFLOPs: 701.7670. Time: 146.5740 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #227: GFLOPs: 2547.5522. Time: 40.3763 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #228: GFLOPs: 3347.9129. Time: 30.7239 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #229: GFLOPs: 41.9105. Time: 2454.2994 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #230: GFLOPs: 370.4379. Time: 277.6735 us. Best GFLOPs: 4178.5679
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #231: GFLOPs: 4183.2146. Time: 24.5889 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #232: GFLOPs: 3937.8084. Time: 26.1213 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #233: GFLOPs: 2410.0837. Time: 42.6793 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #234: GFLOPs: 2437.4354. Time: 42.2004 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #235: GFLOPs: 2124.9690. Time: 48.4058 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #236: GFLOPs: 4163.7068. Time: 24.7041 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #237: GFLOPs: 4032.6496. Time: 25.5070 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #238: GFLOPs: 4161.5077. Time: 24.7172 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #239: GFLOPs: 3344.8843. Time: 30.7517 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #240: GFLOPs: 628.6490. Time: 163.6220 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #241: GFLOPs: 2940.9864. Time: 34.9749 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #242: GFLOPs: 3743.6426. Time: 27.4761 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #243: GFLOPs: 2120.9595. Time: 48.4973 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #244: GFLOPs: 2999.5187. Time: 34.2924 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #245: GFLOPs: 4080.0390. Time: 25.2107 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #246: GFLOPs: 2349.9414. Time: 43.7716 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #247: GFLOPs: 2246.5406. Time: 45.7863 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #248: GFLOPs: 23.4577. Time: 4384.9460 us. Best GFLOPs: 4183.2146
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #249: GFLOPs: 4274.4256. Time: 24.0642 us. Best GFLOPs: 4274.4256
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #250: GFLOPs: 2643.0057. Time: 38.9181 us. Best GFLOPs: 4274.4256
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #251: GFLOPs: 1381.7368. Time: 74.4431 us. Best GFLOPs: 4274.4256
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #252: GFLOPs: 1332.5145. Time: 77.1930 us. Best GFLOPs: 4274.4256
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #253: GFLOPs: 3009.1378. Time: 34.1828 us. Best GFLOPs: 4274.4256
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #254: GFLOPs: 2720.1868. Time: 37.8139 us. Best GFLOPs: 4274.4256
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #255: GFLOPs: 654.2604. Time: 157.2169 us. Best GFLOPs: 4274.4256
2024-03-22 04:57:03 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #256: GFLOPs: 470.3057. Time: 218.7105 us. Best GFLOPs: 4274.4256
2024-03-22 04:59:38 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 04:59:38 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 04:59:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 382 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:59:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 770 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:59:39 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-03-22 04:59:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 98 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:59:43 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 106 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:59:45 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 107 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:59:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 104 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 04:59:48 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	2.0176  1.4819  1.4747  1.4526  1.4348  1.4173  1.3455  1.3305  1.3237  1.3116  1.2910  1.2889  1.2807  1.2774  1.2727  1.2625
[17 : 32]:	1.2571  1.2365  1.2365  1.2338  1.2334  1.2125  1.2124  1.1948  1.1823  1.1759  1.1373  1.1329  1.1274  1.1176  1.1168  1.1168
[33 : 48]:	1.1018  1.0637  1.0560  1.0424  1.0286  1.0280  1.0170  1.0142  1.0068  1.0035  0.9875  0.9760  0.9760  0.9753  0.9705  0.9692
[49 : 64]:	0.9672  0.9669  0.9666  0.9657  0.9650  0.9650  0.9630  0.9618  0.9600  0.9587  0.9573  0.9558  0.9556  0.9545  0.9512  0.9466
2024-03-22 04:59:48 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 04:59:48 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #257: GFLOPs: 1781.5051. Time: 57.7381 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #258: GFLOPs: 1588.1525. Time: 64.7676 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #259: GFLOPs: 1585.4784. Time: 64.8768 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #260: GFLOPs: 1587.7028. Time: 64.7859 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #261: GFLOPs: 1600.9276. Time: 64.2508 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #262: GFLOPs: 1336.9835. Time: 76.9350 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #263: GFLOPs: 1759.6628. Time: 58.4548 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #264: GFLOPs: 1353.0400. Time: 76.0220 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #265: GFLOPs: 1612.8790. Time: 63.7747 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #266: GFLOPs: 1536.8712. Time: 66.9287 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #267: GFLOPs: 2491.3425. Time: 41.2873 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #268: GFLOPs: 1660.8422. Time: 61.9329 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #269: GFLOPs: 2214.0308. Time: 46.4586 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #270: GFLOPs: 3719.5553. Time: 27.6541 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #271: GFLOPs: 2505.9993. Time: 41.0458 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #272: GFLOPs: 2637.2930. Time: 39.0024 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #273: GFLOPs: 2339.7475. Time: 43.9624 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #274: GFLOPs: 1934.1901. Time: 53.1803 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #275: GFLOPs: 1933.9764. Time: 53.1862 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #276: GFLOPs: 2577.4820. Time: 39.9075 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #277: GFLOPs: 3066.7664. Time: 33.5405 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #278: GFLOPs: 1952.4253. Time: 52.6836 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #279: GFLOPs: 1964.1208. Time: 52.3699 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #280: GFLOPs: 373.3080. Time: 275.5387 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #281: GFLOPs: 2485.8974. Time: 41.3777 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #282: GFLOPs: 859.5950. Time: 119.6619 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #283: GFLOPs: 2381.6263. Time: 43.1893 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #284: GFLOPs: 1453.9791. Time: 70.7443 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #285: GFLOPs: 2350.8663. Time: 43.7544 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #286: GFLOPs: 3955.6171. Time: 26.0037 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #287: GFLOPs: 3778.2855. Time: 27.2242 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #288: GFLOPs: 3809.7605. Time: 26.9993 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #289: GFLOPs: 3722.2949. Time: 27.6337 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #290: GFLOPs: 3091.1941. Time: 33.2754 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #291: GFLOPs: 2501.6452. Time: 41.1173 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #292: GFLOPs: 2854.7152. Time: 36.0319 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #293: GFLOPs: 2390.4700. Time: 43.0295 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #294: GFLOPs: 1654.3098. Time: 62.1775 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #295: GFLOPs: 2505.3007. Time: 41.0573 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #296: GFLOPs: 1654.7569. Time: 62.1607 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #297: GFLOPs: 2380.4090. Time: 43.2114 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #298: GFLOPs: 1606.4648. Time: 64.0293 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #299: GFLOPs: 4150.7274. Time: 24.7814 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #300: GFLOPs: 4096.7405. Time: 25.1080 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #301: GFLOPs: 4100.3262. Time: 25.0860 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #302: GFLOPs: 3808.4694. Time: 27.0084 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #303: GFLOPs: 3332.9781. Time: 30.8615 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #304: GFLOPs: 4087.1275. Time: 25.1670 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #305: GFLOPs: 953.8119. Time: 107.8418 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #306: GFLOPs: 3741.6713. Time: 27.4906 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #307: GFLOPs: 1007.6814. Time: 102.0767 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #308: GFLOPs: 3572.9574. Time: 28.7887 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #309: GFLOPs: 4159.1316. Time: 24.7313 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #310: GFLOPs: 4159.4022. Time: 24.7297 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #311: GFLOPs: 1097.9159. Time: 93.6873 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #312: GFLOPs: 3758.5825. Time: 27.3669 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #313: GFLOPs: 3900.4729. Time: 26.3714 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #314: GFLOPs: 2265.4648. Time: 45.4038 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #315: GFLOPs: 1082.2326. Time: 95.0450 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #316: GFLOPs: 2363.0770. Time: 43.5283 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #317: GFLOPs: 1033.8226. Time: 99.4956 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #318: GFLOPs: 403.4345. Time: 254.9628 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #319: GFLOPs: 565.7759. Time: 181.8048 us. Best GFLOPs: 4274.4256
2024-03-22 05:00:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #320: GFLOPs: 24.2574. Time: 4240.3892 us. Best GFLOPs: 4274.4256
2024-03-22 05:04:08 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 05:04:08 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 05:04:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 389 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:04:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 782 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:04:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1168 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:04:10 [INFO] [evolutionary_search.cc:723] Sampled 62 candidate(s)
2024-03-22 05:04:12 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:04:14 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:04:16 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:04:18 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:04:19 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1040  1.1040  1.1020  1.0938  1.0923  1.0890  1.0890  1.0794  1.0792  1.0707  1.0697  1.0654  1.0559  1.0509  1.0497  1.0492
[17 : 32]:	1.0472  1.0447  1.0447  1.0397  1.0329  1.0188  1.0159  1.0145  1.0111  1.0095  1.0045  0.9933  0.9929  0.9928  0.9895  0.9726
[33 : 48]:	0.9714  0.9701  0.9662  0.9651  0.9620  0.9620  0.9618  0.9618  0.9618  0.9602  0.9550  0.9549  0.9547  0.9539  0.9535  0.9520
[49 : 64]:	0.9503  0.9491  0.9491  0.9491  0.9477  0.9465  0.9465  0.9462  0.9462  0.9462  0.9439  0.9439  0.9438  0.9436  0.9436  0.9429
2024-03-22 05:04:19 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 05:04:19 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #321: GFLOPs: 3161.1501. Time: 32.5390 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #322: GFLOPs: 4043.1278. Time: 25.4409 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #323: GFLOPs: 3679.6972. Time: 27.9536 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #324: GFLOPs: 3590.7851. Time: 28.6458 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #325: GFLOPs: 3581.7307. Time: 28.7182 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #326: GFLOPs: 3531.5179. Time: 29.1265 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #327: GFLOPs: 3928.3008. Time: 26.1846 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #328: GFLOPs: 3567.6489. Time: 28.8315 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #329: GFLOPs: 3085.0967. Time: 33.3412 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #330: GFLOPs: 3567.5143. Time: 28.8326 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #331: GFLOPs: 3147.9794. Time: 32.6752 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #332: GFLOPs: 3427.8259. Time: 30.0076 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #333: GFLOPs: 3465.6058. Time: 29.6805 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #334: GFLOPs: 3621.5181. Time: 28.4027 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #335: GFLOPs: 3843.8753. Time: 26.7597 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #336: GFLOPs: 3200.8309. Time: 32.1357 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #337: GFLOPs: 3236.2394. Time: 31.7841 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #338: GFLOPs: 3319.7232. Time: 30.9848 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #339: GFLOPs: 3322.6814. Time: 30.9572 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #340: GFLOPs: 3371.6131. Time: 30.5079 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #341: GFLOPs: 3509.7873. Time: 29.3068 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #342: GFLOPs: 3067.9025. Time: 33.5281 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #343: GFLOPs: 3209.7641. Time: 32.0462 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #344: GFLOPs: 3228.3041. Time: 31.8622 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #345: GFLOPs: 3496.2268. Time: 29.4205 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #346: GFLOPs: 3260.7701. Time: 31.5449 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #347: GFLOPs: 3489.8042. Time: 29.4747 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #348: GFLOPs: 3918.2858. Time: 26.2515 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #349: GFLOPs: 3239.3279. Time: 31.7537 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #350: GFLOPs: 3016.6523. Time: 34.0977 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #351: GFLOPs: 3328.9917. Time: 30.8985 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #352: GFLOPs: 2979.2372. Time: 34.5259 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #353: GFLOPs: 3214.9420. Time: 31.9946 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #354: GFLOPs: 2961.8399. Time: 34.7287 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #355: GFLOPs: 4074.1219. Time: 25.2474 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #356: GFLOPs: 4034.0087. Time: 25.4984 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #357: GFLOPs: 4129.0617. Time: 24.9114 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #358: GFLOPs: 4129.0188. Time: 24.9117 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #359: GFLOPs: 4020.0370. Time: 25.5870 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #360: GFLOPs: 4019.9295. Time: 25.5877 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #361: GFLOPs: 4019.9063. Time: 25.5879 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #362: GFLOPs: 3215.1315. Time: 31.9927 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #363: GFLOPs: 4029.5628. Time: 25.5265 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #364: GFLOPs: 4124.3108. Time: 24.9401 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #365: GFLOPs: 4133.3941. Time: 24.8853 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #366: GFLOPs: 1364.7624. Time: 75.3690 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #367: GFLOPs: 4100.5166. Time: 25.0848 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #368: GFLOPs: 4267.5820. Time: 24.1028 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #369: GFLOPs: 3132.1854. Time: 32.8399 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #370: GFLOPs: 4059.9001. Time: 25.3358 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #371: GFLOPs: 4060.1648. Time: 25.3341 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #372: GFLOPs: 4060.2114. Time: 25.3339 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #373: GFLOPs: 3084.2224. Time: 33.3506 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #374: GFLOPs: 4145.6508. Time: 24.8117 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #375: GFLOPs: 4145.9206. Time: 24.8101 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #376: GFLOPs: 4064.0883. Time: 25.3097 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #377: GFLOPs: 4064.2408. Time: 25.3087 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #378: GFLOPs: 4063.7395. Time: 25.3119 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #379: GFLOPs: 3199.0363. Time: 32.1537 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #380: GFLOPs: 4232.3978. Time: 24.3032 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #381: GFLOPs: 4113.3784. Time: 25.0064 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #382: GFLOPs: 276.4348. Time: 372.0979 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #383: GFLOPs: 24.1049. Time: 4267.2186 us. Best GFLOPs: 4274.4256
2024-03-22 05:05:10 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #384: GFLOPs: 5.4455. Time: 18889.0405 us. Best GFLOPs: 4274.4256
2024-03-22 05:09:11 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 05:09:11 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 05:09:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 382 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:09:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 765 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:09:12 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-03-22 05:09:14 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 132 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:09:16 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 141 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:09:18 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:09:20 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:09:21 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.4581  1.4312  1.3405  1.3364  1.2798  1.2519  1.2469  1.2432  1.2432  1.2381  1.2371  1.2171  1.2149  1.2112  1.2111  1.2110
[17 : 32]:	1.2080  1.1997  1.1967  1.1929  1.1923  1.1860  1.1844  1.1818  1.1766  1.1753  1.1711  1.1647  1.1637  1.1597  1.1589  1.1548
[33 : 48]:	1.1540  1.1535  1.1531  1.1530  1.1490  1.1461  1.1455  1.1372  1.1369  1.1352  1.1335  1.1329  1.1312  1.1295  1.1292  1.1242
[49 : 64]:	1.1221  1.1198  1.1193  1.1193  1.1187  1.1172  1.1170  1.1166  1.1163  1.1133  1.1077  1.1059  1.0995  1.0990  1.0969  1.0969
2024-03-22 05:09:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 05:09:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #385: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(64) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 64, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #386: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(64) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 64, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #387: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1024))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(64) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 64, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #388: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1024))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(64) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 64, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #389: GFLOPs: 3932.0850. Time: 26.1594 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #390: GFLOPs: 3827.0008. Time: 26.8777 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #391: GFLOPs: 2202.5751. Time: 46.7002 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #392: GFLOPs: 2110.1603. Time: 48.7455 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #393: GFLOPs: 2095.2546. Time: 49.0923 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #394: GFLOPs: 2676.2656. Time: 38.4345 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #395: GFLOPs: 2676.1785. Time: 38.4357 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #396: GFLOPs: 2729.9457. Time: 37.6787 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #397: GFLOPs: 3108.0193. Time: 33.0953 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #398: GFLOPs: 3151.0700. Time: 32.6431 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #399: GFLOPs: 2046.7889. Time: 50.2547 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #400: GFLOPs: 1883.3121. Time: 54.6170 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #401: GFLOPs: 2232.0522. Time: 46.0835 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #402: GFLOPs: 3110.9230. Time: 33.0644 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #403: GFLOPs: 3107.0658. Time: 33.1054 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #404: GFLOPs: 3877.6368. Time: 26.5267 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #405: GFLOPs: 3772.7692. Time: 27.2640 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #406: GFLOPs: 2639.0849. Time: 38.9759 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #407: GFLOPs: 3199.0944. Time: 32.1531 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #408: GFLOPs: 3183.8209. Time: 32.3073 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #409: GFLOPs: 2873.7603. Time: 35.7931 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #410: GFLOPs: 2363.1940. Time: 43.5262 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #411: GFLOPs: 3873.1398. Time: 26.5575 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #412: GFLOPs: 4013.1500. Time: 25.6309 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #413: GFLOPs: 2449.7619. Time: 41.9881 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #414: GFLOPs: 2677.6030. Time: 38.4153 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #415: GFLOPs: 3990.1127. Time: 25.7789 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #416: GFLOPs: 3794.8182. Time: 27.1056 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #417: GFLOPs: 4241.8639. Time: 24.2490 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #418: GFLOPs: 2492.0457. Time: 41.2756 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #419: GFLOPs: 2946.4666. Time: 34.9099 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #420: GFLOPs: 3918.5888. Time: 26.2494 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #421: GFLOPs: 2803.7535. Time: 36.6868 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #422: GFLOPs: 2781.0959. Time: 36.9857 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #423: GFLOPs: 2589.2432. Time: 39.7262 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #424: GFLOPs: 3156.8400. Time: 32.5835 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #425: GFLOPs: 2960.9119. Time: 34.7396 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #426: GFLOPs: 2891.7874. Time: 35.5700 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #427: GFLOPs: 2387.3679. Time: 43.0854 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #428: GFLOPs: 4034.6696. Time: 25.4942 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #429: GFLOPs: 2764.4436. Time: 37.2085 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #430: GFLOPs: 4023.0843. Time: 25.5676 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #431: GFLOPs: 4037.0968. Time: 25.4789 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #432: GFLOPs: 2889.8659. Time: 35.5936 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #433: GFLOPs: 4141.7615. Time: 24.8350 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #434: GFLOPs: 2883.7793. Time: 35.6687 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #435: GFLOPs: 2460.9854. Time: 41.7966 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #436: GFLOPs: 2890.8842. Time: 35.5811 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #437: GFLOPs: 3876.7336. Time: 26.5329 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #438: GFLOPs: 3660.4483. Time: 28.1006 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #439: GFLOPs: 2894.0003. Time: 35.5428 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #440: GFLOPs: 2723.5925. Time: 37.7666 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #441: GFLOPs: 3876.1812. Time: 26.5366 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #442: GFLOPs: 2906.3638. Time: 35.3916 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #443: GFLOPs: 2934.8948. Time: 35.0475 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #444: GFLOPs: 1990.2354. Time: 51.6827 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #445: GFLOPs: 2891.3845. Time: 35.5749 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #446: GFLOPs: 593.4394. Time: 173.3299 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #447: GFLOPs: 8.8504. Time: 11622.1474 us. Best GFLOPs: 4274.4256
2024-03-22 05:10:08 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #448: GFLOPs: 35.9384. Time: 2862.1386 us. Best GFLOPs: 4274.4256
2024-03-22 05:18:53 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 05:18:53 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 05:18:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 385 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:18:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 768 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:18:54 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-22 05:18:56 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 148 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:18:58 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:19:00 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 161 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:19:03 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 154 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:19:04 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3518  1.2164  1.1782  1.1570  1.1484  1.1405  1.1379  1.1370  1.1368  1.1329  1.1195  1.1126  1.0985  1.0942  1.0902  1.0817
[17 : 32]:	1.0684  1.0628  1.0418  1.0335  0.9995  0.9950  0.9851  0.9831  0.9815  0.9806  0.9800  0.9791  0.9786  0.9785  0.9785  0.9785
[33 : 48]:	0.9778  0.9775  0.9773  0.9754  0.9749  0.9747  0.9736  0.9732  0.9729  0.9725  0.9708  0.9704  0.9677  0.9665  0.9665  0.9649
[49 : 64]:	0.9646  0.9643  0.9643  0.9640  0.9627  0.9624  0.9624  0.9622  0.9608  0.9590  0.9575  0.9575  0.9565  0.9561  0.9560  0.9555
2024-03-22 05:19:04 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 05:19:04 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #449: GFLOPs: 1880.3664. Time: 54.7025 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #450: GFLOPs: 1980.3726. Time: 51.9401 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #451: GFLOPs: 3021.4265. Time: 34.0438 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #452: GFLOPs: 3369.8384. Time: 30.5240 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #453: GFLOPs: 2824.6104. Time: 36.4159 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #454: GFLOPs: 3181.2150. Time: 32.3338 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #455: GFLOPs: 3212.6486. Time: 32.0174 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #456: GFLOPs: 2648.4423. Time: 38.8382 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #457: GFLOPs: 4130.0988. Time: 24.9052 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #458: GFLOPs: 2814.8597. Time: 36.5421 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #459: GFLOPs: 2710.7947. Time: 37.9449 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #460: GFLOPs: 3064.5013. Time: 33.5653 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #461: GFLOPs: 2873.0838. Time: 35.8015 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #462: GFLOPs: 2867.1200. Time: 35.8760 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #463: GFLOPs: 2644.6940. Time: 38.8933 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #464: GFLOPs: 2607.7391. Time: 39.4444 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #465: GFLOPs: 3077.5218. Time: 33.4233 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #466: GFLOPs: 3877.2012. Time: 26.5297 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #467: GFLOPs: 3330.0421. Time: 30.8887 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #468: GFLOPs: 3778.1696. Time: 27.2250 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #469: GFLOPs: 4068.2818. Time: 25.2836 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #470: GFLOPs: 3704.7961. Time: 27.7642 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #471: GFLOPs: 3224.0676. Time: 31.9040 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #472: GFLOPs: 3817.5422. Time: 26.9442 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #473: GFLOPs: 3731.7875. Time: 27.5634 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #474: GFLOPs: 3939.1364. Time: 26.1125 us. Best GFLOPs: 4274.4256
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #475: GFLOPs: 4318.9331. Time: 23.8163 us. Best GFLOPs: 4318.9331
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #476: GFLOPs: 4051.7328. Time: 25.3869 us. Best GFLOPs: 4318.9331
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #477: GFLOPs: 734.7574. Time: 139.9929 us. Best GFLOPs: 4318.9331
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #478: GFLOPs: 4125.4932. Time: 24.9330 us. Best GFLOPs: 4318.9331
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #479: GFLOPs: 4045.8638. Time: 25.4237 us. Best GFLOPs: 4318.9331
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #480: GFLOPs: 3461.2598. Time: 29.7177 us. Best GFLOPs: 4318.9331
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #481: GFLOPs: 4354.0337. Time: 23.6243 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #482: GFLOPs: 4128.3481. Time: 24.9157 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #483: GFLOPs: 2994.7299. Time: 34.3473 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #484: GFLOPs: 4079.2012. Time: 25.2159 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #485: GFLOPs: 3018.8385. Time: 34.0730 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #486: GFLOPs: 4173.8575. Time: 24.6441 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #487: GFLOPs: 4132.7562. Time: 24.8892 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #488: GFLOPs: 4250.7125. Time: 24.1985 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #489: GFLOPs: 4173.4203. Time: 24.6466 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #490: GFLOPs: 2846.2561. Time: 36.1390 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #491: GFLOPs: 4219.8332. Time: 24.3756 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #492: GFLOPs: 4040.6871. Time: 25.4563 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #493: GFLOPs: 3816.9640. Time: 26.9483 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #494: GFLOPs: 3927.4472. Time: 26.1902 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #495: GFLOPs: 3927.4520. Time: 26.1902 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #496: GFLOPs: 4070.1544. Time: 25.2720 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #497: GFLOPs: 3921.9745. Time: 26.2268 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #498: GFLOPs: 4088.7328. Time: 25.1571 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #499: GFLOPs: 4235.4929. Time: 24.2854 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #500: GFLOPs: 3513.7956. Time: 29.2734 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #501: GFLOPs: 4143.3512. Time: 24.8255 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #502: GFLOPs: 3057.3181. Time: 33.6441 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #503: GFLOPs: 3904.6245. Time: 26.3433 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #504: GFLOPs: 4278.9450. Time: 24.0388 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #505: GFLOPs: 4126.3388. Time: 24.9279 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #506: GFLOPs: 3395.8463. Time: 30.2902 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #507: GFLOPs: 4125.6380. Time: 24.9321 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #508: GFLOPs: 3048.1921. Time: 33.7449 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #509: GFLOPs: 1999.7385. Time: 51.4371 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #510: GFLOPs: 296.6685. Time: 346.7197 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #511: GFLOPs: 31.9323. Time: 3221.2159 us. Best GFLOPs: 4354.0337
2024-03-22 05:19:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #512: GFLOPs: 2718.0464. Time: 37.8437 us. Best GFLOPs: 4354.0337
2024-03-22 05:23:43 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 05:23:43 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 05:23:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 387 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:23:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 779 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:23:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1178 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:23:45 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-22 05:23:46 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:23:49 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 132 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:23:51 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:23:53 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:23:54 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2942  1.2015  1.2010  1.0957  1.0950  1.0950  1.0642  1.0637  1.0569  1.0393  1.0390  1.0319  1.0310  1.0088  1.0040  1.0009
[17 : 32]:	0.9989  0.9983  0.9856  0.9792  0.9724  0.9679  0.9676  0.9631  0.9624  0.9584  0.9562  0.9556  0.9546  0.9544  0.9543  0.9543
[33 : 48]:	0.9543  0.9526  0.9526  0.9525  0.9520  0.9520  0.9520  0.9508  0.9507  0.9502  0.9502  0.9502  0.9501  0.9499  0.9488  0.9482
[49 : 64]:	0.9482  0.9476  0.9476  0.9474  0.9474  0.9468  0.9466  0.9465  0.9465  0.9464  0.9458  0.9453  0.9450  0.9444  0.9442  0.9435
2024-03-22 05:23:54 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 05:23:54 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #513: GFLOPs: 1751.0338. Time: 58.7429 us. Best GFLOPs: 4354.0337
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #514: GFLOPs: 3875.9437. Time: 26.5383 us. Best GFLOPs: 4354.0337
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #515: GFLOPs: 3865.5426. Time: 26.6097 us. Best GFLOPs: 4354.0337
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #516: GFLOPs: 3256.6401. Time: 31.5849 us. Best GFLOPs: 4354.0337
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #517: GFLOPs: 4259.6858. Time: 24.1475 us. Best GFLOPs: 4354.0337
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #518: GFLOPs: 4258.3127. Time: 24.1553 us. Best GFLOPs: 4354.0337
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #519: GFLOPs: 4155.8638. Time: 24.7508 us. Best GFLOPs: 4354.0337
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #520: GFLOPs: 3320.1892. Time: 30.9804 us. Best GFLOPs: 4354.0337
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #521: GFLOPs: 4373.5698. Time: 23.5187 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #522: GFLOPs: 3600.9768. Time: 28.5647 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #523: GFLOPs: 4257.7697. Time: 24.1584 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #524: GFLOPs: 3329.2092. Time: 30.8965 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #525: GFLOPs: 4237.5287. Time: 24.2738 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #526: GFLOPs: 4200.0068. Time: 24.4906 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #527: GFLOPs: 2130.7438. Time: 48.2746 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #528: GFLOPs: 4361.4880. Time: 23.5839 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #529: GFLOPs: 4103.9688. Time: 25.0637 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #530: GFLOPs: 4200.7261. Time: 24.4864 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #531: GFLOPs: 4039.8397. Time: 25.4616 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #532: GFLOPs: 4039.4441. Time: 25.4641 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #533: GFLOPs: 3580.0376. Time: 28.7318 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #534: GFLOPs: 3767.8264. Time: 27.2998 us. Best GFLOPs: 4373.5698
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #535: GFLOPs: 4733.7614. Time: 21.7292 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #536: GFLOPs: 3798.6104. Time: 27.0785 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #537: GFLOPs: 3961.3537. Time: 25.9661 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #538: GFLOPs: 4101.9040. Time: 25.0764 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #539: GFLOPs: 3382.9087. Time: 30.4060 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #540: GFLOPs: 3835.2051. Time: 26.8202 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #541: GFLOPs: 4131.6317. Time: 24.8959 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #542: GFLOPs: 4088.3697. Time: 25.1594 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #543: GFLOPs: 4143.1788. Time: 24.8265 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #544: GFLOPs: 4143.5079. Time: 24.8246 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #545: GFLOPs: 4143.2984. Time: 24.8258 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #546: GFLOPs: 4143.2061. Time: 24.8264 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #547: GFLOPs: 4142.8502. Time: 24.8285 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #548: GFLOPs: 4467.7306. Time: 23.0231 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #549: GFLOPs: 4029.0038. Time: 25.5301 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #550: GFLOPs: 4029.4537. Time: 25.5272 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #551: GFLOPs: 4029.1634. Time: 25.5291 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #552: GFLOPs: 4137.8437. Time: 24.8586 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #553: GFLOPs: 4392.7113. Time: 23.4162 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #554: GFLOPs: 4066.7635. Time: 25.2930 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #555: GFLOPs: 4067.1323. Time: 25.2907 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #556: GFLOPs: 4067.3212. Time: 25.2896 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #557: GFLOPs: 4125.2813. Time: 24.9343 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #558: GFLOPs: 4143.1788. Time: 24.8265 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #559: GFLOPs: 4233.9070. Time: 24.2945 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #560: GFLOPs: 4143.7668. Time: 24.8230 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #561: GFLOPs: 4143.3160. Time: 24.8257 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #562: GFLOPs: 3841.7921. Time: 26.7742 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #563: GFLOPs: 4004.5743. Time: 25.6858 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #564: GFLOPs: 4044.6984. Time: 25.4310 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #565: GFLOPs: 4702.7043. Time: 21.8727 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #566: GFLOPs: 4617.2628. Time: 22.2774 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #567: GFLOPs: 4240.9902. Time: 24.2540 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #568: GFLOPs: 4116.5815. Time: 24.9869 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #569: GFLOPs: 4116.1017. Time: 24.9899 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #570: GFLOPs: 3295.2942. Time: 31.2145 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #571: GFLOPs: 4044.9395. Time: 25.4295 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #572: GFLOPs: 4062.0253. Time: 25.3225 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #573: GFLOPs: 4089.4253. Time: 25.1529 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #574: GFLOPs: 187.9350. Time: 547.3210 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #575: GFLOPs: 588.6300. Time: 174.7461 us. Best GFLOPs: 4733.7614
2024-03-22 05:24:44 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #576: GFLOPs: 593.6129. Time: 173.2792 us. Best GFLOPs: 4733.7614
2024-03-22 05:28:22 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 05:28:22 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 05:28:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 390 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:28:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 756 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:28:23 [INFO] [evolutionary_search.cc:723] Sampled 64 candidate(s)
2024-03-22 05:28:25 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 134 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:28:27 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 131 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:28:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:28:31 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:28:32 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0841  0.9819  0.9676  0.9501  0.9501  0.9498  0.9417  0.9327  0.9174  0.9163  0.9162  0.9153  0.9149  0.9130  0.9118  0.9088
[17 : 32]:	0.9079  0.9070  0.9009  0.9007  0.8999  0.8999  0.8985  0.8985  0.8979  0.8977  0.8975  0.8965  0.8951  0.8949  0.8947  0.8933
[33 : 48]:	0.8853  0.8820  0.8807  0.8799  0.8776  0.8776  0.8775  0.8771  0.8768  0.8768  0.8768  0.8762  0.8762  0.8761  0.8760  0.8757
[49 : 64]:	0.8754  0.8735  0.8728  0.8728  0.8727  0.8723  0.8720  0.8720  0.8711  0.8711  0.8710  0.8706  0.8692  0.8675  0.8675  0.8675
2024-03-22 05:28:33 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 05:28:33 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #577: GFLOPs: 3067.5637. Time: 33.5318 us. Best GFLOPs: 4733.7614
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #578: GFLOPs: 4812.9750. Time: 21.3716 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #579: GFLOPs: 4665.4699. Time: 22.0473 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #580: GFLOPs: 4767.7801. Time: 21.5741 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #581: GFLOPs: 4492.0108. Time: 22.8986 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #582: GFLOPs: 4550.0143. Time: 22.6067 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #583: GFLOPs: 4189.1462. Time: 24.5541 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #584: GFLOPs: 4786.6727. Time: 21.4890 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #585: GFLOPs: 4639.6789. Time: 22.1698 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #586: GFLOPs: 4674.4348. Time: 22.0050 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #587: GFLOPs: 3179.0545. Time: 32.3558 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #588: GFLOPs: 4674.2920. Time: 22.0056 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #589: GFLOPs: 4668.5818. Time: 22.0326 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #590: GFLOPs: 4496.6292. Time: 22.8751 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #591: GFLOPs: 3659.9763. Time: 28.1042 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #592: GFLOPs: 3720.2462. Time: 27.6489 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #593: GFLOPs: 3659.3098. Time: 28.1093 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #594: GFLOPs: 3688.5025. Time: 27.8869 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #595: GFLOPs: 4496.8578. Time: 22.8739 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #596: GFLOPs: 4118.1624. Time: 24.9774 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #597: GFLOPs: 4471.0959. Time: 23.0057 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #598: GFLOPs: 4471.4003. Time: 23.0042 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #599: GFLOPs: 4132.4687. Time: 24.8909 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #600: GFLOPs: 4358.9777. Time: 23.5975 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #601: GFLOPs: 3641.3638. Time: 28.2479 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #602: GFLOPs: 4206.7411. Time: 24.4514 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #603: GFLOPs: 4245.8679. Time: 24.2261 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #604: GFLOPs: 4355.5143. Time: 23.6162 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #605: GFLOPs: 4429.2860. Time: 23.2229 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #606: GFLOPs: 3701.1382. Time: 27.7917 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #607: GFLOPs: 4019.1375. Time: 25.5928 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #608: GFLOPs: 3205.8929. Time: 32.0849 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #609: GFLOPs: 4166.6493. Time: 24.6867 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #610: GFLOPs: 4358.3165. Time: 23.6010 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #611: GFLOPs: 4254.3456. Time: 24.1778 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #612: GFLOPs: 2811.8217. Time: 36.5816 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #613: GFLOPs: 4152.3044. Time: 24.7720 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #614: GFLOPs: 4152.4348. Time: 24.7712 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #615: GFLOPs: 4153.2864. Time: 24.7661 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #616: GFLOPs: 3998.7742. Time: 25.7231 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #617: GFLOPs: 4165.2636. Time: 24.6949 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #618: GFLOPs: 4151.7176. Time: 24.7755 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #619: GFLOPs: 4199.4902. Time: 24.4936 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #620: GFLOPs: 4152.5963. Time: 24.7702 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #621: GFLOPs: 4152.5398. Time: 24.7706 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #622: GFLOPs: 4188.3206. Time: 24.5590 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #623: GFLOPs: 4179.7687. Time: 24.6092 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #624: GFLOPs: 4302.4920. Time: 23.9073 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #625: GFLOPs: 4152.2679. Time: 24.7722 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #626: GFLOPs: 4165.1043. Time: 24.6959 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #627: GFLOPs: 4152.4881. Time: 24.7709 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #628: GFLOPs: 4152.4307. Time: 24.7712 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #629: GFLOPs: 4187.8809. Time: 24.5615 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #630: GFLOPs: 4310.5264. Time: 23.8627 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #631: GFLOPs: 4102.8325. Time: 25.0707 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #632: GFLOPs: 4139.9057. Time: 24.8462 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #633: GFLOPs: 4345.5868. Time: 23.6702 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #634: GFLOPs: 4320.9777. Time: 23.8050 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #635: GFLOPs: 4138.9362. Time: 24.8520 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #636: GFLOPs: 4232.5117. Time: 24.3025 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #637: GFLOPs: 4204.6806. Time: 24.4634 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #638: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(7)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(7) + yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + xx_3_init * T.int64(7) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(196))
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(7)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(7) + yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + xx_3 * T.int64(7) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(7), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #639: GFLOPs: 1051.1941. Time: 97.8514 us. Best GFLOPs: 4812.9750
2024-03-22 05:29:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #640: GFLOPs: 871.0801. Time: 118.0842 us. Best GFLOPs: 4812.9750
2024-03-22 05:35:10 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 05:35:10 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 05:35:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 377 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:35:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 759 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:35:11 [INFO] [evolutionary_search.cc:723] Sampled 61 candidate(s)
2024-03-22 05:35:13 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:35:15 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:35:17 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:35:19 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 151 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:35:21 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3308  1.3263  1.3236  1.3112  1.3085  1.3079  1.3039  1.2785  1.2695  1.2674  1.2593  1.2587  1.2544  1.2480  1.2378  1.2219
[17 : 32]:	1.2198  1.1971  1.1800  1.1749  1.1605  1.1560  1.1432  1.1409  1.1409  1.1391  1.1376  1.1275  1.1271  1.1249  1.1094  1.0892
[33 : 48]:	1.0840  1.0742  1.0661  1.0632  1.0628  1.0547  1.0512  1.0391  1.0324  1.0096  1.0092  1.0092  1.0082  0.9899  0.9861  0.9771
[49 : 64]:	0.9732  0.9729  0.9719  0.9715  0.9712  0.9712  0.9631  0.9589  0.9586  0.9583  0.9572  0.9569  0.9569  0.9568  0.9560  0.9517
2024-03-22 05:35:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 05:35:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #641: GFLOPs: 4996.9712. Time: 20.5846 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #642: GFLOPs: 4962.9445. Time: 20.7258 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #643: GFLOPs: 4713.8882. Time: 21.8208 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #644: GFLOPs: 4897.4212. Time: 21.0031 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #645: GFLOPs: 4245.9996. Time: 24.2253 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #646: GFLOPs: 4800.3931. Time: 21.4276 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #647: GFLOPs: 4236.9577. Time: 24.2770 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #648: GFLOPs: 4341.3481. Time: 23.6933 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #649: GFLOPs: 4940.8996. Time: 20.8182 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #650: GFLOPs: 4056.0169. Time: 25.3601 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #651: GFLOPs: 4109.8643. Time: 25.0278 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #652: GFLOPs: 4362.1052. Time: 23.5805 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #653: GFLOPs: 4866.8449. Time: 21.1350 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #654: GFLOPs: 3574.2540. Time: 28.7783 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #655: GFLOPs: 2999.6761. Time: 34.2906 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #656: GFLOPs: 2564.5127. Time: 40.1093 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #657: GFLOPs: 4214.5532. Time: 24.4061 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #658: GFLOPs: 4251.3008. Time: 24.1951 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #659: GFLOPs: 3729.1272. Time: 27.5831 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #660: GFLOPs: 3783.7782. Time: 27.1847 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #661: GFLOPs: 3076.7764. Time: 33.4314 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #662: GFLOPs: 4304.7312. Time: 23.8948 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #663: GFLOPs: 3098.7349. Time: 33.1944 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #664: GFLOPs: 4305.5760. Time: 23.8901 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #665: GFLOPs: 4208.2185. Time: 24.4428 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #666: GFLOPs: 3195.4427. Time: 32.1898 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #667: GFLOPs: 3111.5738. Time: 33.0575 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #668: GFLOPs: 2946.8151. Time: 34.9058 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #669: GFLOPs: 4274.3689. Time: 24.0646 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #670: GFLOPs: 4247.2047. Time: 24.2185 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #671: GFLOPs: 2820.2495. Time: 36.4722 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #672: GFLOPs: 4179.5251. Time: 24.6106 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #673: GFLOPs: 3872.0454. Time: 26.5650 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #674: GFLOPs: 4314.5613. Time: 23.8404 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #675: GFLOPs: 3836.1797. Time: 26.8133 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #676: GFLOPs: 4176.7669. Time: 24.6269 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #677: GFLOPs: 3143.9226. Time: 32.7173 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #678: GFLOPs: 3534.0840. Time: 29.1054 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #679: GFLOPs: 2639.1041. Time: 38.9757 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #680: GFLOPs: 2009.0794. Time: 51.1980 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #681: GFLOPs: 3699.0261. Time: 27.8075 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #682: GFLOPs: 3823.5280. Time: 26.9021 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #683: GFLOPs: 1957.2798. Time: 52.5529 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #684: GFLOPs: 1957.2417. Time: 52.5540 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #685: GFLOPs: 4418.7101. Time: 23.2785 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #686: GFLOPs: 4420.7679. Time: 23.2676 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #687: GFLOPs: 3889.8711. Time: 26.4432 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #688: GFLOPs: 3538.9499. Time: 29.0653 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #689: GFLOPs: 4738.7445. Time: 21.7063 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #690: GFLOPs: 4732.5014. Time: 21.7350 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #691: GFLOPs: 2401.0287. Time: 42.8403 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #692: GFLOPs: 4738.3401. Time: 21.7082 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #693: GFLOPs: 4732.5723. Time: 21.7346 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #694: GFLOPs: 4732.6456. Time: 21.7343 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #695: GFLOPs: 2394.1634. Time: 42.9632 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #696: GFLOPs: 4619.7515. Time: 22.2654 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #697: GFLOPs: 4639.7275. Time: 22.1696 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #698: GFLOPs: 4620.6018. Time: 22.2613 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #699: GFLOPs: 4620.7857. Time: 22.2605 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #700: GFLOPs: 4639.8102. Time: 22.1692 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #701: GFLOPs: 4640.4251. Time: 22.1662 us. Best GFLOPs: 4996.9712
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #702: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(112), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(1024), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused)
                                    v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(98))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 16, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108 = sch.split(loop=l106, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113 = sch.get_loops(block=b87)
l114, l115, l116 = sch.split(loop=l113, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l116)
sch.bind(loop=l115, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b118)
l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #703: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(14)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), xx_3_init * T.int64(14) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(196))
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(14)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), xx_3 * T.int64(14) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(14)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 05:36:13 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #704: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(7)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(16) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + xx_3_init * T.int64(7) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(196))
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(7)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(16) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + xx_3 * T.int64(7) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(2), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 4, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 05:44:46 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 05:44:46 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 05:44:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 394 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:44:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 783 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:44:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1177 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:44:48 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-03-22 05:44:50 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 154 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:44:52 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 135 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:44:54 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:44:56 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:44:58 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9779  0.9748  0.9719  0.9712  0.9712  0.9707  0.9666  0.9610  0.9594  0.9577  0.9565  0.9553  0.9529  0.9529  0.9529  0.9512
[17 : 32]:	0.9482  0.9437  0.9418  0.9380  0.9366  0.9361  0.9350  0.9349  0.9338  0.9333  0.9312  0.9299  0.9299  0.9288  0.9284  0.9284
[33 : 48]:	0.9278  0.9268  0.9252  0.9247  0.9247  0.9247  0.9242  0.9228  0.9228  0.9213  0.9212  0.9200  0.9196  0.9151  0.9114  0.9113
[49 : 64]:	0.9109  0.9109  0.9109  0.9109  0.9102  0.9098  0.9087  0.9083  0.9083  0.9082  0.9074  0.9071  0.9071  0.9071  0.9071  0.9061
2024-03-22 05:44:58 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 05:44:58 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #705: GFLOPs: 4166.8454. Time: 24.6855 us. Best GFLOPs: 4996.9712
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #706: GFLOPs: 5019.4167. Time: 20.4926 us. Best GFLOPs: 5019.4167
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #707: GFLOPs: 4959.0380. Time: 20.7421 us. Best GFLOPs: 5019.4167
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #708: GFLOPs: 5019.9652. Time: 20.4903 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #709: GFLOPs: 5013.5990. Time: 20.5164 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #710: GFLOPs: 4928.5980. Time: 20.8702 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #711: GFLOPs: 4988.3890. Time: 20.6200 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #712: GFLOPs: 4077.0227. Time: 25.2294 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #713: GFLOPs: 4901.3607. Time: 20.9862 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #714: GFLOPs: 4087.5686. Time: 25.1643 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #715: GFLOPs: 4879.3249. Time: 21.0809 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #716: GFLOPs: 4879.7801. Time: 21.0790 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #717: GFLOPs: 4879.4419. Time: 21.0804 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #718: GFLOPs: 4879.2276. Time: 21.0814 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #719: GFLOPs: 4878.8049. Time: 21.0832 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #720: GFLOPs: 4872.7091. Time: 21.1096 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #721: GFLOPs: 4879.0892. Time: 21.0820 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #722: GFLOPs: 4830.7092. Time: 21.2931 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #723: GFLOPs: 4759.1512. Time: 21.6133 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #724: GFLOPs: 4953.8104. Time: 20.7640 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #725: GFLOPs: 4738.4601. Time: 21.7076 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #726: GFLOPs: 4928.7967. Time: 20.8694 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #727: GFLOPs: 4901.5244. Time: 20.9855 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #728: GFLOPs: 3150.6166. Time: 32.6478 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #729: GFLOPs: 4650.1539. Time: 22.1199 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #730: GFLOPs: 4119.9401. Time: 24.9666 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #731: GFLOPs: 4640.6498. Time: 22.1652 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #732: GFLOPs: 4649.2466. Time: 22.1242 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #733: GFLOPs: 4649.0920. Time: 22.1249 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #734: GFLOPs: 3771.0850. Time: 27.2762 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #735: GFLOPs: 4734.1677. Time: 21.7273 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #736: GFLOPs: 4734.2092. Time: 21.7271 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #737: GFLOPs: 4584.9665. Time: 22.4344 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #738: GFLOPs: 4626.4236. Time: 22.2333 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #739: GFLOPs: 4651.9026. Time: 22.1116 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #740: GFLOPs: 4584.8410. Time: 22.4350 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #741: GFLOPs: 4584.9906. Time: 22.4342 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #742: GFLOPs: 4584.7772. Time: 22.4353 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #743: GFLOPs: 4605.6970. Time: 22.3334 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #744: GFLOPs: 4651.8718. Time: 22.1117 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #745: GFLOPs: 4652.0901. Time: 22.1107 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #746: GFLOPs: 4859.1089. Time: 21.1687 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #747: GFLOPs: 4201.8412. Time: 24.4799 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #748: GFLOPs: 4724.6314. Time: 21.7712 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #749: GFLOPs: 4872.9678. Time: 21.1085 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #750: GFLOPs: 4713.7611. Time: 21.8214 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #751: GFLOPs: 4366.9299. Time: 23.5545 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #752: GFLOPs: 4715.1512. Time: 21.8150 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #753: GFLOPs: 4444.6785. Time: 23.1425 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #754: GFLOPs: 4444.1791. Time: 23.1451 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #755: GFLOPs: 4444.4015. Time: 23.1439 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #756: GFLOPs: 4443.3415. Time: 23.1494 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #757: GFLOPs: 4489.6689. Time: 22.9106 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #758: GFLOPs: 4738.4532. Time: 21.7077 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #759: GFLOPs: 4785.7765. Time: 21.4930 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #760: GFLOPs: 4365.7771. Time: 23.5607 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #761: GFLOPs: 4365.7444. Time: 23.5609 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #762: GFLOPs: 4712.8509. Time: 21.8256 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #763: GFLOPs: 3142.9591. Time: 32.7274 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #764: GFLOPs: 4489.9477. Time: 22.9091 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #765: GFLOPs: 4490.0818. Time: 22.9084 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #766: GFLOPs: 256.8922. Time: 400.4045 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #767: GFLOPs: 793.7158. Time: 129.5940 us. Best GFLOPs: 5019.9652
2024-03-22 05:45:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #768: GFLOPs: 2082.2258. Time: 49.3994 us. Best GFLOPs: 5019.9652
2024-03-22 05:53:52 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 05:53:52 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 05:53:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 390 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:53:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 781 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:53:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:53:54 [INFO] [evolutionary_search.cc:723] Sampled 60 candidate(s)
2024-03-22 05:53:56 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:53:58 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 152 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:54:00 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:54:03 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 118 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 05:54:04 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.4356  1.4112  1.3764  1.3566  1.3463  1.3439  1.3189  1.3162  1.3136  1.3131  1.2627  1.2338  1.1096  1.0974  1.0792  1.0533
[17 : 32]:	1.0181  1.0080  1.0010  0.9931  0.9923  0.9882  0.9869  0.9857  0.9854  0.9817  0.9761  0.9761  0.9718  0.9647  0.9635  0.9625
[33 : 48]:	0.9622  0.9612  0.9575  0.9553  0.9546  0.9536  0.9506  0.9468  0.9468  0.9468  0.9453  0.9443  0.9442  0.9428  0.9409  0.9409
[49 : 64]:	0.9406  0.9363  0.9363  0.9363  0.9360  0.9342  0.9332  0.9304  0.9286  0.9286  0.9286  0.9251  0.9205  0.9205  0.9187  0.9187
2024-03-22 05:54:04 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 05:54:04 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #769: GFLOPs: 2526.7285. Time: 40.7091 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #770: GFLOPs: 2509.7806. Time: 40.9840 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #771: GFLOPs: 2509.7258. Time: 40.9849 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #772: GFLOPs: 1623.7151. Time: 63.3490 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #773: GFLOPs: 2131.7674. Time: 48.2514 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #774: GFLOPs: 2544.5967. Time: 40.4232 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #775: GFLOPs: 1638.0648. Time: 62.7941 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #776: GFLOPs: 2057.6288. Time: 49.9900 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #777: GFLOPs: 2056.9721. Time: 50.0059 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #778: GFLOPs: 1633.0911. Time: 62.9853 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #779: GFLOPs: 2131.2555. Time: 48.2630 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #780: GFLOPs: 2472.6624. Time: 41.5992 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #781: GFLOPs: 2707.0198. Time: 37.9978 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #782: GFLOPs: 2690.1611. Time: 38.2359 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #783: GFLOPs: 4012.6817. Time: 25.6339 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #784: GFLOPs: 4012.1030. Time: 25.6376 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #785: GFLOPs: 2429.2791. Time: 42.3421 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #786: GFLOPs: 2423.1704. Time: 42.4489 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #787: GFLOPs: 2427.2208. Time: 42.3780 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #788: GFLOPs: 3813.6800. Time: 26.9715 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #789: GFLOPs: 3976.8613. Time: 25.8648 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #790: GFLOPs: 2192.5991. Time: 46.9127 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #791: GFLOPs: 2060.5330. Time: 49.9195 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #792: GFLOPs: 2060.5663. Time: 49.9187 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #793: GFLOPs: 4952.9896. Time: 20.7674 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #794: GFLOPs: 4028.9507. Time: 25.5304 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #795: GFLOPs: 4978.0203. Time: 20.6630 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #796: GFLOPs: 4945.0286. Time: 20.8009 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #797: GFLOPs: 4917.6800. Time: 20.9165 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #798: GFLOPs: 4836.1245. Time: 21.2693 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #799: GFLOPs: 4944.6385. Time: 20.8025 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #800: GFLOPs: 4945.1329. Time: 20.8004 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #801: GFLOPs: 3907.0263. Time: 26.3271 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #802: GFLOPs: 1072.4452. Time: 95.9124 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #803: GFLOPs: 4865.7462. Time: 21.1398 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #804: GFLOPs: 4866.5612. Time: 21.1362 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #805: GFLOPs: 4853.5602. Time: 21.1929 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #806: GFLOPs: 4857.6499. Time: 21.1750 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #807: GFLOPs: 4854.8875. Time: 21.1871 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #808: GFLOPs: 4854.5255. Time: 21.1886 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #809: GFLOPs: 4827.4026. Time: 21.3077 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #810: GFLOPs: 4854.7929. Time: 21.1875 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #811: GFLOPs: 4862.1611. Time: 21.1554 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #812: GFLOPs: 4865.5199. Time: 21.1408 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #813: GFLOPs: 4792.2299. Time: 21.4641 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #814: GFLOPs: 3954.0903. Time: 26.0138 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #815: GFLOPs: 4686.5893. Time: 21.9479 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #816: GFLOPs: 4686.8687. Time: 21.9466 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #817: GFLOPs: 4687.2768. Time: 21.9447 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #818: GFLOPs: 4729.8825. Time: 21.7470 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #819: GFLOPs: 4729.4398. Time: 21.7490 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #820: GFLOPs: 4729.9140. Time: 21.7469 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #821: GFLOPs: 4729.4438. Time: 21.7490 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #822: GFLOPs: 4818.4447. Time: 21.3473 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #823: GFLOPs: 4822.9979. Time: 21.3272 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #824: GFLOPs: 3986.3041. Time: 25.8036 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #825: GFLOPs: 4718.2322. Time: 21.8007 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #826: GFLOPs: 4717.9503. Time: 21.8020 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #827: GFLOPs: 4709.4467. Time: 21.8414 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #828: GFLOPs: 4782.2235. Time: 21.5090 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #829: GFLOPs: 4696.4754. Time: 21.9017 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #830: GFLOPs: 296.0416. Time: 347.4539 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #831: GFLOPs: 346.1199. Time: 297.1825 us. Best GFLOPs: 5019.9652
2024-03-22 05:54:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #832: GFLOPs: 14.5770. Time: 7056.3837 us. Best GFLOPs: 5019.9652
2024-03-22 06:03:29 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 06:03:30 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 06:03:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 386 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:03:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 774 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:03:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:03:31 [INFO] [evolutionary_search.cc:723] Sampled 73 candidate(s)
2024-03-22 06:03:33 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:03:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 141 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:03:38 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 154 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:03:40 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 133 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:03:41 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9907  0.9907  0.9834  0.9800  0.9684  0.9682  0.9680  0.9647  0.9609  0.9609  0.9609  0.9609  0.9586  0.9552  0.9532  0.9456
[17 : 32]:	0.9436  0.9422  0.9327  0.9286  0.9212  0.9208  0.9208  0.9196  0.9185  0.9174  0.9174  0.9172  0.9158  0.9153  0.9147  0.9141
[33 : 48]:	0.9134  0.9103  0.9087  0.9076  0.9071  0.9046  0.9046  0.9041  0.9038  0.9037  0.9037  0.9030  0.9030  0.9030  0.9022  0.9022
[49 : 64]:	0.9011  0.9006  0.8986  0.8983  0.8968  0.8968  0.8968  0.8965  0.8964  0.8959  0.8959  0.8945  0.8944  0.8934  0.8933  0.8933
2024-03-22 06:03:41 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 06:03:41 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #833: GFLOPs: 4986.1195. Time: 20.6294 us. Best GFLOPs: 5019.9652
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #834: GFLOPs: 4958.4965. Time: 20.7444 us. Best GFLOPs: 5019.9652
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #835: GFLOPs: 5023.5679. Time: 20.4756 us. Best GFLOPs: 5023.5679
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #836: GFLOPs: 5023.3013. Time: 20.4767 us. Best GFLOPs: 5023.5679
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #837: GFLOPs: 5025.5637. Time: 20.4675 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #838: GFLOPs: 4910.4611. Time: 20.9473 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #839: GFLOPs: 5024.6368. Time: 20.4713 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #840: GFLOPs: 4896.0285. Time: 21.0090 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #841: GFLOPs: 4898.9021. Time: 20.9967 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #842: GFLOPs: 4898.8871. Time: 20.9968 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #843: GFLOPs: 4129.5054. Time: 24.9087 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #844: GFLOPs: 4899.4919. Time: 20.9942 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #845: GFLOPs: 4863.3672. Time: 21.1501 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #846: GFLOPs: 4863.3335. Time: 21.1503 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #847: GFLOPs: 4896.4415. Time: 21.0073 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #848: GFLOPs: 4888.1800. Time: 21.0428 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #849: GFLOPs: 4868.4329. Time: 21.1281 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #850: GFLOPs: 4896.3790. Time: 21.0075 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #851: GFLOPs: 4769.5781. Time: 21.5660 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #852: GFLOPs: 4769.9137. Time: 21.5645 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #853: GFLOPs: 4769.6575. Time: 21.5657 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #854: GFLOPs: 4770.0415. Time: 21.5639 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #855: GFLOPs: 4770.1379. Time: 21.5635 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #856: GFLOPs: 4637.0259. Time: 22.1825 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #857: GFLOPs: 4637.0263. Time: 22.1825 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #858: GFLOPs: 4701.1600. Time: 21.8799 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #859: GFLOPs: 4594.1169. Time: 22.3897 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #860: GFLOPs: 4637.0574. Time: 22.1823 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #861: GFLOPs: 4614.4721. Time: 22.2909 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #862: GFLOPs: 4740.2620. Time: 21.6994 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #863: GFLOPs: 4614.0514. Time: 22.2929 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #864: GFLOPs: 4737.0073. Time: 21.7143 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #865: GFLOPs: 4613.8362. Time: 22.2940 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #866: GFLOPs: 4639.8373. Time: 22.1691 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #867: GFLOPs: 3836.2794. Time: 26.8126 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #868: GFLOPs: 4635.5308. Time: 22.1896 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #869: GFLOPs: 4831.9986. Time: 21.2874 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #870: GFLOPs: 4600.3955. Time: 22.3591 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #871: GFLOPs: 4600.6867. Time: 22.3577 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #872: GFLOPs: 4653.0155. Time: 22.1063 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #873: GFLOPs: 4547.5500. Time: 22.6189 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #874: GFLOPs: 3712.6239. Time: 27.7057 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #875: GFLOPs: 3712.3731. Time: 27.7076 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #876: GFLOPs: 4725.9821. Time: 21.7650 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #877: GFLOPs: 4726.0559. Time: 21.7646 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #878: GFLOPs: 4726.3596. Time: 21.7632 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #879: GFLOPs: 4457.9902. Time: 23.0734 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #880: GFLOPs: 4458.1977. Time: 23.0723 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #881: GFLOPs: 4457.8026. Time: 23.0743 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #882: GFLOPs: 4651.7539. Time: 22.1123 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #883: GFLOPs: 4358.3811. Time: 23.6007 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #884: GFLOPs: 4505.2739. Time: 22.8312 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #885: GFLOPs: 4201.0880. Time: 24.4843 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #886: GFLOPs: 4200.9744. Time: 24.4850 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #887: GFLOPs: 4200.9240. Time: 24.4853 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #888: GFLOPs: 4652.0305. Time: 22.1109 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #889: GFLOPs: 3610.7876. Time: 28.4871 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #890: GFLOPs: 4505.1933. Time: 22.8316 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #891: GFLOPs: 4505.0434. Time: 22.8324 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #892: GFLOPs: 4379.7100. Time: 23.4858 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #893: GFLOPs: 4729.7482. Time: 21.7476 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #894: GFLOPs: 32.7416. Time: 3141.5949 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #895: GFLOPs: 725.8252. Time: 141.7157 us. Best GFLOPs: 5025.5637
2024-03-22 06:04:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #896: GFLOPs: 103.6489. Time: 992.3961 us. Best GFLOPs: 5025.5637
2024-03-22 06:12:24 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 06:12:25 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 06:12:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 381 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:12:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 765 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:12:26 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-03-22 06:12:28 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:12:30 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:12:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 134 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:12:34 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:12:36 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0274  0.9903  0.9873  0.9865  0.9858  0.9650  0.9635  0.9630  0.9588  0.9528  0.9520  0.9463  0.9365  0.9316  0.9267  0.9255
[17 : 32]:	0.9180  0.9178  0.9131  0.9122  0.9122  0.9122  0.9085  0.9039  0.9037  0.9035  0.9030  0.9030  0.9022  0.9016  0.9013  0.9011
[33 : 48]:	0.9010  0.9010  0.9008  0.9007  0.9005  0.9002  0.8998  0.8990  0.8985  0.8983  0.8979  0.8979  0.8979  0.8977  0.8969  0.8960
[49 : 64]:	0.8951  0.8951  0.8948  0.8940  0.8939  0.8936  0.8936  0.8936  0.8930  0.8927  0.8918  0.8918  0.8914  0.8914  0.8907  0.8906
2024-03-22 06:12:36 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 06:12:36 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #897: GFLOPs: 1297.8143. Time: 79.2569 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #898: GFLOPs: 4605.7379. Time: 22.3332 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #899: GFLOPs: 4974.7805. Time: 20.6765 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #900: GFLOPs: 4959.1631. Time: 20.7416 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #901: GFLOPs: 4991.4678. Time: 20.6073 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #902: GFLOPs: 4899.1052. Time: 20.9958 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #903: GFLOPs: 4661.8441. Time: 22.0644 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #904: GFLOPs: 4897.2322. Time: 21.0039 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #905: GFLOPs: 4923.5299. Time: 20.8917 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #906: GFLOPs: 4929.9310. Time: 20.8646 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #907: GFLOPs: 4838.1125. Time: 21.2605 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #908: GFLOPs: 4628.5665. Time: 22.2230 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #909: GFLOPs: 4727.9001. Time: 21.7561 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #910: GFLOPs: 4877.4447. Time: 21.0891 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #911: GFLOPs: 4197.1568. Time: 24.5073 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #912: GFLOPs: 4750.6116. Time: 21.6521 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #913: GFLOPs: 4752.2436. Time: 21.6447 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #914: GFLOPs: 4751.7351. Time: 21.6470 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #915: GFLOPs: 4760.3612. Time: 21.6078 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #916: GFLOPs: 4760.2012. Time: 21.6085 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #917: GFLOPs: 4760.2847. Time: 21.6081 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #918: GFLOPs: 4760.3927. Time: 21.6076 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #919: GFLOPs: 4760.3328. Time: 21.6079 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #920: GFLOPs: 4170.3794. Time: 24.6646 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #921: GFLOPs: 4643.9534. Time: 22.1494 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #922: GFLOPs: 4644.3158. Time: 22.1477 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #923: GFLOPs: 4682.5436. Time: 21.9669 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #924: GFLOPs: 4173.9564. Time: 24.6435 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #925: GFLOPs: 4665.6955. Time: 22.0462 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #926: GFLOPs: 4680.7127. Time: 21.9755 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #927: GFLOPs: 4640.5893. Time: 22.1655 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #928: GFLOPs: 4879.0649. Time: 21.0821 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #929: GFLOPs: 3796.2225. Time: 27.0956 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #930: GFLOPs: 4605.7993. Time: 22.3329 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #931: GFLOPs: 4607.2305. Time: 22.3260 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #932: GFLOPs: 4651.3731. Time: 22.1141 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #933: GFLOPs: 4172.9795. Time: 24.6492 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #934: GFLOPs: 4180.0046. Time: 24.6078 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #935: GFLOPs: 4228.5651. Time: 24.3252 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #936: GFLOPs: 4212.1902. Time: 24.4198 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #937: GFLOPs: 4614.0831. Time: 22.2928 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #938: GFLOPs: 4215.4550. Time: 24.4009 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #939: GFLOPs: 4619.6262. Time: 22.2660 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #940: GFLOPs: 4619.2109. Time: 22.2680 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #941: GFLOPs: 4619.4952. Time: 22.2667 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #942: GFLOPs: 4203.1302. Time: 24.4724 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #943: GFLOPs: 4652.4066. Time: 22.1092 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #944: GFLOPs: 2192.9844. Time: 46.9045 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #945: GFLOPs: 4601.0395. Time: 22.3560 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #946: GFLOPs: 4596.6567. Time: 22.3773 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #947: GFLOPs: 4182.5665. Time: 24.5927 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #948: GFLOPs: 4124.1726. Time: 24.9410 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #949: GFLOPs: 4170.4658. Time: 24.6641 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #950: GFLOPs: 4166.7385. Time: 24.6862 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #951: GFLOPs: 4202.6272. Time: 24.4754 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #952: GFLOPs: 4202.7592. Time: 24.4746 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #953: GFLOPs: 4202.3591. Time: 24.4769 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #954: GFLOPs: 4382.9607. Time: 23.4683 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #955: GFLOPs: 4382.9441. Time: 23.4684 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #956: GFLOPs: 4382.8822. Time: 23.4688 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #957: GFLOPs: 4168.6480. Time: 24.6749 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #958: GFLOPs: 1032.1716. Time: 99.6548 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #959: GFLOPs: 702.9192. Time: 146.3338 us. Best GFLOPs: 5025.5637
2024-03-22 06:13:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #960: GFLOPs: 625.6734. Time: 164.4002 us. Best GFLOPs: 5025.5637
2024-03-22 06:20:36 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 06:20:37 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 06:20:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 388 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:20:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 780 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:20:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1164 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:20:38 [INFO] [evolutionary_search.cc:723] Sampled 66 candidate(s)
2024-03-22 06:20:40 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 163 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:20:42 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 122 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:20:45 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 150 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:20:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 161 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:20:48 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9755  0.9694  0.9681  0.9664  0.9654  0.9641  0.9614  0.9448  0.9425  0.9418  0.9409  0.9369  0.9363  0.9357  0.9327  0.9326
[17 : 32]:	0.9318  0.9318  0.9317  0.9304  0.9300  0.9271  0.9226  0.9226  0.9209  0.9188  0.9135  0.9113  0.9084  0.9072  0.9055  0.9046
[33 : 48]:	0.9036  0.9021  0.9021  0.9017  0.9006  0.8988  0.8983  0.8981  0.8979  0.8979  0.8959  0.8956  0.8943  0.8942  0.8936  0.8926
[49 : 64]:	0.8909  0.8894  0.8880  0.8877  0.8875  0.8873  0.8864  0.8858  0.8852  0.8849  0.8833  0.8827  0.8824  0.8823  0.8818  0.8789
2024-03-22 06:20:48 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 06:20:48 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #961: GFLOPs: 4915.9169. Time: 20.9240 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #962: GFLOPs: 4932.3368. Time: 20.8544 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #963: GFLOPs: 4925.3258. Time: 20.8841 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #964: GFLOPs: 4939.1439. Time: 20.8256 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #965: GFLOPs: 4891.9367. Time: 21.0266 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #966: GFLOPs: 4892.0000. Time: 21.0263 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #967: GFLOPs: 4917.4818. Time: 20.9174 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #968: GFLOPs: 4943.3479. Time: 20.8079 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #969: GFLOPs: 4120.6823. Time: 24.9621 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #970: GFLOPs: 4912.8650. Time: 20.9370 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #971: GFLOPs: 4873.5849. Time: 21.1058 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #972: GFLOPs: 4879.1046. Time: 21.0819 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #973: GFLOPs: 4860.3288. Time: 21.1633 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #974: GFLOPs: 4905.9863. Time: 20.9664 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #975: GFLOPs: 4789.3200. Time: 21.4771 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #976: GFLOPs: 4740.2620. Time: 21.6994 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #977: GFLOPs: 4760.5564. Time: 21.6069 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #978: GFLOPs: 4267.6231. Time: 24.1026 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #979: GFLOPs: 4916.4629. Time: 20.9217 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #980: GFLOPs: 4854.2252. Time: 21.1900 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #981: GFLOPs: 4855.2174. Time: 21.1856 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #982: GFLOPs: 4767.3538. Time: 21.5761 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #983: GFLOPs: 4759.0994. Time: 21.6135 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #984: GFLOPs: 4884.8659. Time: 21.0570 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #985: GFLOPs: 4741.7484. Time: 21.6926 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #986: GFLOPs: 4714.9769. Time: 21.8158 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #987: GFLOPs: 4721.7203. Time: 21.7846 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #988: GFLOPs: 4737.5086. Time: 21.7120 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #989: GFLOPs: 3724.9361. Time: 27.6141 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #990: GFLOPs: 3278.9087. Time: 31.3704 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #991: GFLOPs: 4865.0410. Time: 21.1428 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #992: GFLOPs: 4235.6209. Time: 24.2847 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #993: GFLOPs: 3724.6523. Time: 27.6162 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #994: GFLOPs: 4393.9079. Time: 23.4099 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #995: GFLOPs: 4393.9492. Time: 23.4096 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #996: GFLOPs: 3289.1406. Time: 31.2728 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #997: GFLOPs: 4794.5838. Time: 21.4535 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #998: GFLOPs: 4443.9735. Time: 23.1461 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #999: GFLOPs: 4613.3118. Time: 22.2965 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1000: GFLOPs: 4475.6013. Time: 22.9826 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1001: GFLOPs: 4817.7050. Time: 21.3506 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1002: GFLOPs: 4525.3589. Time: 22.7299 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1003: GFLOPs: 4531.4838. Time: 22.6991 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1004: GFLOPs: 4445.6201. Time: 23.1376 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1005: GFLOPs: 4507.9950. Time: 22.8174 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1006: GFLOPs: 4651.5533. Time: 22.1132 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1007: GFLOPs: 4425.8061. Time: 23.2411 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1008: GFLOPs: 4269.5433. Time: 24.0918 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1009: GFLOPs: 4207.8154. Time: 24.4452 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1010: GFLOPs: 3842.2894. Time: 26.7707 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1011: GFLOPs: 4886.2243. Time: 21.0512 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1012: GFLOPs: 4355.0867. Time: 23.6185 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1013: GFLOPs: 3599.3251. Time: 28.5778 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1014: GFLOPs: 4446.5792. Time: 23.1326 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1015: GFLOPs: 4193.0193. Time: 24.5314 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1016: GFLOPs: 3557.7862. Time: 28.9115 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1017: GFLOPs: 4207.6153. Time: 24.4463 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1018: GFLOPs: 4584.7191. Time: 22.4356 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1019: GFLOPs: 4546.8651. Time: 22.6224 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1020: GFLOPs: 4548.7807. Time: 22.6128 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1021: GFLOPs: 4137.7578. Time: 24.8591 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1022: GFLOPs: 727.5810. Time: 141.3737 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1023: GFLOPs: 372.8694. Time: 275.8628 us. Best GFLOPs: 5025.5637
2024-03-22 06:21:41 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1024: GFLOPs: 825.5953. Time: 124.5899 us. Best GFLOPs: 5025.5637
2024-03-22 06:27:12 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 06:27:13 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 06:27:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 386 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:27:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 784 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:27:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1174 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:27:14 [INFO] [evolutionary_search.cc:723] Sampled 56 candidate(s)
2024-03-22 06:27:16 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 162 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:27:18 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 158 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:27:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:27:23 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 166 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:27:24 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9803  0.9801  0.9777  0.9734  0.9712  0.9651  0.9626  0.9625  0.9599  0.9565  0.9540  0.9539  0.9539  0.9538  0.9538  0.9522
[17 : 32]:	0.9521  0.9511  0.9510  0.9508  0.9506  0.9503  0.9496  0.9493  0.9493  0.9449  0.9428  0.9389  0.9387  0.9387  0.9352  0.9345
[33 : 48]:	0.9345  0.9345  0.9319  0.9309  0.9261  0.9253  0.9241  0.9239  0.9198  0.9196  0.9196  0.9194  0.9168  0.9166  0.9158  0.9156
[49 : 64]:	0.9114  0.9073  0.9060  0.9055  0.9049  0.9043  0.9021  0.9008  0.8991  0.8958  0.8951  0.8951  0.8949  0.8948  0.8947  0.8944
2024-03-22 06:27:24 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 06:27:24 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1025: GFLOPs: 4946.9901. Time: 20.7926 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1026: GFLOPs: 4878.8702. Time: 21.0829 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1027: GFLOPs: 4945.1568. Time: 20.8003 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1028: GFLOPs: 4931.8420. Time: 20.8565 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1029: GFLOPs: 4924.2578. Time: 20.8886 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1030: GFLOPs: 4795.2563. Time: 21.4505 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1031: GFLOPs: 4766.7143. Time: 21.5790 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1032: GFLOPs: 4853.5281. Time: 21.1930 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1033: GFLOPs: 4870.9919. Time: 21.1170 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1034: GFLOPs: 4893.7901. Time: 21.0186 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1035: GFLOPs: 4895.5679. Time: 21.0110 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1036: GFLOPs: 4897.6249. Time: 21.0022 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1037: GFLOPs: 4940.3936. Time: 20.8204 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1038: GFLOPs: 4813.4059. Time: 21.3697 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1039: GFLOPs: 4929.1096. Time: 20.8680 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1040: GFLOPs: 4901.8467. Time: 20.9841 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1041: GFLOPs: 4903.4732. Time: 20.9771 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1042: GFLOPs: 4867.4006. Time: 21.1326 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1043: GFLOPs: 4931.4428. Time: 20.8582 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1044: GFLOPs: 4903.0276. Time: 20.9790 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1045: GFLOPs: 4736.1869. Time: 21.7181 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1046: GFLOPs: 4876.5499. Time: 21.0929 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1047: GFLOPs: 4821.8668. Time: 21.3322 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1048: GFLOPs: 4735.9708. Time: 21.7191 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1049: GFLOPs: 4853.0538. Time: 21.1951 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1050: GFLOPs: 4724.2299. Time: 21.7730 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1051: GFLOPs: 4890.5189. Time: 21.0327 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1052: GFLOPs: 4596.6873. Time: 22.3772 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1053: GFLOPs: 4738.6867. Time: 21.7066 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1054: GFLOPs: 4633.0123. Time: 22.2017 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1055: GFLOPs: 4890.2019. Time: 21.0341 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1056: GFLOPs: 4755.7782. Time: 21.6286 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1057: GFLOPs: 4871.0414. Time: 21.1168 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1058: GFLOPs: 4857.4402. Time: 21.1759 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1059: GFLOPs: 4722.0619. Time: 21.7830 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1060: GFLOPs: 4830.1676. Time: 21.2955 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1061: GFLOPs: 4611.9698. Time: 22.3030 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1062: GFLOPs: 4677.4767. Time: 21.9907 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1063: GFLOPs: 4732.8713. Time: 21.7333 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1064: GFLOPs: 4864.8494. Time: 21.1437 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1065: GFLOPs: 4700.4102. Time: 21.8834 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1066: GFLOPs: 4859.2267. Time: 21.1681 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1067: GFLOPs: 4866.3926. Time: 21.1370 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1068: GFLOPs: 4698.8151. Time: 21.8908 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1069: GFLOPs: 4639.1128. Time: 22.1725 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1070: GFLOPs: 4638.7095. Time: 22.1744 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1071: GFLOPs: 4701.9119. Time: 21.8764 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1072: GFLOPs: 4777.8367. Time: 21.5287 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1073: GFLOPs: 4859.3727. Time: 21.1675 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1074: GFLOPs: 4565.4634. Time: 22.5302 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1075: GFLOPs: 4639.9343. Time: 22.1686 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1076: GFLOPs: 4711.1638. Time: 21.8334 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1077: GFLOPs: 4504.7040. Time: 22.8341 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1078: GFLOPs: 4126.6410. Time: 24.9260 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1079: GFLOPs: 4725.9913. Time: 21.7649 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1080: GFLOPs: 4792.6656. Time: 21.4621 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1081: GFLOPs: 4504.6658. Time: 22.8343 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1082: GFLOPs: 4374.6993. Time: 23.5127 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1083: GFLOPs: 4577.2605. Time: 22.4721 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1084: GFLOPs: 4494.0866. Time: 22.8880 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1085: GFLOPs: 4653.7578. Time: 22.1027 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1086: GFLOPs: 445.2299. Time: 231.0285 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1087: GFLOPs: 759.8773. Time: 135.3650 us. Best GFLOPs: 5025.5637
2024-03-22 06:28:21 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1088: GFLOPs: 385.0910. Time: 267.1078 us. Best GFLOPs: 5025.5637
2024-03-22 06:33:34 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 06:33:34 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 06:33:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 390 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:33:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 774 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:33:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1163 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:33:36 [INFO] [evolutionary_search.cc:723] Sampled 67 candidate(s)
2024-03-22 06:33:37 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:33:40 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:33:42 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 172 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:33:44 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 132 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:33:45 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9832  0.9798  0.9798  0.9796  0.9749  0.9729  0.9728  0.9723  0.9712  0.9712  0.9710  0.9702  0.9682  0.9680  0.9655  0.9652
[17 : 32]:	0.9648  0.9638  0.9635  0.9613  0.9610  0.9594  0.9575  0.9570  0.9566  0.9566  0.9565  0.9564  0.9563  0.9563  0.9562  0.9553
[33 : 48]:	0.9541  0.9541  0.9541  0.9541  0.9506  0.9506  0.9493  0.9493  0.9485  0.9467  0.9462  0.9446  0.9421  0.9418  0.9415  0.9405
[49 : 64]:	0.9365  0.9355  0.9347  0.9328  0.9314  0.9305  0.9290  0.9287  0.9277  0.9241  0.9230  0.9222  0.9201  0.9196  0.9186  0.9182
2024-03-22 06:33:46 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 06:33:46 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1089: GFLOPs: 4937.9838. Time: 20.8305 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1090: GFLOPs: 4932.7129. Time: 20.8528 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1091: GFLOPs: 4902.9933. Time: 20.9792 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1092: GFLOPs: 4911.8519. Time: 20.9413 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1093: GFLOPs: 4939.2540. Time: 20.8252 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1094: GFLOPs: 4983.0666. Time: 20.6421 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1095: GFLOPs: 4927.4708. Time: 20.8750 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1096: GFLOPs: 4942.5596. Time: 20.8112 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1097: GFLOPs: 4896.4137. Time: 21.0074 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1098: GFLOPs: 4891.5780. Time: 21.0281 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1099: GFLOPs: 4894.8761. Time: 21.0140 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1100: GFLOPs: 4871.5701. Time: 21.1145 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1101: GFLOPs: 4898.8367. Time: 20.9970 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1102: GFLOPs: 4931.1273. Time: 20.8595 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1103: GFLOPs: 4891.3624. Time: 21.0291 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1104: GFLOPs: 4881.0156. Time: 21.0736 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1105: GFLOPs: 4926.8865. Time: 20.8774 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1106: GFLOPs: 4875.1077. Time: 21.0992 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1107: GFLOPs: 4945.1506. Time: 20.8003 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1108: GFLOPs: 4892.8890. Time: 21.0225 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1109: GFLOPs: 4879.0956. Time: 21.0819 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1110: GFLOPs: 4881.3344. Time: 21.0723 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1111: GFLOPs: 4807.2297. Time: 21.3971 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1112: GFLOPs: 4907.6979. Time: 20.9591 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1113: GFLOPs: 4895.8677. Time: 21.0097 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1114: GFLOPs: 4859.9601. Time: 21.1649 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1115: GFLOPs: 4779.5303. Time: 21.5211 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1116: GFLOPs: 4949.0078. Time: 20.7841 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1117: GFLOPs: 4890.0349. Time: 21.0348 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1118: GFLOPs: 4895.8944. Time: 21.0096 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1119: GFLOPs: 4752.4531. Time: 21.6437 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1120: GFLOPs: 4932.4382. Time: 20.8539 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1121: GFLOPs: 4884.1763. Time: 21.0600 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1122: GFLOPs: 4852.2745. Time: 21.1985 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1123: GFLOPs: 4883.1303. Time: 21.0645 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1124: GFLOPs: 4924.4024. Time: 20.8880 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1125: GFLOPs: 4899.2013. Time: 20.9954 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1126: GFLOPs: 4855.3942. Time: 21.1849 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1127: GFLOPs: 4837.9932. Time: 21.2610 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1128: GFLOPs: 4790.7733. Time: 21.4706 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1129: GFLOPs: 4865.9075. Time: 21.1391 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1130: GFLOPs: 4737.1023. Time: 21.7139 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1131: GFLOPs: 4667.3749. Time: 22.0383 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1132: GFLOPs: 4771.8544. Time: 21.5557 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1133: GFLOPs: 4869.7244. Time: 21.1225 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1134: GFLOPs: 4697.9394. Time: 21.8949 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1135: GFLOPs: 4737.4180. Time: 21.7124 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1136: GFLOPs: 4590.4802. Time: 22.4074 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1137: GFLOPs: 4922.8556. Time: 20.8945 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1138: GFLOPs: 4125.2734. Time: 24.9343 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1139: GFLOPs: 4521.5227. Time: 22.7492 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1140: GFLOPs: 4821.7613. Time: 21.3326 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1141: GFLOPs: 4641.6663. Time: 22.1603 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1142: GFLOPs: 4873.4825. Time: 21.1062 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1143: GFLOPs: 4680.7465. Time: 21.9753 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1144: GFLOPs: 4711.0951. Time: 21.8337 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1145: GFLOPs: 4848.9843. Time: 21.2129 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1146: GFLOPs: 4555.9439. Time: 22.5773 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1147: GFLOPs: 4138.1653. Time: 24.8566 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1148: GFLOPs: 4898.5645. Time: 20.9982 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1149: GFLOPs: 4128.1994. Time: 24.9166 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1150: GFLOPs: 366.4817. Time: 280.6710 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1151: GFLOPs: 3305.1656. Time: 31.1212 us. Best GFLOPs: 5025.5637
2024-03-22 06:34:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1152: GFLOPs: 1124.1974. Time: 91.4971 us. Best GFLOPs: 5025.5637
2024-03-22 06:41:20 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 06:41:21 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 06:41:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 387 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:41:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 773 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:41:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:41:22 [INFO] [evolutionary_search.cc:723] Sampled 74 candidate(s)
2024-03-22 06:41:24 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:41:26 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:41:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 150 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:41:31 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:41:32 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2950  0.9826  0.9809  0.9809  0.9807  0.9789  0.9778  0.9777  0.9768  0.9737  0.9721  0.9719  0.9718  0.9702  0.9689  0.9676
[17 : 32]:	0.9671  0.9659  0.9648  0.9633  0.9592  0.9589  0.9583  0.9583  0.9560  0.9544  0.9541  0.9538  0.9534  0.9524  0.9524  0.9512
[33 : 48]:	0.9504  0.9498  0.9489  0.9474  0.9453  0.9453  0.9438  0.9414  0.9411  0.9409  0.9376  0.9364  0.9355  0.9344  0.9320  0.9304
[49 : 64]:	0.9294  0.9291  0.9279  0.9266  0.9263  0.9223  0.9216  0.9203  0.9194  0.9187  0.9180  0.9176  0.9161  0.9158  0.9151  0.9151
2024-03-22 06:41:32 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 06:41:32 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1153: GFLOPs: 2501.5402. Time: 41.1190 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1154: GFLOPs: 4898.7582. Time: 20.9973 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1155: GFLOPs: 4914.2818. Time: 20.9310 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1156: GFLOPs: 4915.6341. Time: 20.9252 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1157: GFLOPs: 4873.1907. Time: 21.1075 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1158: GFLOPs: 4900.7671. Time: 20.9887 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1159: GFLOPs: 4895.5629. Time: 21.0110 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1160: GFLOPs: 4902.8696. Time: 20.9797 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1161: GFLOPs: 4881.3181. Time: 21.0723 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1162: GFLOPs: 4906.7524. Time: 20.9631 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1163: GFLOPs: 4830.6400. Time: 21.2934 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1164: GFLOPs: 4881.5686. Time: 21.0713 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1165: GFLOPs: 4905.0369. Time: 20.9704 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1166: GFLOPs: 4928.7636. Time: 20.8695 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1167: GFLOPs: 4929.1668. Time: 20.8678 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1168: GFLOPs: 4910.9908. Time: 20.9450 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1169: GFLOPs: 4889.7161. Time: 21.0361 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1170: GFLOPs: 4902.4035. Time: 20.9817 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1171: GFLOPs: 4905.4301. Time: 20.9688 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1172: GFLOPs: 4881.3664. Time: 21.0721 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1173: GFLOPs: 4903.8817. Time: 20.9754 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1174: GFLOPs: 4861.9356. Time: 21.1563 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1175: GFLOPs: 4950.2547. Time: 20.7789 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1176: GFLOPs: 4835.0197. Time: 21.2741 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1177: GFLOPs: 4769.4014. Time: 21.5668 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1178: GFLOPs: 4834.7258. Time: 21.2754 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1179: GFLOPs: 4942.8970. Time: 20.8098 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1180: GFLOPs: 4857.1790. Time: 21.1771 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1181: GFLOPs: 4863.7499. Time: 21.1485 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1182: GFLOPs: 4782.8514. Time: 21.5062 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1183: GFLOPs: 4809.1932. Time: 21.3884 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1184: GFLOPs: 4805.8616. Time: 21.4032 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1185: GFLOPs: 3631.1218. Time: 28.3276 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1186: GFLOPs: 4876.8137. Time: 21.0918 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1187: GFLOPs: 4755.2725. Time: 21.6309 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1188: GFLOPs: 4858.9688. Time: 21.1693 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1189: GFLOPs: 4852.3085. Time: 21.1983 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1190: GFLOPs: 4738.5552. Time: 21.7072 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1191: GFLOPs: 4835.6911. Time: 21.2712 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1192: GFLOPs: 4738.4295. Time: 21.7078 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1193: GFLOPs: 4716.5026. Time: 21.8087 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1194: GFLOPs: 4833.5489. Time: 21.2806 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1195: GFLOPs: 3144.7673. Time: 32.7086 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1196: GFLOPs: 4704.2200. Time: 21.8656 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1197: GFLOPs: 4739.6021. Time: 21.7024 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1198: GFLOPs: 3575.7747. Time: 28.7660 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1199: GFLOPs: 4608.9369. Time: 22.3177 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1200: GFLOPs: 3603.5369. Time: 28.5444 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1201: GFLOPs: 3451.4778. Time: 29.8020 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1202: GFLOPs: 4740.1956. Time: 21.6997 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1203: GFLOPs: 4789.2970. Time: 21.4772 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1204: GFLOPs: 4058.0327. Time: 25.3475 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1205: GFLOPs: 4721.0532. Time: 21.7877 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1206: GFLOPs: 4682.8215. Time: 21.9656 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1207: GFLOPs: 3034.7092. Time: 33.8948 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1208: GFLOPs: 4693.7222. Time: 21.9145 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1209: GFLOPs: 3216.6130. Time: 31.9780 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1210: GFLOPs: 4730.9528. Time: 21.7421 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1211: GFLOPs: 4546.5671. Time: 22.6238 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1212: GFLOPs: 3873.4665. Time: 26.5552 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1213: GFLOPs: 4651.2866. Time: 22.1145 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1214: GFLOPs: 799.2863. Time: 128.6908 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1215: GFLOPs: 763.8931. Time: 134.6534 us. Best GFLOPs: 5025.5637
2024-03-22 06:42:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1216: GFLOPs: 520.8594. Time: 197.4829 us. Best GFLOPs: 5025.5637
2024-03-22 06:48:45 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 06:48:45 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 06:48:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 395 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:48:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 779 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:48:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1162 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:48:47 [INFO] [evolutionary_search.cc:723] Sampled 68 candidate(s)
2024-03-22 06:48:49 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:48:51 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:48:53 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 135 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:48:56 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:48:57 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.4926  1.1293  0.9925  0.9800  0.9776  0.9753  0.9750  0.9745  0.9742  0.9732  0.9725  0.9715  0.9702  0.9691  0.9685  0.9676
[17 : 32]:	0.9667  0.9655  0.9649  0.9634  0.9630  0.9629  0.9629  0.9624  0.9603  0.9593  0.9573  0.9513  0.9501  0.9494  0.9491  0.9487
[33 : 48]:	0.9481  0.9454  0.9431  0.9408  0.9390  0.9362  0.9356  0.9345  0.9336  0.9332  0.9304  0.9283  0.9279  0.9276  0.9276  0.9268
[49 : 64]:	0.9266  0.9262  0.9241  0.9241  0.9233  0.9231  0.9227  0.9222  0.9210  0.9191  0.9188  0.9175  0.9139  0.9133  0.9121  0.9115
2024-03-22 06:48:57 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 06:48:57 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1217: GFLOPs: 3342.8601. Time: 30.7703 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1218: GFLOPs: 2153.4770. Time: 47.7650 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1219: GFLOPs: 4725.0931. Time: 21.7691 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1220: GFLOPs: 4902.3643. Time: 20.9819 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1221: GFLOPs: 4899.5905. Time: 20.9938 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1222: GFLOPs: 4890.9546. Time: 21.0308 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1223: GFLOPs: 4879.0973. Time: 21.0819 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1224: GFLOPs: 4896.7153. Time: 21.0061 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1225: GFLOPs: 3990.9987. Time: 25.7732 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1226: GFLOPs: 4859.5812. Time: 21.1666 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1227: GFLOPs: 4924.6553. Time: 20.8869 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1228: GFLOPs: 4879.3898. Time: 21.0807 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1229: GFLOPs: 4942.3971. Time: 20.8119 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1230: GFLOPs: 4910.8952. Time: 20.9454 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1231: GFLOPs: 4873.6120. Time: 21.1057 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1232: GFLOPs: 4859.9391. Time: 21.1650 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1233: GFLOPs: 4864.8121. Time: 21.1438 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1234: GFLOPs: 4851.7462. Time: 21.2008 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1235: GFLOPs: 4834.8856. Time: 21.2747 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1236: GFLOPs: 4849.1971. Time: 21.2119 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1237: GFLOPs: 4869.1442. Time: 21.1250 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1238: GFLOPs: 4864.5198. Time: 21.1451 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1239: GFLOPs: 4853.9597. Time: 21.1911 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1240: GFLOPs: 4835.7359. Time: 21.2710 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1241: GFLOPs: 4861.8096. Time: 21.1569 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1242: GFLOPs: 4867.6002. Time: 21.1317 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1243: GFLOPs: 4854.6790. Time: 21.1880 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1244: GFLOPs: 4589.6868. Time: 22.4113 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1245: GFLOPs: 4873.6120. Time: 21.1057 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1246: GFLOPs: 4718.0729. Time: 21.8014 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1247: GFLOPs: 4713.1155. Time: 21.8244 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1248: GFLOPs: 4778.2029. Time: 21.5271 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1249: GFLOPs: 4800.7912. Time: 21.4258 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1250: GFLOPs: 4754.5766. Time: 21.6341 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1251: GFLOPs: 4606.1006. Time: 22.3314 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1252: GFLOPs: 4712.7665. Time: 21.8260 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1253: GFLOPs: 4014.2538. Time: 25.6239 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1254: GFLOPs: 4686.0490. Time: 21.9504 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1255: GFLOPs: 4801.1672. Time: 21.4241 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1256: GFLOPs: 4483.8109. Time: 22.9405 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1257: GFLOPs: 4725.1023. Time: 21.7690 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1258: GFLOPs: 4669.8341. Time: 22.0266 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1259: GFLOPs: 4545.7106. Time: 22.6281 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1260: GFLOPs: 4734.7984. Time: 21.7244 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1261: GFLOPs: 4764.5352. Time: 21.5888 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1262: GFLOPs: 4643.0807. Time: 22.1536 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1263: GFLOPs: 4709.1493. Time: 21.8428 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1264: GFLOPs: 4757.1261. Time: 21.6225 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1265: GFLOPs: 4681.2650. Time: 21.9729 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1266: GFLOPs: 4580.5648. Time: 22.4559 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1267: GFLOPs: 4622.8477. Time: 22.2505 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1268: GFLOPs: 4624.4582. Time: 22.2428 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1269: GFLOPs: 4629.2184. Time: 22.2199 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1270: GFLOPs: 4609.4092. Time: 22.3154 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1271: GFLOPs: 4080.8539. Time: 25.2057 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1272: GFLOPs: 4703.1439. Time: 21.8706 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1273: GFLOPs: 4634.7822. Time: 22.1932 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1274: GFLOPs: 4746.0547. Time: 21.6729 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1275: GFLOPs: 4691.1077. Time: 21.9268 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1276: GFLOPs: 4063.5985. Time: 25.3127 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1277: GFLOPs: 4629.6534. Time: 22.2178 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1278: GFLOPs: 677.9491. Time: 151.7235 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1279: GFLOPs: 493.2646. Time: 208.5307 us. Best GFLOPs: 5025.5637
2024-03-22 06:49:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1280: GFLOPs: 1165.0070. Time: 88.2920 us. Best GFLOPs: 5025.5637
2024-03-22 06:53:56 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 06:53:56 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 06:53:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 387 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:53:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 777 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:53:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1161 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:53:58 [INFO] [evolutionary_search.cc:723] Sampled 69 candidate(s)
2024-03-22 06:53:59 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 135 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:54:02 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 133 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:54:04 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 160 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:54:06 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 06:54:07 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3614  1.3538  1.2799  1.2774  1.2774  1.2773  1.2753  1.2727  1.2684  1.2632  1.2632  1.2632  1.2563  1.2421  1.2287  1.2069
[17 : 32]:	1.2052  1.2025  1.1315  1.1029  1.0936  1.0424  1.0237  1.0152  0.9926  0.9895  0.9883  0.9869  0.9795  0.9787  0.9751  0.9745
[33 : 48]:	0.9726  0.9726  0.9721  0.9721  0.9718  0.9718  0.9710  0.9706  0.9701  0.9699  0.9671  0.9665  0.9657  0.9647  0.9627  0.9618
[49 : 64]:	0.9612  0.9608  0.9602  0.9601  0.9593  0.9585  0.9568  0.9558  0.9506  0.9461  0.9450  0.9433  0.9430  0.9419  0.9415  0.9385
2024-03-22 06:54:07 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 06:54:07 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1281: GFLOPs: 821.9333. Time: 125.1449 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1282: GFLOPs: 717.7914. Time: 143.3018 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1283: GFLOPs: 674.0542. Time: 152.6002 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1284: GFLOPs: 694.9584. Time: 148.0100 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1285: GFLOPs: 695.0499. Time: 147.9905 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1286: GFLOPs: 698.1430. Time: 147.3349 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1287: GFLOPs: 702.4976. Time: 146.4216 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1288: GFLOPs: 694.8706. Time: 148.0287 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1289: GFLOPs: 712.2095. Time: 144.4249 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1290: GFLOPs: 693.6952. Time: 148.2795 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1291: GFLOPs: 810.8050. Time: 126.8626 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1292: GFLOPs: 696.7806. Time: 147.6229 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1293: GFLOPs: 702.7190. Time: 146.3754 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1294: GFLOPs: 704.2909. Time: 146.0487 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1295: GFLOPs: 695.4742. Time: 147.9002 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1296: GFLOPs: 3985.4759. Time: 25.8089 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1297: GFLOPs: 810.4943. Time: 126.9112 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1298: GFLOPs: 2025.1179. Time: 50.7925 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1299: GFLOPs: 838.8534. Time: 122.6207 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1300: GFLOPs: 3942.8163. Time: 26.0882 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1301: GFLOPs: 3957.7920. Time: 25.9894 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1302: GFLOPs: 4190.6520. Time: 24.5453 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1303: GFLOPs: 3953.4897. Time: 26.0177 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1304: GFLOPs: 1465.8856. Time: 70.1697 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1305: GFLOPs: 3400.3983. Time: 30.2496 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1306: GFLOPs: 291.4781. Time: 352.8938 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1307: GFLOPs: 314.9429. Time: 326.6014 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1308: GFLOPs: 4474.7821. Time: 22.9868 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1309: GFLOPs: 2067.5059. Time: 49.7512 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1310: GFLOPs: 599.6283. Time: 171.5409 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1311: GFLOPs: 2128.0653. Time: 48.3354 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1312: GFLOPs: 3683.7862. Time: 27.9226 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1313: GFLOPs: 4304.1384. Time: 23.8981 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1314: GFLOPs: 4312.9655. Time: 23.8492 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1315: GFLOPs: 4866.3603. Time: 21.1371 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1316: GFLOPs: 4868.3908. Time: 21.1283 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1317: GFLOPs: 3861.2028. Time: 26.6396 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1318: GFLOPs: 1590.7330. Time: 64.6625 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1319: GFLOPs: 4781.4025. Time: 21.5127 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1320: GFLOPs: 4884.1792. Time: 21.0600 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1321: GFLOPs: 4819.5203. Time: 21.3425 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1322: GFLOPs: 4844.5094. Time: 21.2324 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1323: GFLOPs: 4886.6556. Time: 21.0493 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1324: GFLOPs: 4923.2651. Time: 20.8928 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1325: GFLOPs: 3307.7113. Time: 31.0973 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1326: GFLOPs: 1643.3418. Time: 62.5925 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1327: GFLOPs: 1560.1052. Time: 65.9320 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1328: GFLOPs: 4837.7913. Time: 21.2619 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1329: GFLOPs: 4844.4657. Time: 21.2326 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1330: GFLOPs: 4830.9531. Time: 21.2920 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1331: GFLOPs: 3774.5194. Time: 27.2514 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1332: GFLOPs: 4893.8235. Time: 21.0185 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1333: GFLOPs: 3638.2063. Time: 28.2724 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1334: GFLOPs: 4289.4096. Time: 23.9802 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1335: GFLOPs: 4853.4368. Time: 21.1934 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1336: GFLOPs: 4871.8395. Time: 21.1133 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1337: GFLOPs: 4848.9402. Time: 21.2130 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1338: GFLOPs: 4848.0739. Time: 21.2168 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1339: GFLOPs: 3807.4595. Time: 27.0156 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1340: GFLOPs: 4114.3850. Time: 25.0003 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1341: GFLOPs: 4850.4754. Time: 21.2063 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1342: GFLOPs: 997.8880. Time: 103.0785 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1343: GFLOPs: 80.0668. Time: 1284.6867 us. Best GFLOPs: 5025.5637
2024-03-22 06:54:54 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1344: GFLOPs: 817.9142. Time: 125.7599 us. Best GFLOPs: 5025.5637
2024-03-22 07:00:36 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 07:00:36 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 07:00:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 386 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:00:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 776 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:00:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:00:38 [INFO] [evolutionary_search.cc:723] Sampled 75 candidate(s)
2024-03-22 07:00:40 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 160 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:00:42 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 141 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:00:44 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:00:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:00:48 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0726  0.9783  0.9780  0.9765  0.9695  0.9670  0.9661  0.9651  0.9620  0.9540  0.9529  0.9498  0.9487  0.9458  0.9450  0.9442
[17 : 32]:	0.9441  0.9440  0.9429  0.9408  0.9389  0.9383  0.9382  0.9354  0.9286  0.9283  0.9280  0.9273  0.9242  0.9230  0.9213  0.9207
[33 : 48]:	0.9184  0.9184  0.9183  0.9182  0.9181  0.9174  0.9168  0.9163  0.9157  0.9151  0.9145  0.9141  0.9126  0.9124  0.9120  0.9120
[49 : 64]:	0.9118  0.9116  0.9103  0.9097  0.9082  0.9077  0.9062  0.9058  0.9058  0.9055  0.9055  0.9053  0.9046  0.9046  0.9039  0.9035
2024-03-22 07:00:48 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 07:00:48 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1345: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(7), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 16, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 64, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1346: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 64, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1347: GFLOPs: 4953.5809. Time: 20.7649 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1348: GFLOPs: 4839.1615. Time: 21.2559 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1349: GFLOPs: 4936.2636. Time: 20.8378 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1350: GFLOPs: 4908.1580. Time: 20.9571 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1351: GFLOPs: 4895.0399. Time: 21.0133 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1352: GFLOPs: 2948.4778. Time: 34.8861 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1353: GFLOPs: 4885.2946. Time: 21.0552 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1354: GFLOPs: 4256.8683. Time: 24.1635 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1355: GFLOPs: 4257.0216. Time: 24.1626 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1356: GFLOPs: 4079.2549. Time: 25.2156 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1357: GFLOPs: 4249.6213. Time: 24.2047 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1358: GFLOPs: 4758.4482. Time: 21.6165 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1359: GFLOPs: 4169.6505. Time: 24.6689 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1360: GFLOPs: 4541.5829. Time: 22.6487 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1361: GFLOPs: 4285.1007. Time: 24.0043 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1362: GFLOPs: 4257.9951. Time: 24.1571 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1363: GFLOPs: 4841.3727. Time: 21.2462 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1364: GFLOPs: 4277.4161. Time: 24.0474 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1365: GFLOPs: 4277.5264. Time: 24.0468 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1366: GFLOPs: 4703.9974. Time: 21.8667 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1367: GFLOPs: 4799.4851. Time: 21.4316 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1368: GFLOPs: 4750.7184. Time: 21.6516 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1369: GFLOPs: 4692.5528. Time: 21.9200 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1370: GFLOPs: 4083.2957. Time: 25.1906 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1371: GFLOPs: 4707.9493. Time: 21.8483 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1372: GFLOPs: 4236.6632. Time: 24.2787 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1373: GFLOPs: 4653.0350. Time: 22.1062 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1374: GFLOPs: 3051.9045. Time: 33.7038 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1375: GFLOPs: 4653.1776. Time: 22.1055 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1376: GFLOPs: 4141.5350. Time: 24.8364 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1377: GFLOPs: 4602.1895. Time: 22.3504 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1378: GFLOPs: 4591.3743. Time: 22.4031 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1379: GFLOPs: 4880.3978. Time: 21.0763 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1380: GFLOPs: 4583.2287. Time: 22.4429 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1381: GFLOPs: 4670.2732. Time: 22.0246 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1382: GFLOPs: 4281.4493. Time: 24.0248 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1383: GFLOPs: 4888.7353. Time: 21.0404 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1384: GFLOPs: 4643.7632. Time: 22.1503 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1385: GFLOPs: 4262.7330. Time: 24.1302 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1386: GFLOPs: 4291.6158. Time: 23.9678 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1387: GFLOPs: 3961.6569. Time: 25.9641 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1388: GFLOPs: 4888.3271. Time: 21.0421 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1389: GFLOPs: 4693.5841. Time: 21.9152 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1390: GFLOPs: 4887.9197. Time: 21.0439 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1391: GFLOPs: 4250.6548. Time: 24.1988 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1392: GFLOPs: 3681.3055. Time: 27.9414 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1393: GFLOPs: 4640.4321. Time: 22.1662 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1394: GFLOPs: 4862.5810. Time: 21.1535 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1395: GFLOPs: 4583.7113. Time: 22.4405 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1396: GFLOPs: 4374.0691. Time: 23.5160 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1397: GFLOPs: 3786.6253. Time: 27.1642 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1398: GFLOPs: 4466.7822. Time: 23.0279 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1399: GFLOPs: 4396.2460. Time: 23.3974 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1400: GFLOPs: 4175.3470. Time: 24.6353 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1401: GFLOPs: 4616.1758. Time: 22.2827 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1402: GFLOPs: 4860.3869. Time: 21.1631 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1403: GFLOPs: 3833.0920. Time: 26.8349 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1404: GFLOPs: 4640.0485. Time: 22.1680 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1405: GFLOPs: 4632.4504. Time: 22.2044 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1406: GFLOPs: 448.5121. Time: 229.3378 us. Best GFLOPs: 5025.5637
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1407: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(14) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(14) // T.int64(7) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1024), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0)
                                    v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(98))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(14) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(14) // T.int64(7) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(14) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(14) // T.int64(7) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 07:01:48 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1408: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(14), T.int64(1), T.int64(16), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(16) + ff_3_init * T.int64(16) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(196))
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(14), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(7), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(16) + ff_3 * T.int64(16) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(7), T.int64(14)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 07:07:27 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 07:07:28 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 07:07:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 388 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:07:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 771 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:07:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1162 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:07:29 [INFO] [evolutionary_search.cc:723] Sampled 68 candidate(s)
2024-03-22 07:07:31 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 147 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:07:33 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:07:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:07:38 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 171 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:07:39 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9784  0.9739  0.9698  0.9686  0.9682  0.9642  0.9575  0.9539  0.9527  0.9494  0.9491  0.9475  0.9474  0.9435  0.9418  0.9402
[17 : 32]:	0.9377  0.9373  0.9362  0.9345  0.9340  0.9339  0.9328  0.9327  0.9311  0.9303  0.9301  0.9300  0.9294  0.9292  0.9290  0.9290
[33 : 48]:	0.9290  0.9287  0.9282  0.9280  0.9279  0.9278  0.9276  0.9259  0.9236  0.9236  0.9212  0.9190  0.9190  0.9190  0.9183  0.9181
[49 : 64]:	0.9177  0.9175  0.9158  0.9143  0.9136  0.9136  0.9129  0.9127  0.9112  0.9095  0.9088  0.9087  0.9076  0.9072  0.9065  0.9048
2024-03-22 07:07:39 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 07:07:39 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1409: GFLOPs: 4963.6972. Time: 20.7226 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1410: GFLOPs: 4996.2956. Time: 20.5874 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1411: GFLOPs: 4906.2917. Time: 20.9651 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1412: GFLOPs: 4975.1091. Time: 20.6751 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1413: GFLOPs: 4924.4044. Time: 20.8880 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1414: GFLOPs: 4916.6895. Time: 20.9207 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1415: GFLOPs: 4914.3835. Time: 20.9306 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1416: GFLOPs: 4906.7859. Time: 20.9630 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1417: GFLOPs: 4923.7656. Time: 20.8907 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1418: GFLOPs: 4639.8205. Time: 22.1691 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1419: GFLOPs: 4877.6683. Time: 21.0881 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1420: GFLOPs: 4681.6983. Time: 21.9708 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1421: GFLOPs: 4790.8618. Time: 21.4702 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1422: GFLOPs: 4743.8770. Time: 21.6829 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1423: GFLOPs: 4743.6788. Time: 21.6838 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1424: GFLOPs: 4745.0531. Time: 21.6775 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1425: GFLOPs: 4825.6637. Time: 21.3154 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1426: GFLOPs: 4839.2940. Time: 21.2553 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1427: GFLOPs: 4675.7277. Time: 21.9989 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1428: GFLOPs: 4895.2358. Time: 21.0124 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1429: GFLOPs: 4644.6723. Time: 22.1460 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1430: GFLOPs: 4633.2485. Time: 22.2006 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1431: GFLOPs: 4825.6700. Time: 21.3153 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1432: GFLOPs: 4641.4859. Time: 22.1612 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1433: GFLOPs: 4720.2476. Time: 21.7914 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1434: GFLOPs: 4790.6708. Time: 21.4711 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1435: GFLOPs: 4681.4954. Time: 21.9718 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1436: GFLOPs: 4694.0322. Time: 21.9131 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1437: GFLOPs: 4730.8460. Time: 21.7426 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1438: GFLOPs: 4626.9017. Time: 22.2310 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1439: GFLOPs: 4899.8225. Time: 20.9928 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1440: GFLOPs: 4900.0227. Time: 20.9919 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1441: GFLOPs: 4899.9182. Time: 20.9924 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1442: GFLOPs: 4716.9485. Time: 21.8066 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1443: GFLOPs: 4728.7542. Time: 21.7522 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1444: GFLOPs: 4626.4814. Time: 22.2331 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1445: GFLOPs: 4645.8628. Time: 22.1403 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1446: GFLOPs: 4639.2355. Time: 22.1719 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1447: GFLOPs: 4609.2754. Time: 22.3160 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1448: GFLOPs: 4641.6399. Time: 22.1604 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1449: GFLOPs: 4886.3885. Time: 21.0505 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1450: GFLOPs: 4885.7028. Time: 21.0534 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1451: GFLOPs: 4636.5326. Time: 22.1849 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1452: GFLOPs: 4872.6396. Time: 21.1099 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1453: GFLOPs: 4875.5888. Time: 21.0971 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1454: GFLOPs: 4870.3799. Time: 21.1197 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1455: GFLOPs: 4137.4878. Time: 24.8607 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1456: GFLOPs: 4869.3105. Time: 21.1243 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1457: GFLOPs: 4369.9882. Time: 23.5380 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1458: GFLOPs: 4404.2936. Time: 23.3547 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1459: GFLOPs: 4871.9643. Time: 21.1128 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1460: GFLOPs: 4846.3457. Time: 21.2244 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1461: GFLOPs: 4860.2903. Time: 21.1635 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1462: GFLOPs: 4860.3761. Time: 21.1631 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1463: GFLOPs: 4365.7576. Time: 23.5608 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1464: GFLOPs: 4382.9616. Time: 23.4683 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1465: GFLOPs: 4663.7072. Time: 22.0556 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1466: GFLOPs: 4371.9122. Time: 23.5276 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1467: GFLOPs: 4152.3390. Time: 24.7718 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1468: GFLOPs: 3937.8785. Time: 26.1209 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1469: GFLOPs: 4517.9427. Time: 22.7672 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1470: GFLOPs: 200.3063. Time: 513.5176 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1471: GFLOPs: 2162.3121. Time: 47.5698 us. Best GFLOPs: 5025.5637
2024-03-22 07:08:37 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1472: GFLOPs: 1118.7303. Time: 91.9442 us. Best GFLOPs: 5025.5637
2024-03-22 07:11:30 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 07:11:31 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 07:11:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 384 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:11:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 771 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:11:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:11:32 [INFO] [evolutionary_search.cc:723] Sampled 74 candidate(s)
2024-03-22 07:11:34 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:11:36 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 134 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:11:39 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 120 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:11:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:11:42 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3763  1.3546  1.3460  1.3393  1.3174  1.1835  1.1605  1.1544  1.1472  1.1424  1.1222  1.1163  1.1110  1.1043  1.0897  1.0897
[17 : 32]:	1.0882  1.0776  1.0634  1.0622  1.0619  1.0516  1.0513  1.0300  1.0288  1.0143  0.9783  0.9763  0.9732  0.9695  0.9686  0.9672
[33 : 48]:	0.9655  0.9655  0.9642  0.9642  0.9625  0.9611  0.9608  0.9608  0.9589  0.9586  0.9566  0.9565  0.9565  0.9565  0.9557  0.9535
[49 : 64]:	0.9506  0.9504  0.9458  0.9458  0.9442  0.9437  0.9427  0.9422  0.9416  0.9416  0.9415  0.9411  0.9376  0.9372  0.9347  0.9344
2024-03-22 07:11:42 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 07:11:42 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1473: GFLOPs: 1037.5511. Time: 99.1381 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1474: GFLOPs: 1788.5517. Time: 57.5107 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1475: GFLOPs: 1467.6409. Time: 70.0858 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1476: GFLOPs: 1503.0457. Time: 68.4349 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1477: GFLOPs: 1775.9631. Time: 57.9183 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1478: GFLOPs: 1840.6653. Time: 55.8824 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1479: GFLOPs: 1440.6436. Time: 71.3992 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1480: GFLOPs: 913.5444. Time: 112.5953 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1481: GFLOPs: 889.5351. Time: 115.6343 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1482: GFLOPs: 1334.7446. Time: 77.0640 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1483: GFLOPs: 983.1157. Time: 104.6274 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1484: GFLOPs: 1329.0145. Time: 77.3963 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1485: GFLOPs: 1454.8306. Time: 70.7029 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1486: GFLOPs: 1494.4519. Time: 68.8284 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1487: GFLOPs: 1270.6277. Time: 80.9527 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1488: GFLOPs: 1312.3596. Time: 78.3785 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1489: GFLOPs: 1319.2659. Time: 77.9682 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1490: GFLOPs: 1214.8631. Time: 84.6686 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1491: GFLOPs: 1293.0785. Time: 79.5472 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1492: GFLOPs: 1313.1436. Time: 78.3317 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1493: GFLOPs: 1338.1794. Time: 76.8662 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1494: GFLOPs: 1227.3056. Time: 83.8103 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1495: GFLOPs: 1242.7638. Time: 82.7678 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1496: GFLOPs: 995.9732. Time: 103.2767 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1497: GFLOPs: 1223.9887. Time: 84.0374 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1498: GFLOPs: 1007.1023. Time: 102.1354 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1499: GFLOPs: 4675.3050. Time: 22.0009 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1500: GFLOPs: 4902.5416. Time: 20.9811 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1501: GFLOPs: 4901.9736. Time: 20.9835 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1502: GFLOPs: 4904.5106. Time: 20.9727 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1503: GFLOPs: 4916.3267. Time: 20.9223 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1504: GFLOPs: 4906.2911. Time: 20.9651 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1505: GFLOPs: 4903.5000. Time: 20.9770 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1506: GFLOPs: 4903.5586. Time: 20.9768 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1507: GFLOPs: 4855.7078. Time: 21.1835 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1508: GFLOPs: 4855.8422. Time: 21.1829 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1509: GFLOPs: 4902.6726. Time: 20.9806 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1510: GFLOPs: 4899.2975. Time: 20.9950 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1511: GFLOPs: 4575.3842. Time: 22.4813 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1512: GFLOPs: 4524.4487. Time: 22.7344 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1513: GFLOPs: 4857.7446. Time: 21.1746 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1514: GFLOPs: 4601.6655. Time: 22.3530 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1515: GFLOPs: 4858.2558. Time: 21.1724 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1516: GFLOPs: 4829.5954. Time: 21.2980 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1517: GFLOPs: 4829.4112. Time: 21.2988 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1518: GFLOPs: 4872.1857. Time: 21.1118 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1519: GFLOPs: 4624.4990. Time: 22.2426 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1520: GFLOPs: 4829.4493. Time: 21.2987 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1521: GFLOPs: 4855.3955. Time: 21.1848 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1522: GFLOPs: 4783.3017. Time: 21.5041 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1523: GFLOPs: 4739.3449. Time: 21.7036 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1524: GFLOPs: 4739.4716. Time: 21.7030 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1525: GFLOPs: 4713.2570. Time: 21.8237 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1526: GFLOPs: 4738.1442. Time: 21.7091 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1527: GFLOPs: 4736.2814. Time: 21.7176 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1528: GFLOPs: 4681.8550. Time: 21.9701 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1529: GFLOPs: 4829.6898. Time: 21.2976 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1530: GFLOPs: 4789.1761. Time: 21.4778 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1531: GFLOPs: 4712.0368. Time: 21.8294 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1532: GFLOPs: 4617.6481. Time: 22.2756 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1533: GFLOPs: 4797.7354. Time: 21.4394 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1534: GFLOPs: 38.7743. Time: 2652.8067 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1535: GFLOPs: 238.9694. Time: 430.4350 us. Best GFLOPs: 5025.5637
2024-03-22 07:12:36 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1536: GFLOPs: 436.3592. Time: 235.7251 us. Best GFLOPs: 5025.5637
2024-03-22 07:23:21 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 07:23:21 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 07:23:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 385 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:23:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 775 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:23:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:23:23 [INFO] [evolutionary_search.cc:723] Sampled 63 candidate(s)
2024-03-22 07:23:24 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:23:27 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:23:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 152 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:23:32 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:23:33 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1258  0.9719  0.9709  0.9709  0.9706  0.9696  0.9689  0.9687  0.9665  0.9656  0.9648  0.9640  0.9633  0.9624  0.9624  0.9601
[17 : 32]:	0.9592  0.9584  0.9579  0.9566  0.9562  0.9535  0.9530  0.9530  0.9530  0.9511  0.9494  0.9477  0.9440  0.9440  0.9421  0.9419
[33 : 48]:	0.9409  0.9409  0.9409  0.9399  0.9375  0.9368  0.9364  0.9354  0.9350  0.9338  0.9335  0.9329  0.9318  0.9293  0.9271  0.9264
[49 : 64]:	0.9247  0.9235  0.9222  0.9210  0.9208  0.9206  0.9182  0.9175  0.9172  0.9169  0.9169  0.9167  0.9164  0.9152  0.9151  0.9149
2024-03-22 07:23:33 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 07:23:33 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1537: GFLOPs: 2126.9990. Time: 48.3596 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1538: GFLOPs: 4946.8315. Time: 20.7933 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1539: GFLOPs: 4932.7129. Time: 20.8528 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1540: GFLOPs: 4938.8115. Time: 20.8270 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1541: GFLOPs: 4886.0455. Time: 21.0520 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1542: GFLOPs: 4932.8791. Time: 20.8521 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1543: GFLOPs: 4935.1719. Time: 20.8424 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1544: GFLOPs: 4903.0066. Time: 20.9791 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1545: GFLOPs: 4913.7219. Time: 20.9334 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1546: GFLOPs: 4896.7848. Time: 21.0058 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1547: GFLOPs: 4904.8481. Time: 20.9713 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1548: GFLOPs: 4904.9911. Time: 20.9706 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1549: GFLOPs: 4903.5363. Time: 20.9769 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1550: GFLOPs: 4903.3861. Time: 20.9775 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1551: GFLOPs: 4903.4603. Time: 20.9772 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1552: GFLOPs: 4903.4713. Time: 20.9771 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1553: GFLOPs: 4901.9808. Time: 20.9835 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1554: GFLOPs: 4901.7214. Time: 20.9846 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1555: GFLOPs: 4860.4840. Time: 21.1627 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1556: GFLOPs: 4858.6777. Time: 21.1705 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1557: GFLOPs: 4858.0066. Time: 21.1735 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1558: GFLOPs: 4890.2691. Time: 21.0338 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1559: GFLOPs: 4858.6855. Time: 21.1705 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1560: GFLOPs: 4858.4614. Time: 21.1715 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1561: GFLOPs: 4858.4210. Time: 21.1717 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1562: GFLOPs: 4765.9541. Time: 21.5824 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1563: GFLOPs: 4737.7871. Time: 21.7107 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1564: GFLOPs: 4738.9980. Time: 21.7052 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1565: GFLOPs: 4800.6487. Time: 21.4264 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1566: GFLOPs: 4800.5531. Time: 21.4269 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1567: GFLOPs: 4796.9463. Time: 21.4430 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1568: GFLOPs: 4699.9612. Time: 21.8855 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1569: GFLOPs: 4731.4655. Time: 21.7397 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1570: GFLOPs: 4736.9214. Time: 21.7147 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1571: GFLOPs: 4736.6090. Time: 21.7161 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1572: GFLOPs: 4735.8195. Time: 21.7197 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1573: GFLOPs: 4729.3788. Time: 21.7493 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1574: GFLOPs: 4736.1361. Time: 21.7183 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1575: GFLOPs: 4740.9269. Time: 21.6963 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1576: GFLOPs: 4730.5199. Time: 21.7441 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1577: GFLOPs: 4661.3784. Time: 22.0666 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1578: GFLOPs: 4110.7661. Time: 25.0223 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1579: GFLOPs: 4791.5847. Time: 21.4670 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1580: GFLOPs: 4695.3895. Time: 21.9068 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1581: GFLOPs: 4720.8335. Time: 21.7887 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1582: GFLOPs: 4708.8539. Time: 21.8441 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1583: GFLOPs: 4722.5641. Time: 21.7807 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1584: GFLOPs: 4708.9416. Time: 21.8437 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1585: GFLOPs: 3326.5707. Time: 30.9210 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1586: GFLOPs: 4757.7815. Time: 21.6195 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1587: GFLOPs: 4621.7836. Time: 22.2557 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1588: GFLOPs: 4711.1950. Time: 21.8333 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1589: GFLOPs: 4704.6827. Time: 21.8635 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1590: GFLOPs: 4638.8362. Time: 22.1738 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1591: GFLOPs: 4660.1996. Time: 22.0722 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1592: GFLOPs: 4638.8169. Time: 22.1739 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1593: GFLOPs: 4756.6329. Time: 21.6247 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1594: GFLOPs: 3829.9617. Time: 26.8569 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1595: GFLOPs: 4565.0633. Time: 22.5322 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1596: GFLOPs: 4756.2760. Time: 21.6263 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1597: GFLOPs: 4323.6363. Time: 23.7903 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1598: GFLOPs: 137.4407. Time: 748.4011 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1599: GFLOPs: 2594.8707. Time: 39.6400 us. Best GFLOPs: 5025.5637
2024-03-22 07:24:28 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1600: GFLOPs: 129.3718. Time: 795.0791 us. Best GFLOPs: 5025.5637
2024-03-22 07:27:50 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 07:27:50 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 07:27:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 388 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:27:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 779 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:27:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 1160 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:27:52 [INFO] [evolutionary_search.cc:723] Sampled 70 candidate(s)
2024-03-22 07:27:53 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 172 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:27:56 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:27:58 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:28:01 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x64ac4c4c3018)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x64ac60b0b338)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x64ac53cd0a98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x64ac5aac0d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x64ac63b0ffd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x64ac557f5c48)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x64ac60b0b238)]: 0 failure(s)
2024-03-22 07:28:02 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0204  0.9794  0.9791  0.9754  0.9754  0.9750  0.9749  0.9743  0.9739  0.9730  0.9729  0.9729  0.9718  0.9692  0.9682  0.9682
[17 : 32]:	0.9662  0.9646  0.9645  0.9642  0.9642  0.9635  0.9615  0.9568  0.9567  0.9535  0.9494  0.9475  0.9475  0.9417  0.9399  0.9333
[33 : 48]:	0.9327  0.9325  0.9319  0.9318  0.9318  0.9315  0.9308  0.9307  0.9283  0.9282  0.9280  0.9277  0.9277  0.9237  0.9236  0.9235
[49 : 64]:	0.9232  0.9227  0.9225  0.9217  0.9201  0.9201  0.9201  0.9200  0.9184  0.9183  0.9183  0.9180  0.9172  0.9172  0.9167  0.9154
2024-03-22 07:28:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 07:28:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1601: GFLOPs: 4650.1754. Time: 22.1198 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1602: GFLOPs: 4941.2000. Time: 20.8170 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1603: GFLOPs: 4951.3416. Time: 20.7743 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1604: GFLOPs: 4950.7870. Time: 20.7767 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1605: GFLOPs: 4927.3654. Time: 20.8754 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1606: GFLOPs: 4909.2648. Time: 20.9524 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1607: GFLOPs: 4038.3864. Time: 25.4708 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1608: GFLOPs: 4932.3145. Time: 20.8545 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1609: GFLOPs: 4885.7639. Time: 21.0532 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1610: GFLOPs: 4932.1736. Time: 20.8551 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1611: GFLOPs: 4886.1592. Time: 21.0515 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1612: GFLOPs: 4885.9960. Time: 21.0522 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1613: GFLOPs: 4940.1086. Time: 20.8216 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1614: GFLOPs: 4942.3261. Time: 20.8122 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1615: GFLOPs: 4942.4082. Time: 20.8119 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1616: GFLOPs: 4942.4667. Time: 20.8116 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1617: GFLOPs: 4928.9945. Time: 20.8685 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1618: GFLOPs: 4926.6866. Time: 20.8783 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1619: GFLOPs: 4928.3989. Time: 20.8710 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1620: GFLOPs: 4883.8536. Time: 21.0614 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1621: GFLOPs: 4884.0744. Time: 21.0604 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1622: GFLOPs: 4898.3149. Time: 20.9992 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1623: GFLOPs: 4884.3674. Time: 21.0592 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1624: GFLOPs: 4615.8412. Time: 22.2843 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1625: GFLOPs: 4856.6331. Time: 21.1794 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1626: GFLOPs: 4857.8983. Time: 21.1739 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1627: GFLOPs: 4767.1303. Time: 21.5771 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1628: GFLOPs: 4768.1874. Time: 21.5723 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1629: GFLOPs: 4768.6017. Time: 21.5704 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1630: GFLOPs: 4730.1973. Time: 21.7456 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1631: GFLOPs: 4433.3996. Time: 23.2013 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1632: GFLOPs: 4734.1992. Time: 21.7272 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1633: GFLOPs: 4811.5347. Time: 21.3780 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1634: GFLOPs: 4731.3625. Time: 21.7402 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1635: GFLOPs: 4026.5592. Time: 25.5456 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1636: GFLOPs: 4663.9407. Time: 22.0545 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1637: GFLOPs: 4663.5017. Time: 22.0566 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1638: GFLOPs: 4736.8376. Time: 21.7151 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1639: GFLOPs: 4749.4803. Time: 21.6573 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1640: GFLOPs: 4734.0838. Time: 21.7277 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1641: GFLOPs: 4371.4595. Time: 23.5301 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1642: GFLOPs: 4438.0267. Time: 23.1771 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1643: GFLOPs: 4734.5145. Time: 21.7257 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1644: GFLOPs: 4338.0037. Time: 23.7116 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1645: GFLOPs: 4301.8392. Time: 23.9109 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1646: GFLOPs: 4243.6002. Time: 24.2390 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1647: GFLOPs: 4703.1466. Time: 21.8706 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1648: GFLOPs: 4644.4948. Time: 22.1468 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1649: GFLOPs: 4637.9922. Time: 22.1779 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1650: GFLOPs: 4640.9931. Time: 22.1635 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1651: GFLOPs: 4734.1777. Time: 21.7273 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1652: GFLOPs: 4198.8919. Time: 24.4971 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1653: GFLOPs: 4667.2507. Time: 22.0388 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1654: GFLOPs: 4667.8163. Time: 22.0362 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1655: GFLOPs: 4667.3439. Time: 22.0384 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1656: GFLOPs: 4646.6938. Time: 22.1363 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1657: GFLOPs: 4337.7974. Time: 23.7127 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1658: GFLOPs: 4340.2153. Time: 23.6995 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1659: GFLOPs: 4644.2263. Time: 22.1481 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1660: GFLOPs: 4613.8872. Time: 22.2937 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1661: GFLOPs: 4667.8585. Time: 22.0360 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1662: GFLOPs: 245.9467. Time: 418.2240 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1663: GFLOPs: 783.2196. Time: 131.3307 us. Best GFLOPs: 5025.5637
2024-03-22 07:28:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1664: GFLOPs: 1407.2416. Time: 73.0939 us. Best GFLOPs: 5025.5637
