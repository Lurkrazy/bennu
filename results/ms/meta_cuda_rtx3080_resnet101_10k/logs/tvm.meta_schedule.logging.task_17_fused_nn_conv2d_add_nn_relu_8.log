2024-03-22 00:52:26 [INFO] [task_scheduler.cc:160] Initializing Task #17: "fused_nn_conv2d_add_nn_relu_8"
2024-03-22 00:52:26 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)))
        conv2d_nchw = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1024), T.int64(14), T.int64(14)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = p0[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(256), T.int64(14), T.int64(14), T.int64(1024), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1[v_ff, v_rc, v_ry, v_rx])
                T.writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(14), T.int64(14)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(14), T.int64(14)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2024-03-22 00:52:26 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-03-22 00:52:26 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(784), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for rc_0, ry_0, rx_0 in T.grid(T.int64(4), T.int64(1), T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(1024)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(4) // T.int64(2))
                                    v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(2))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(16) + ax0_ax1_ax2_ax3_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(256))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(256) + rc_1 * T.int64(16) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(4) + ax1)
                                v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 4, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 16, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
2024-03-22 00:52:26 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(784), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(4), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(1024)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(256) + ax0_ax1_ax2_ax3_fused // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(4) // T.int64(2))
                                    v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(2))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(16) + ax0_ax1_ax2_ax3_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(256))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(256) + rc_1 * T.int64(16) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(4) + ax1)
                                v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 4, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 16, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
2024-03-22 00:52:26 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(784), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(4), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(1024)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(256) + ax0_ax1_ax2_ax3_fused // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(4) // T.int64(2))
                                    v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(2))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(16) + ax0_ax1_ax2_ax3_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(256))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(256) + rc_1 * T.int64(16) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(49) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(4) + ax1)
                                v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(49) // T.int64(7) * T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 4, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 16, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
2024-03-22 01:13:10 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 01:13:10 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-03-22 01:13:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 492 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:13:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 975 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:13:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1465 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:13:12 [INFO] [evolutionary_search.cc:723] Sampled 71 candidate(s)
2024-03-22 01:13:13 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 93 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:13:15 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 73 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:13:16 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 71 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:13:17 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 81 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:13:18 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9994  0.9992  0.9991  0.9987  0.9972  0.9968  0.9961  0.9958  0.9953  0.9948  0.9946  0.9936  0.9934  0.9930  0.9918  0.9912
[17 : 32]:	0.9889  0.9878  0.9877  0.9875  0.9872  0.9866  0.9848  0.9844  0.9843  0.9843  0.9809  0.9800  0.9799  0.9795  0.9790  0.9746
[33 : 48]:	0.9737  0.9713  0.9708  0.9689  0.9688  0.9687  0.9687  0.9684  0.9675  0.9674  0.9664  0.9661  0.9658  0.9653  0.9650  0.9643
[49 : 64]:	0.9632  0.9626  0.9595  0.9594  0.9589  0.9587  0.9584  0.9576  0.9575  0.9572  0.9572  0.9563  0.9562  0.9560  0.9554  0.9550
2024-03-22 01:13:18 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 01:13:18 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1: GFLOPs: 635.6075. Time: 161.8307 us. Best GFLOPs: 635.6075
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #2: GFLOPs: 52.0763. Time: 1975.1956 us. Best GFLOPs: 635.6075
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #3: GFLOPs: 516.4672. Time: 199.1623 us. Best GFLOPs: 635.6075
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #4: GFLOPs: 226.9998. Time: 453.1316 us. Best GFLOPs: 635.6075
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #5: GFLOPs: 915.4232. Time: 112.3642 us. Best GFLOPs: 915.4232
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #6: GFLOPs: 1024.2797. Time: 100.4226 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #7: GFLOPs: 252.6295. Time: 407.1607 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #8: GFLOPs: 554.6123. Time: 185.4643 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #9: GFLOPs: 57.2592. Time: 1796.4069 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #10: GFLOPs: 856.1425. Time: 120.1445 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #11: GFLOPs: 225.8272. Time: 455.4845 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #12: GFLOPs: 259.4388. Time: 396.4742 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #13: GFLOPs: 836.6014. Time: 122.9508 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #14: GFLOPs: 546.3198. Time: 188.2795 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #15: GFLOPs: 975.8628. Time: 105.4050 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #16: GFLOPs: 364.1572. Time: 282.4626 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #17: GFLOPs: 452.7050. Time: 227.2137 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #18: GFLOPs: 98.9897. Time: 1039.1057 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #19: GFLOPs: 13.2277. Time: 7776.1770 us. Best GFLOPs: 1024.2797
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #20: GFLOPs: 1068.4188. Time: 96.2739 us. Best GFLOPs: 1068.4188
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #21: GFLOPs: 1195.6489. Time: 86.0293 us. Best GFLOPs: 1195.6489
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #22: GFLOPs: 1468.1496. Time: 70.0615 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #23: GFLOPs: 71.3677. Time: 1441.2800 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #24: GFLOPs: 413.7213. Time: 248.6234 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #25: GFLOPs: 227.6794. Time: 451.7791 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #26: GFLOPs: 669.5342. Time: 153.6304 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #27: GFLOPs: 32.5351. Time: 3161.5360 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #28: GFLOPs: 50.4759. Time: 2037.8215 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #29: GFLOPs: 409.2747. Time: 251.3246 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #30: GFLOPs: 45.0963. Time: 2280.9134 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #31: GFLOPs: 45.4360. Time: 2263.8592 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #32: GFLOPs: 1334.1381. Time: 77.0991 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #33: GFLOPs: 101.6711. Time: 1011.7016 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #34: GFLOPs: 141.7219. Time: 725.7934 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #35: GFLOPs: 15.2389. Time: 6749.8667 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #36: GFLOPs: 344.4262. Time: 298.6439 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #37: GFLOPs: 872.0832. Time: 117.9484 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #38: GFLOPs: 9.5033. Time: 10823.6801 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #39: GFLOPs: 352.5546. Time: 291.7585 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #40: GFLOPs: 525.9330. Time: 195.5778 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #41: GFLOPs: 761.3790. Time: 135.0980 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #42: GFLOPs: 54.0970. Time: 1901.4134 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #43: GFLOPs: 471.0112. Time: 218.3829 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #44: GFLOPs: 1010.1197. Time: 101.8303 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #45: GFLOPs: 46.8336. Time: 2196.3018 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #46: GFLOPs: 445.3398. Time: 230.9715 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #47: GFLOPs: 550.1936. Time: 186.9538 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #48: GFLOPs: 29.1451. Time: 3529.2690 us. Best GFLOPs: 1468.1496
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #49: GFLOPs: 3106.8833. Time: 33.1074 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #50: GFLOPs: 514.9515. Time: 199.7485 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #51: GFLOPs: 1452.5208. Time: 70.8154 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #52: GFLOPs: 171.1472. Time: 601.0078 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #53: GFLOPs: 937.5959. Time: 109.7070 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #54: GFLOPs: 381.6674. Time: 269.5038 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #55: GFLOPs: 418.5789. Time: 245.7381 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #56: GFLOPs: 183.1242. Time: 561.6995 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #57: GFLOPs: 74.9847. Time: 1371.7578 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #58: GFLOPs: 871.2250. Time: 118.0646 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #59: GFLOPs: 831.2626. Time: 123.7404 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #60: GFLOPs: 1072.9462. Time: 95.8676 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #61: GFLOPs: 290.0326. Time: 354.6526 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #62: GFLOPs: 126.6604. Time: 812.0989 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #63: GFLOPs: 1004.8349. Time: 102.3659 us. Best GFLOPs: 3106.8833
2024-03-22 01:24:23 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #64: GFLOPs: 1811.6245. Time: 56.7782 us. Best GFLOPs: 3106.8833
2024-03-22 01:26:20 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 01:26:20 [INFO] [evolutionary_search.cc:715] Picked top 64 candidate(s) from database
2024-03-22 01:26:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 423 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:26:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 840 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:26:22 [INFO] [evolutionary_search.cc:723] Sampled 56 candidate(s)
2024-03-22 01:26:23 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 81 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:26:25 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 86 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:26:27 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 86 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:26:30 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 72 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:26:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.9845  1.9538  1.9438  1.9417  1.7871  1.7816  1.7310  1.7043  1.6931  1.6860  1.6854  1.6854  1.6628  1.6624  1.6623  1.6543
[17 : 32]:	1.6516  1.6508  1.6490  1.6406  1.6374  1.6262  1.6229  1.6206  1.6198  1.6109  1.6109  1.5917  1.5914  1.5891  1.5856  1.5823
[33 : 48]:	1.5741  1.5741  1.5739  1.5707  1.5674  1.5650  1.5585  1.5474  1.5463  1.5274  1.5193  1.5192  1.5192  1.5118  1.4693  1.4644
[49 : 64]:	1.4634  1.4606  1.4601  1.4574  1.4504  1.4486  1.4482  1.4457  1.4426  1.4396  1.4376  1.4339  1.4299  1.4158  1.4099  1.4097
2024-03-22 01:26:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 01:26:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #65: GFLOPs: 2156.4904. Time: 47.6982 us. Best GFLOPs: 3106.8833
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #66: GFLOPs: 1992.7018. Time: 51.6188 us. Best GFLOPs: 3106.8833
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #67: GFLOPs: 2173.5211. Time: 47.3245 us. Best GFLOPs: 3106.8833
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #68: GFLOPs: 2054.9298. Time: 50.0556 us. Best GFLOPs: 3106.8833
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #69: GFLOPs: 3368.0445. Time: 30.5402 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #70: GFLOPs: 1536.4079. Time: 66.9489 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #71: GFLOPs: 1980.5262. Time: 51.9361 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #72: GFLOPs: 1568.1836. Time: 65.5923 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #73: GFLOPs: 2142.2456. Time: 48.0154 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #74: GFLOPs: 1900.6511. Time: 54.1187 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #75: GFLOPs: 2263.5366. Time: 45.4425 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #76: GFLOPs: 1586.2838. Time: 64.8439 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #77: GFLOPs: 1977.1062. Time: 52.0259 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #78: GFLOPs: 1976.8630. Time: 52.0323 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #79: GFLOPs: 1978.9511. Time: 51.9774 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #80: GFLOPs: 1658.7691. Time: 62.0103 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #81: GFLOPs: 1513.3309. Time: 67.9698 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #82: GFLOPs: 1517.7304. Time: 67.7728 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #83: GFLOPs: 1464.6526. Time: 70.2288 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #84: GFLOPs: 2419.8285. Time: 42.5075 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #85: GFLOPs: 1433.8631. Time: 71.7368 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #86: GFLOPs: 1432.8103. Time: 71.7895 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #87: GFLOPs: 1753.0132. Time: 58.6766 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #88: GFLOPs: 1521.8939. Time: 67.5874 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #89: GFLOPs: 1710.3564. Time: 60.1400 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #90: GFLOPs: 1710.4249. Time: 60.1376 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #91: GFLOPs: 1710.4580. Time: 60.1364 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #92: GFLOPs: 1433.5192. Time: 71.7540 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #93: GFLOPs: 1908.3785. Time: 53.8996 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #94: GFLOPs: 1710.3458. Time: 60.1404 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #95: GFLOPs: 1710.2014. Time: 60.1454 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #96: GFLOPs: 1410.0931. Time: 72.9461 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #97: GFLOPs: 1710.4559. Time: 60.1365 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #98: GFLOPs: 1710.3012. Time: 60.1419 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #99: GFLOPs: 1710.2790. Time: 60.1427 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #100: GFLOPs: 1428.9157. Time: 71.9852 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #101: GFLOPs: 1678.6062. Time: 61.2775 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #102: GFLOPs: 1464.9553. Time: 70.2143 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #103: GFLOPs: 1464.8810. Time: 70.2179 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #104: GFLOPs: 2575.8913. Time: 39.9321 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #105: GFLOPs: 1678.6519. Time: 61.2758 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #106: GFLOPs: 1627.5186. Time: 63.2010 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #107: GFLOPs: 1464.9617. Time: 70.2140 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #108: GFLOPs: 1464.9377. Time: 70.2151 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #109: GFLOPs: 1464.9590. Time: 70.2141 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #110: GFLOPs: 1533.4594. Time: 67.0776 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #111: GFLOPs: 2005.1007. Time: 51.2996 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #112: GFLOPs: 1637.4791. Time: 62.8166 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #113: GFLOPs: 2005.1447. Time: 51.2984 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #114: GFLOPs: 1130.2056. Time: 91.0107 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #115: GFLOPs: 2287.2342. Time: 44.9717 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #116: GFLOPs: 1631.8709. Time: 63.0324 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #117: GFLOPs: 1517.2991. Time: 67.7920 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #118: GFLOPs: 1638.9316. Time: 62.7609 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #119: GFLOPs: 1972.0692. Time: 52.1588 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #120: GFLOPs: 2859.9687. Time: 35.9657 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #121: GFLOPs: 2361.8448. Time: 43.5510 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #122: GFLOPs: 1964.4256. Time: 52.3618 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #123: GFLOPs: 3018.4667. Time: 34.0772 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #124: GFLOPs: 1919.5680. Time: 53.5854 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #125: GFLOPs: 2055.1354. Time: 50.0506 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #126: GFLOPs: 1831.8183. Time: 56.1523 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #127: GFLOPs: 81.6235. Time: 1260.1856 us. Best GFLOPs: 3368.0445
2024-03-22 01:27:19 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #128: GFLOPs: 1174.3385. Time: 87.5904 us. Best GFLOPs: 3368.0445
2024-03-22 01:30:53 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 01:30:53 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 01:30:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 387 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:30:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 774 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:30:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1163 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:30:55 [INFO] [evolutionary_search.cc:723] Sampled 67 candidate(s)
2024-03-22 01:30:56 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 71 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:30:58 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 55 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:31:00 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 62 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:31:02 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 72 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:31:03 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.4293  1.3816  1.3418  1.3343  1.3290  1.3256  1.3256  1.3239  1.3218  1.2916  1.2908  1.2740  1.2635  1.2588  1.2351  1.2351
[17 : 32]:	1.2132  1.1747  1.1495  1.1341  1.1269  1.1225  1.1218  1.1103  1.1083  1.1043  1.0954  1.0942  1.0918  1.0878  1.0854  1.0851
[33 : 48]:	1.0822  1.0815  1.0792  1.0792  1.0737  1.0710  1.0679  1.0660  1.0538  1.0537  1.0534  1.0531  1.0490  1.0482  1.0408  1.0389
[49 : 64]:	1.0384  1.0379  1.0346  1.0337  1.0322  1.0292  1.0292  1.0270  1.0211  1.0178  1.0145  1.0125  1.0114  1.0095  1.0083  1.0042
2024-03-22 01:31:03 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 01:31:03 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #129: GFLOPs: 2014.5850. Time: 51.0581 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #130: GFLOPs: 1949.0593. Time: 52.7746 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #131: GFLOPs: 2697.9710. Time: 38.1252 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #132: GFLOPs: 2815.4451. Time: 36.5345 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #133: GFLOPs: 2165.1167. Time: 47.5082 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #134: GFLOPs: 1923.9914. Time: 53.4622 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #135: GFLOPs: 2815.6775. Time: 36.5315 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #136: GFLOPs: 2432.6073. Time: 42.2842 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #137: GFLOPs: 1682.0791. Time: 61.1510 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #138: GFLOPs: 1913.3825. Time: 53.7586 us. Best GFLOPs: 3368.0445
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #139: GFLOPs: 3505.5317. Time: 29.3424 us. Best GFLOPs: 3505.5317
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #140: GFLOPs: 2624.7984. Time: 39.1881 us. Best GFLOPs: 3505.5317
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #141: GFLOPs: 3897.9693. Time: 26.3883 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #142: GFLOPs: 1988.9599. Time: 51.7159 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #143: GFLOPs: 1107.4108. Time: 92.8840 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #144: GFLOPs: 1107.4944. Time: 92.8770 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #145: GFLOPs: 3019.1120. Time: 34.0699 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #146: GFLOPs: 2094.9420. Time: 49.0996 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #147: GFLOPs: 1907.9838. Time: 53.9107 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #148: GFLOPs: 2322.4507. Time: 44.2898 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #149: GFLOPs: 2540.2909. Time: 40.4917 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #150: GFLOPs: 2594.3470. Time: 39.6480 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #151: GFLOPs: 2529.3051. Time: 40.6676 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #152: GFLOPs: 1905.1098. Time: 53.9921 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #153: GFLOPs: 1657.5716. Time: 62.0551 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #154: GFLOPs: 2582.6990. Time: 39.8269 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #155: GFLOPs: 2819.9151. Time: 36.4766 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #156: GFLOPs: 1434.8896. Time: 71.6855 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #157: GFLOPs: 2282.9397. Time: 45.0563 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #158: GFLOPs: 2124.3579. Time: 48.4197 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #159: GFLOPs: 2111.9103. Time: 48.7051 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #160: GFLOPs: 2269.8524. Time: 45.3161 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #161: GFLOPs: 1556.7921. Time: 66.0723 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #162: GFLOPs: 2382.1018. Time: 43.1807 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #163: GFLOPs: 1420.6144. Time: 72.4059 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #164: GFLOPs: 1484.1317. Time: 69.3071 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #165: GFLOPs: 1426.4478. Time: 72.1098 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #166: GFLOPs: 1576.2278. Time: 65.2576 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #167: GFLOPs: 3270.2860. Time: 31.4532 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #168: GFLOPs: 2614.9114. Time: 39.3362 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #169: GFLOPs: 2494.0180. Time: 41.2430 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #170: GFLOPs: 749.9000. Time: 137.1660 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #171: GFLOPs: 1401.5627. Time: 73.3901 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #172: GFLOPs: 1410.0260. Time: 72.9496 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #173: GFLOPs: 1974.4408. Time: 52.0962 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #174: GFLOPs: 2577.2802. Time: 39.9106 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #175: GFLOPs: 1470.0305. Time: 69.9719 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #176: GFLOPs: 1592.0188. Time: 64.6103 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #177: GFLOPs: 2711.6618. Time: 37.9328 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #178: GFLOPs: 2069.7686. Time: 49.6968 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #179: GFLOPs: 2725.4656. Time: 37.7406 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #180: GFLOPs: 1995.9502. Time: 51.5348 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #181: GFLOPs: 2058.6126. Time: 49.9661 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #182: GFLOPs: 2600.8752. Time: 39.5485 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #183: GFLOPs: 2601.0446. Time: 39.5460 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #184: GFLOPs: 1420.9558. Time: 72.3885 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #185: GFLOPs: 404.6008. Time: 254.2279 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #186: GFLOPs: 2618.6330. Time: 39.2803 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #187: GFLOPs: 1493.2506. Time: 68.8838 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #188: GFLOPs: 1914.4605. Time: 53.7283 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #189: GFLOPs: 1459.4155. Time: 70.4808 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #190: GFLOPs: 757.2176. Time: 135.8405 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #191: GFLOPs: 500.9481. Time: 205.3323 us. Best GFLOPs: 3897.9693
2024-03-22 01:31:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #192: GFLOPs: 25.6771. Time: 4005.9289 us. Best GFLOPs: 3897.9693
2024-03-22 01:34:39 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 01:34:39 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 01:34:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 389 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:34:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 778 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:34:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1163 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:34:40 [INFO] [evolutionary_search.cc:723] Sampled 67 candidate(s)
2024-03-22 01:34:42 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 80 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:34:44 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 85 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:34:46 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 75 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:34:48 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 78 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:34:49 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	2.1110  2.0351  2.0351  2.0335  2.0335  2.0199  1.9852  1.9852  1.9723  1.9716  1.9587  1.9536  1.9524  1.9519  1.9432  1.9388
[17 : 32]:	1.9210  1.9140  1.7424  1.6929  1.6929  1.6924  1.6894  1.6894  1.6892  1.6892  1.6392  1.6090  1.5726  1.5622  1.5326  1.4243
[33 : 48]:	1.4166  1.4142  1.4102  1.3905  1.3582  1.3443  1.3377  1.3196  1.3183  1.2984  1.2980  1.2980  1.2918  1.2914  1.2897  1.2867
[49 : 64]:	1.2845  1.2827  1.2821  1.2798  1.2762  1.2716  1.2680  1.2675  1.2671  1.2474  1.2401  1.2368  1.2266  1.2212  1.2211  1.2160
2024-03-22 01:34:49 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 01:34:49 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #193: GFLOPs: 1316.9349. Time: 78.1062 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #194: GFLOPs: 1597.0645. Time: 64.4062 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #195: GFLOPs: 1585.1182. Time: 64.8916 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #196: GFLOPs: 1572.9981. Time: 65.3916 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #197: GFLOPs: 1584.8260. Time: 64.9035 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #198: GFLOPs: 1552.6734. Time: 66.2475 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #199: GFLOPs: 1456.2597. Time: 70.6336 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #200: GFLOPs: 1453.0158. Time: 70.7912 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #201: GFLOPs: 1414.8895. Time: 72.6988 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #202: GFLOPs: 1449.6458. Time: 70.9558 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #203: GFLOPs: 1065.5733. Time: 96.5309 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #204: GFLOPs: 1431.4878. Time: 71.8559 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #205: GFLOPs: 1620.8177. Time: 63.4623 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #206: GFLOPs: 1620.6692. Time: 63.4681 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #207: GFLOPs: 1521.4197. Time: 67.6084 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #208: GFLOPs: 1565.7071. Time: 65.6961 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #209: GFLOPs: 1456.1227. Time: 70.6402 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #210: GFLOPs: 1312.1959. Time: 78.3883 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #211: GFLOPs: 2128.8300. Time: 48.3180 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #212: GFLOPs: 1626.6304. Time: 63.2355 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #213: GFLOPs: 1647.0714. Time: 62.4507 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #214: GFLOPs: 1708.0432. Time: 60.2214 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #215: GFLOPs: 1704.5608. Time: 60.3445 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #216: GFLOPs: 1617.7028. Time: 63.5845 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #217: GFLOPs: 1604.9143. Time: 64.0911 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #218: GFLOPs: 1542.7912. Time: 66.6719 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #219: GFLOPs: 1477.5457. Time: 69.6160 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #220: GFLOPs: 1689.2311. Time: 60.8921 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #221: GFLOPs: 1173.3558. Time: 87.6638 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #222: GFLOPs: 1621.3391. Time: 63.4419 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #223: GFLOPs: 1392.5719. Time: 73.8639 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #224: GFLOPs: 1188.8869. Time: 86.5186 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #225: GFLOPs: 1104.0009. Time: 93.1709 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #226: GFLOPs: 1081.0315. Time: 95.1506 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #227: GFLOPs: 1132.1282. Time: 90.8561 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #228: GFLOPs: 882.4495. Time: 116.5628 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #229: GFLOPs: 882.1683. Time: 116.6000 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #230: GFLOPs: 1648.2081. Time: 62.4077 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #231: GFLOPs: 1643.6399. Time: 62.5811 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #232: GFLOPs: 682.9716. Time: 150.6077 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #233: GFLOPs: 1727.2279. Time: 59.5525 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #234: GFLOPs: 1717.5983. Time: 59.8864 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #235: GFLOPs: 1884.6057. Time: 54.5795 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #236: GFLOPs: 1884.5338. Time: 54.5816 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #237: GFLOPs: 1671.9610. Time: 61.5211 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #238: GFLOPs: 1700.4105. Time: 60.4917 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #239: GFLOPs: 1814.8189. Time: 56.6783 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #240: GFLOPs: 1814.8122. Time: 56.6785 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #241: GFLOPs: 1884.5997. Time: 54.5797 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #242: GFLOPs: 1673.2135. Time: 61.4750 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #243: GFLOPs: 1644.6397. Time: 62.5431 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #244: GFLOPs: 1673.3255. Time: 61.4709 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #245: GFLOPs: 1816.7299. Time: 56.6187 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #246: GFLOPs: 1590.4718. Time: 64.6731 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #247: GFLOPs: 1750.6457. Time: 58.7559 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #248: GFLOPs: 1619.3836. Time: 63.5185 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #249: GFLOPs: 1689.2593. Time: 60.8911 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #250: GFLOPs: 1710.8371. Time: 60.1231 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #251: GFLOPs: 1668.7089. Time: 61.6409 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #252: GFLOPs: 1700.2237. Time: 60.4984 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #253: GFLOPs: 1790.5407. Time: 57.4468 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #254: GFLOPs: 249.2270. Time: 412.7194 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #255: GFLOPs: 630.8811. Time: 163.0431 us. Best GFLOPs: 3897.9693
2024-03-22 01:35:35 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #256: GFLOPs: 1493.2961. Time: 68.8817 us. Best GFLOPs: 3897.9693
2024-03-22 01:37:23 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 01:37:24 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 01:37:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 392 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:37:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 777 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:37:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1162 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:37:25 [INFO] [evolutionary_search.cc:723] Sampled 68 candidate(s)
2024-03-22 01:37:27 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 81 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:37:29 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 88 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:37:31 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:37:33 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 82 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:37:34 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3242  1.3128  1.2887  1.2689  1.2689  1.2584  1.2518  1.2485  1.2442  1.2412  1.2412  1.2409  1.2349  1.2322  1.2314  1.2307
[17 : 32]:	1.2208  1.1927  1.1814  1.1782  1.1772  1.1732  1.1512  1.1373  1.1345  1.1250  1.1157  1.0771  1.0684  1.0443  1.0331  1.0331
[33 : 48]:	1.0238  1.0144  1.0115  1.0086  1.0081  1.0079  1.0079  1.0040  0.9991  0.9797  0.9793  0.9754  0.9706  0.9584  0.9535  0.9448
[49 : 64]:	0.9350  0.9350  0.9337  0.9336  0.9328  0.9294  0.9263  0.9258  0.9235  0.9197  0.9169  0.9140  0.9105  0.9074  0.9040  0.9033
2024-03-22 01:37:34 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 01:37:34 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #257: GFLOPs: 2073.9398. Time: 49.5968 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #258: GFLOPs: 1882.6447. Time: 54.6363 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #259: GFLOPs: 2052.4491. Time: 50.1161 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #260: GFLOPs: 2137.8037. Time: 48.1152 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #261: GFLOPs: 2128.6029. Time: 48.3232 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #262: GFLOPs: 2102.3531. Time: 48.9265 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #263: GFLOPs: 2002.7950. Time: 51.3586 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #264: GFLOPs: 2128.5396. Time: 48.3246 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #265: GFLOPs: 2074.7601. Time: 49.5772 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #266: GFLOPs: 1976.9655. Time: 52.0296 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #267: GFLOPs: 1978.8743. Time: 51.9795 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #268: GFLOPs: 2134.5714. Time: 48.1880 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #269: GFLOPs: 1975.1121. Time: 52.0785 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #270: GFLOPs: 2459.7135. Time: 41.8182 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #271: GFLOPs: 1999.6101. Time: 51.4404 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #272: GFLOPs: 1987.6349. Time: 51.7504 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #273: GFLOPs: 2002.7685. Time: 51.3593 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #274: GFLOPs: 2213.6119. Time: 46.4674 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #275: GFLOPs: 1984.0815. Time: 51.8430 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #276: GFLOPs: 2153.6058. Time: 47.7621 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #277: GFLOPs: 1101.3972. Time: 93.3912 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #278: GFLOPs: 3437.0139. Time: 29.9274 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #279: GFLOPs: 3507.3700. Time: 29.3270 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #280: GFLOPs: 3365.2477. Time: 30.5656 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #281: GFLOPs: 3365.6744. Time: 30.5617 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #282: GFLOPs: 2831.4685. Time: 36.3277 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #283: GFLOPs: 1107.0326. Time: 92.9158 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #284: GFLOPs: 2031.6835. Time: 50.6284 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #285: GFLOPs: 2678.4134. Time: 38.4036 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #286: GFLOPs: 931.9372. Time: 110.3731 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #287: GFLOPs: 2646.6356. Time: 38.8647 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #288: GFLOPs: 2571.8914. Time: 39.9942 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #289: GFLOPs: 899.7045. Time: 114.3273 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #290: GFLOPs: 845.1693. Time: 121.7044 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #291: GFLOPs: 3042.1741. Time: 33.8116 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #292: GFLOPs: 1186.9461. Time: 86.6600 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #293: GFLOPs: 2533.2467. Time: 40.6043 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #294: GFLOPs: 1720.6562. Time: 59.7800 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #295: GFLOPs: 1773.1641. Time: 58.0097 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #296: GFLOPs: 1186.8393. Time: 86.6678 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #297: GFLOPs: 2555.5739. Time: 40.2496 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #298: GFLOPs: 3579.0963. Time: 28.7393 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #299: GFLOPs: 3565.6905. Time: 28.8474 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #300: GFLOPs: 1657.4040. Time: 62.0614 us. Best GFLOPs: 3897.9693
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #301: GFLOPs: 3930.5282. Time: 26.1697 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #302: GFLOPs: 2995.3372. Time: 34.3403 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #303: GFLOPs: 3051.6841. Time: 33.7062 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #304: GFLOPs: 3170.6974. Time: 32.4411 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #305: GFLOPs: 3170.7843. Time: 32.4402 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #306: GFLOPs: 3146.7178. Time: 32.6883 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #307: GFLOPs: 3609.4780. Time: 28.4974 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #308: GFLOPs: 1870.6821. Time: 54.9857 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #309: GFLOPs: 2432.3740. Time: 42.2882 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #310: GFLOPs: 1517.9249. Time: 67.7641 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #311: GFLOPs: 976.6688. Time: 105.3180 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #312: GFLOPs: 3041.0765. Time: 33.8238 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #313: GFLOPs: 2156.1516. Time: 47.7057 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #314: GFLOPs: 3533.1376. Time: 29.1132 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #315: GFLOPs: 3532.1979. Time: 29.1209 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #316: GFLOPs: 3146.9692. Time: 32.6857 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #317: GFLOPs: 3107.0969. Time: 33.1051 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #318: GFLOPs: 101.1715. Time: 1016.6975 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #319: GFLOPs: 167.6361. Time: 613.5959 us. Best GFLOPs: 3930.5282
2024-03-22 01:38:24 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #320: GFLOPs: 162.9068. Time: 631.4087 us. Best GFLOPs: 3930.5282
2024-03-22 01:42:15 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 01:42:16 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 01:42:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 385 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:42:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 771 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:42:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1161 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:42:17 [INFO] [evolutionary_search.cc:723] Sampled 69 candidate(s)
2024-03-22 01:42:19 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 94 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:42:21 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:42:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:42:26 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 111 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:42:27 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.6111  1.5383  1.5270  1.4729  1.4467  1.4355  1.2714  1.2623  1.2482  1.2440  1.2330  1.2321  1.2235  1.2186  1.2175  1.2128
[17 : 32]:	1.2109  1.2073  1.2059  1.1968  1.1827  1.1806  1.1741  1.1719  1.1709  1.1580  1.1579  1.1524  1.1505  1.1482  1.1456  1.1296
[33 : 48]:	1.1214  1.1213  1.1184  1.1174  1.1123  1.1078  1.1071  1.1066  1.1063  1.1028  1.0967  1.0953  1.0911  1.0903  1.0862  1.0808
[49 : 64]:	1.0785  1.0757  1.0740  1.0708  1.0634  1.0488  1.0440  1.0434  1.0342  1.0277  1.0066  0.9926  0.9890  0.9830  0.9825  0.9703
2024-03-22 01:42:27 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 01:42:27 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #321: GFLOPs: 900.6723. Time: 114.2045 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #322: GFLOPs: 2620.5881. Time: 39.2510 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #323: GFLOPs: 2590.2684. Time: 39.7105 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #324: GFLOPs: 2578.5707. Time: 39.8906 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #325: GFLOPs: 2623.3820. Time: 39.2092 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #326: GFLOPs: 2589.9616. Time: 39.7152 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #327: GFLOPs: 2726.1147. Time: 37.7316 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #328: GFLOPs: 2699.5893. Time: 38.1024 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #329: GFLOPs: 2355.2145. Time: 43.6736 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #330: GFLOPs: 2541.2665. Time: 40.4762 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #331: GFLOPs: 3152.6402. Time: 32.6269 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #332: GFLOPs: 2573.8932. Time: 39.9631 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #333: GFLOPs: 2737.1350. Time: 37.5797 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #334: GFLOPs: 1746.6581. Time: 58.8901 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #335: GFLOPs: 2070.1565. Time: 49.6875 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #336: GFLOPs: 2857.6727. Time: 35.9946 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #337: GFLOPs: 1871.4016. Time: 54.9646 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #338: GFLOPs: 2993.6775. Time: 34.3593 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #339: GFLOPs: 1890.6966. Time: 54.4037 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #340: GFLOPs: 1705.5954. Time: 60.3079 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #341: GFLOPs: 2508.8548. Time: 40.9991 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #342: GFLOPs: 1757.5702. Time: 58.5244 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #343: GFLOPs: 1752.6129. Time: 58.6900 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #344: GFLOPs: 2906.1506. Time: 35.3942 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #345: GFLOPs: 1718.0034. Time: 59.8723 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #346: GFLOPs: 2177.2626. Time: 47.2432 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #347: GFLOPs: 1759.4817. Time: 58.4609 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #348: GFLOPs: 2566.3984. Time: 40.0798 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #349: GFLOPs: 3068.3615. Time: 33.5230 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #350: GFLOPs: 2541.4347. Time: 40.4735 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #351: GFLOPs: 2508.8740. Time: 40.9988 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #352: GFLOPs: 2600.7708. Time: 39.5501 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #353: GFLOPs: 1487.3847. Time: 69.1555 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #354: GFLOPs: 1690.6629. Time: 60.8405 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #355: GFLOPs: 2574.8890. Time: 39.9477 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #356: GFLOPs: 2512.3670. Time: 40.9418 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #357: GFLOPs: 2205.2435. Time: 46.6437 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #358: GFLOPs: 2190.1971. Time: 46.9642 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #359: GFLOPs: 2175.5246. Time: 47.2809 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #360: GFLOPs: 2344.0844. Time: 43.8810 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #361: GFLOPs: 1305.5408. Time: 78.7879 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #362: GFLOPs: 1285.9821. Time: 79.9862 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #363: GFLOPs: 1705.1642. Time: 60.3231 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #364: GFLOPs: 2587.7158. Time: 39.7497 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #365: GFLOPs: 2523.6592. Time: 40.7586 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #366: GFLOPs: 2513.2382. Time: 40.9276 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #367: GFLOPs: 511.7983. Time: 200.9792 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #368: GFLOPs: 2528.7361. Time: 40.6768 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #369: GFLOPs: 460.8912. Time: 223.1780 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #370: GFLOPs: 511.7950. Time: 200.9805 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #371: GFLOPs: 1752.2980. Time: 58.7005 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #372: GFLOPs: 1718.7976. Time: 59.8446 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #373: GFLOPs: 494.4117. Time: 208.0469 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #374: GFLOPs: 2523.2639. Time: 40.7650 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #375: GFLOPs: 922.2386. Time: 111.5338 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #376: GFLOPs: 2550.2929. Time: 40.3329 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #377: GFLOPs: 2706.4752. Time: 38.0054 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #378: GFLOPs: 2479.9888. Time: 41.4763 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #379: GFLOPs: 2999.6413. Time: 34.2910 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #380: GFLOPs: 2676.0912. Time: 38.4370 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #381: GFLOPs: 3707.0764. Time: 27.7471 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #382: GFLOPs: 904.3260. Time: 113.7430 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #383: GFLOPs: 157.4067. Time: 653.4716 us. Best GFLOPs: 3930.5282
2024-03-22 01:43:13 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #384: GFLOPs: 600.6018. Time: 171.2629 us. Best GFLOPs: 3930.5282
2024-03-22 01:48:00 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 01:48:00 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 01:48:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 382 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:48:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 773 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:48:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:48:02 [INFO] [evolutionary_search.cc:723] Sampled 73 candidate(s)
2024-03-22 01:48:03 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:48:05 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:48:08 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 114 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:48:10 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:48:11 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1024  1.0463  1.0307  1.0296  1.0288  1.0288  1.0285  1.0276  1.0250  1.0140  0.9857  0.9829  0.9752  0.9732  0.9712  0.9601
[17 : 32]:	0.9546  0.9546  0.9454  0.9407  0.9400  0.9370  0.9365  0.9364  0.9359  0.9333  0.9316  0.9311  0.9290  0.9286  0.9271  0.9260
[33 : 48]:	0.9256  0.9219  0.9217  0.9207  0.9203  0.9148  0.9143  0.9143  0.9142  0.9127  0.9100  0.9080  0.9071  0.9064  0.9037  0.9037
[49 : 64]:	0.9033  0.9028  0.9019  0.9003  0.9001  0.8991  0.8979  0.8960  0.8956  0.8927  0.8917  0.8917  0.8913  0.8907  0.8904  0.8890
2024-03-22 01:48:11 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 01:48:11 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #385: GFLOPs: 2860.9672. Time: 35.9532 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #386: GFLOPs: 3891.0294. Time: 26.4354 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #387: GFLOPs: 3751.5675. Time: 27.4181 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #388: GFLOPs: 3768.6157. Time: 27.2941 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #389: GFLOPs: 3830.0297. Time: 26.8564 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #390: GFLOPs: 3751.7384. Time: 27.4168 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #391: GFLOPs: 2795.3143. Time: 36.7976 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #392: GFLOPs: 3759.4327. Time: 27.3607 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #393: GFLOPs: 3059.1250. Time: 33.6243 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #394: GFLOPs: 3739.0520. Time: 27.5099 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #395: GFLOPs: 3713.5945. Time: 27.6984 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #396: GFLOPs: 3305.6793. Time: 31.1164 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #397: GFLOPs: 3505.4807. Time: 29.3429 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #398: GFLOPs: 3208.3500. Time: 32.0603 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #399: GFLOPs: 3611.4561. Time: 28.4818 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #400: GFLOPs: 3585.2182. Time: 28.6902 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #401: GFLOPs: 2851.3055. Time: 36.0750 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #402: GFLOPs: 3208.2606. Time: 32.0612 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #403: GFLOPs: 3631.1197. Time: 28.3276 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #404: GFLOPs: 3064.5181. Time: 33.5651 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #405: GFLOPs: 2782.7848. Time: 36.9633 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #406: GFLOPs: 3583.2961. Time: 28.7056 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #407: GFLOPs: 3629.9296. Time: 28.3369 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #408: GFLOPs: 3466.9832. Time: 29.6687 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #409: GFLOPs: 3002.6063. Time: 34.2572 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #410: GFLOPs: 3155.6090. Time: 32.5962 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #411: GFLOPs: 2757.8948. Time: 37.2969 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #412: GFLOPs: 2811.0216. Time: 36.5920 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #413: GFLOPs: 2962.2376. Time: 34.7240 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #414: GFLOPs: 2822.5123. Time: 36.4430 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #415: GFLOPs: 3861.0213. Time: 26.6408 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #416: GFLOPs: 2652.6701. Time: 38.7763 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #417: GFLOPs: 3530.4053. Time: 29.1357 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #418: GFLOPs: 2545.6915. Time: 40.4058 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #419: GFLOPs: 3051.0712. Time: 33.7130 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #420: GFLOPs: 3522.3106. Time: 29.2026 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #421: GFLOPs: 3879.2437. Time: 26.5157 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #422: GFLOPs: 2942.9284. Time: 34.9519 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #423: GFLOPs: 1368.4008. Time: 75.1686 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #424: GFLOPs: 3485.0998. Time: 29.5144 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #425: GFLOPs: 3543.0033. Time: 29.0321 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #426: GFLOPs: 3784.3272. Time: 27.1807 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #427: GFLOPs: 3695.1406. Time: 27.8368 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #428: GFLOPs: 3728.2489. Time: 27.5896 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #429: GFLOPs: 3420.9390. Time: 30.0680 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #430: GFLOPs: 2812.8161. Time: 36.5686 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #431: GFLOPs: 2947.2023. Time: 34.9012 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #432: GFLOPs: 2562.6852. Time: 40.1379 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #433: GFLOPs: 3786.2279. Time: 27.1671 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #434: GFLOPs: 2807.9925. Time: 36.6314 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #435: GFLOPs: 1381.4024. Time: 74.4611 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #436: GFLOPs: 3462.7752. Time: 29.7047 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #437: GFLOPs: 3799.5357. Time: 27.0719 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #438: GFLOPs: 2787.3505. Time: 36.9027 us. Best GFLOPs: 3930.5282
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #439: GFLOPs: 4098.3739. Time: 25.0980 us. Best GFLOPs: 4098.3739
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #440: GFLOPs: 3337.2172. Time: 30.8223 us. Best GFLOPs: 4098.3739
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #441: GFLOPs: 3367.8437. Time: 30.5420 us. Best GFLOPs: 4098.3739
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #442: GFLOPs: 3918.8715. Time: 26.2476 us. Best GFLOPs: 4098.3739
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #443: GFLOPs: 3660.7138. Time: 28.0986 us. Best GFLOPs: 4098.3739
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #444: GFLOPs: 3763.1988. Time: 27.3333 us. Best GFLOPs: 4098.3739
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #445: GFLOPs: 3654.3186. Time: 28.1477 us. Best GFLOPs: 4098.3739
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #446: GFLOPs: 179.6959. Time: 572.4160 us. Best GFLOPs: 4098.3739
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #447: GFLOPs: 336.7254. Time: 305.4738 us. Best GFLOPs: 4098.3739
2024-03-22 01:49:02 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #448: GFLOPs: 31.6218. Time: 3252.8495 us. Best GFLOPs: 4098.3739
2024-03-22 01:54:34 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 01:54:34 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 01:54:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 385 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:54:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 771 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:54:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1150 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:54:36 [INFO] [evolutionary_search.cc:723] Sampled 80 candidate(s)
2024-03-22 01:54:38 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 120 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:54:40 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:54:42 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 110 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:54:45 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 72 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 01:54:46 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.5077  1.5074  1.1033  1.0933  1.0462  1.0340  1.0120  1.0004  0.9857  0.9781  0.9742  0.9668  0.9667  0.9667  0.9664  0.9611
[17 : 32]:	0.9556  0.9551  0.9547  0.9506  0.9500  0.9440  0.9428  0.9412  0.9411  0.9405  0.9405  0.9386  0.9386  0.9357  0.9352  0.9344
[33 : 48]:	0.9344  0.9328  0.9328  0.9318  0.9302  0.9290  0.9270  0.9259  0.9257  0.9233  0.9232  0.9231  0.9231  0.9214  0.9214  0.9206
[49 : 64]:	0.9201  0.9199  0.9192  0.9187  0.9187  0.9187  0.9185  0.9177  0.9174  0.9167  0.9156  0.9146  0.9140  0.9137  0.9136  0.9134
2024-03-22 01:54:46 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 01:54:46 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #449: GFLOPs: 2305.6375. Time: 44.6127 us. Best GFLOPs: 4098.3739
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #450: GFLOPs: 2074.0101. Time: 49.5951 us. Best GFLOPs: 4098.3739
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #451: GFLOPs: 995.9182. Time: 103.2824 us. Best GFLOPs: 4098.3739
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #452: GFLOPs: 2693.7911. Time: 38.1844 us. Best GFLOPs: 4098.3739
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #453: GFLOPs: 2693.8109. Time: 38.1841 us. Best GFLOPs: 4098.3739
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #454: GFLOPs: 2979.3120. Time: 34.5250 us. Best GFLOPs: 4098.3739
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #455: GFLOPs: 3873.3642. Time: 26.5559 us. Best GFLOPs: 4098.3739
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #456: GFLOPs: 3950.4453. Time: 26.0378 us. Best GFLOPs: 4098.3739
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #457: GFLOPs: 4134.9046. Time: 24.8762 us. Best GFLOPs: 4134.9046
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #458: GFLOPs: 4029.9696. Time: 25.5240 us. Best GFLOPs: 4134.9046
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #459: GFLOPs: 4102.2149. Time: 25.0745 us. Best GFLOPs: 4134.9046
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #460: GFLOPs: 3781.9572. Time: 27.1978 us. Best GFLOPs: 4134.9046
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #461: GFLOPs: 3799.5509. Time: 27.0718 us. Best GFLOPs: 4134.9046
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #462: GFLOPs: 3729.9883. Time: 27.5767 us. Best GFLOPs: 4134.9046
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #463: GFLOPs: 3775.0135. Time: 27.2478 us. Best GFLOPs: 4134.9046
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #464: GFLOPs: 4039.2662. Time: 25.4652 us. Best GFLOPs: 4134.9046
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #465: GFLOPs: 4025.6663. Time: 25.5512 us. Best GFLOPs: 4134.9046
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #466: GFLOPs: 4172.2842. Time: 24.6534 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #467: GFLOPs: 3670.1853. Time: 28.0261 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #468: GFLOPs: 3766.8000. Time: 27.3072 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #469: GFLOPs: 3804.6599. Time: 27.0355 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #470: GFLOPs: 3469.6754. Time: 29.6457 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #471: GFLOPs: 3232.8369. Time: 31.8175 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #472: GFLOPs: 3841.7870. Time: 26.7742 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #473: GFLOPs: 4121.4558. Time: 24.9574 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #474: GFLOPs: 4110.9025. Time: 25.0215 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #475: GFLOPs: 4098.8060. Time: 25.0953 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #476: GFLOPs: 4071.1297. Time: 25.2659 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #477: GFLOPs: 3616.6751. Time: 28.4407 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #478: GFLOPs: 4085.1568. Time: 25.1792 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #479: GFLOPs: 3822.0388. Time: 26.9125 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #480: GFLOPs: 3788.3508. Time: 27.1519 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #481: GFLOPs: 3128.3696. Time: 32.8800 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #482: GFLOPs: 3750.6856. Time: 27.4245 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #483: GFLOPs: 4086.9614. Time: 25.1680 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #484: GFLOPs: 3790.9454. Time: 27.1333 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #485: GFLOPs: 3586.1133. Time: 28.6831 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #486: GFLOPs: 4041.1178. Time: 25.4536 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #487: GFLOPs: 4099.0487. Time: 25.0938 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #488: GFLOPs: 3767.6254. Time: 27.3012 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #489: GFLOPs: 3824.0031. Time: 26.8987 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #490: GFLOPs: 4042.5338. Time: 25.4446 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #491: GFLOPs: 3779.0174. Time: 27.2189 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #492: GFLOPs: 3018.2451. Time: 34.0797 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #493: GFLOPs: 3764.1234. Time: 27.3266 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #494: GFLOPs: 3697.0215. Time: 27.8226 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #495: GFLOPs: 3536.3824. Time: 29.0864 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #496: GFLOPs: 3778.4948. Time: 27.2227 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #497: GFLOPs: 4144.9057. Time: 24.8162 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #498: GFLOPs: 4094.8955. Time: 25.1193 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #499: GFLOPs: 3833.2658. Time: 26.8337 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #500: GFLOPs: 3704.9188. Time: 27.7633 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #501: GFLOPs: 3702.1766. Time: 27.7839 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #502: GFLOPs: 4091.7498. Time: 25.1386 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #503: GFLOPs: 3751.2697. Time: 27.4203 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #504: GFLOPs: 3555.3026. Time: 28.9317 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #505: GFLOPs: 3002.7444. Time: 34.2556 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #506: GFLOPs: 3824.9078. Time: 26.8924 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #507: GFLOPs: 3826.9756. Time: 26.8778 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #508: GFLOPs: 4122.0762. Time: 24.9536 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #509: GFLOPs: 3768.1272. Time: 27.2976 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #510: GFLOPs: 563.6535. Time: 182.4894 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #511: GFLOPs: 2212.9580. Time: 46.4811 us. Best GFLOPs: 4172.2842
2024-03-22 01:55:32 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #512: GFLOPs: 881.2305. Time: 116.7241 us. Best GFLOPs: 4172.2842
2024-03-22 02:00:32 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 02:00:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 02:00:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 383 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:00:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 761 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:00:33 [INFO] [evolutionary_search.cc:723] Sampled 59 candidate(s)
2024-03-22 02:00:35 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 75 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:00:37 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 80 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:00:39 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 71 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:00:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 91 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:00:42 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1079  1.0995  1.0807  1.0363  1.0276  1.0201  1.0060  1.0050  1.0030  1.0021  0.9981  0.9938  0.9921  0.9921  0.9921  0.9859
[17 : 32]:	0.9849  0.9845  0.9833  0.9820  0.9804  0.9797  0.9758  0.9758  0.9755  0.9748  0.9744  0.9744  0.9738  0.9734  0.9734  0.9726
[33 : 48]:	0.9709  0.9695  0.9681  0.9666  0.9657  0.9652  0.9642  0.9642  0.9638  0.9635  0.9635  0.9622  0.9619  0.9612  0.9609  0.9593
[49 : 64]:	0.9593  0.9575  0.9563  0.9563  0.9548  0.9546  0.9544  0.9542  0.9524  0.9523  0.9515  0.9501  0.9485  0.9478  0.9468  0.9468
2024-03-22 02:00:42 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 02:00:42 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #513: GFLOPs: 197.3166. Time: 521.2983 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #514: GFLOPs: 196.6896. Time: 522.9601 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #515: GFLOPs: 214.7927. Time: 478.8841 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #516: GFLOPs: 3253.1160. Time: 31.6192 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #517: GFLOPs: 3294.3843. Time: 31.2231 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #518: GFLOPs: 3254.5297. Time: 31.6054 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #519: GFLOPs: 3235.4370. Time: 31.7919 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #520: GFLOPs: 3272.1230. Time: 31.4355 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #521: GFLOPs: 3113.1705. Time: 33.0405 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #522: GFLOPs: 3249.1177. Time: 31.6581 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #523: GFLOPs: 3205.9849. Time: 32.0840 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #524: GFLOPs: 4091.6324. Time: 25.1393 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #525: GFLOPs: 3195.7552. Time: 32.1867 us. Best GFLOPs: 4172.2842
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #526: GFLOPs: 4190.4618. Time: 24.5464 us. Best GFLOPs: 4190.4618
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #527: GFLOPs: 4371.4049. Time: 23.5304 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #528: GFLOPs: 4190.5545. Time: 24.5459 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #529: GFLOPs: 4216.9597. Time: 24.3922 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #530: GFLOPs: 4132.4443. Time: 24.8910 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #531: GFLOPs: 4132.1604. Time: 24.8927 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #532: GFLOPs: 3741.7416. Time: 27.4901 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #533: GFLOPs: 3121.2829. Time: 32.9547 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #534: GFLOPs: 3013.5459. Time: 34.1328 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #535: GFLOPs: 4201.6746. Time: 24.4809 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #536: GFLOPs: 4201.8220. Time: 24.4800 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #537: GFLOPs: 4120.1291. Time: 24.9654 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #538: GFLOPs: 4233.8016. Time: 24.2951 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #539: GFLOPs: 4104.4360. Time: 25.0609 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #540: GFLOPs: 4104.3593. Time: 25.0614 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #541: GFLOPs: 4132.6230. Time: 24.8900 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #542: GFLOPs: 4140.9332. Time: 24.8400 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #543: GFLOPs: 4140.9463. Time: 24.8399 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #544: GFLOPs: 4150.8492. Time: 24.7807 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #545: GFLOPs: 3001.4271. Time: 34.2706 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #546: GFLOPs: 4117.3077. Time: 24.9825 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #547: GFLOPs: 4098.6829. Time: 25.0961 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #548: GFLOPs: 4091.7256. Time: 25.1387 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #549: GFLOPs: 3187.2636. Time: 32.2724 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #550: GFLOPs: 4120.4936. Time: 24.9632 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #551: GFLOPs: 4095.6545. Time: 25.1146 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #552: GFLOPs: 4095.9468. Time: 25.1128 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #553: GFLOPs: 3097.8883. Time: 33.2035 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #554: GFLOPs: 2990.6254. Time: 34.3944 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #555: GFLOPs: 2990.3303. Time: 34.3978 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #556: GFLOPs: 3767.7288. Time: 27.3005 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #557: GFLOPs: 4116.0478. Time: 24.9902 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #558: GFLOPs: 2991.2547. Time: 34.3872 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #559: GFLOPs: 4095.7907. Time: 25.1138 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #560: GFLOPs: 4109.1027. Time: 25.0324 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #561: GFLOPs: 4108.8283. Time: 25.0341 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #562: GFLOPs: 2999.3028. Time: 34.2949 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #563: GFLOPs: 3047.2451. Time: 33.7553 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #564: GFLOPs: 3045.1307. Time: 33.7788 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #565: GFLOPs: 2932.1000. Time: 35.0809 us. Best GFLOPs: 4371.4049
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #566: GFLOPs: 4491.6855. Time: 22.9003 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #567: GFLOPs: 3976.5751. Time: 25.8667 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #568: GFLOPs: 3013.2627. Time: 34.1360 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #569: GFLOPs: 3010.3281. Time: 34.1693 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #570: GFLOPs: 4162.7521. Time: 24.7098 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #571: GFLOPs: 4011.6627. Time: 25.6404 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #572: GFLOPs: 4120.5615. Time: 24.9628 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #573: GFLOPs: 3106.3137. Time: 33.1135 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #574: GFLOPs: 1429.2048. Time: 71.9707 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #575: GFLOPs: 924.2105. Time: 111.2959 us. Best GFLOPs: 4491.6855
2024-03-22 02:01:27 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #576: GFLOPs: 207.2344. Time: 496.3501 us. Best GFLOPs: 4491.6855
2024-03-22 02:08:20 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 02:08:20 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 02:08:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 390 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:08:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 781 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:08:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:08:22 [INFO] [evolutionary_search.cc:723] Sampled 60 candidate(s)
2024-03-22 02:08:23 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 73 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:08:25 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 65 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:08:28 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 88 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:08:30 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 53 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:08:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1668  1.1149  1.0875  1.0743  1.0495  1.0488  1.0472  1.0418  1.0224  1.0224  1.0195  1.0126  1.0112  1.0077  0.9925  0.9864
[17 : 32]:	0.9703  0.9701  0.9559  0.9537  0.9536  0.9532  0.9518  0.9455  0.9439  0.9384  0.9365  0.9345  0.9340  0.9329  0.9329  0.9323
[33 : 48]:	0.9314  0.9302  0.9266  0.9264  0.9261  0.9255  0.9250  0.9244  0.9243  0.9239  0.9230  0.9200  0.9199  0.9163  0.9147  0.9147
[49 : 64]:	0.9107  0.9107  0.9098  0.9089  0.9080  0.9076  0.9063  0.9063  0.9058  0.9053  0.9039  0.9035  0.9032  0.9024  0.9021  0.9017
2024-03-22 02:08:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 02:08:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #577: GFLOPs: 4246.4881. Time: 24.2226 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #578: GFLOPs: 4191.1910. Time: 24.5421 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #579: GFLOPs: 4350.4103. Time: 23.6439 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #580: GFLOPs: 4301.6471. Time: 23.9120 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #581: GFLOPs: 3900.9449. Time: 26.3682 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #582: GFLOPs: 813.9442. Time: 126.3733 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #583: GFLOPs: 4317.2704. Time: 23.8254 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #584: GFLOPs: 3211.0915. Time: 32.0330 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #585: GFLOPs: 3900.8610. Time: 26.3687 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #586: GFLOPs: 3893.8883. Time: 26.4160 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #587: GFLOPs: 4166.4338. Time: 24.6880 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #588: GFLOPs: 3847.3525. Time: 26.7355 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #589: GFLOPs: 3201.4045. Time: 32.1299 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #590: GFLOPs: 2509.4595. Time: 40.9892 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #591: GFLOPs: 2699.8497. Time: 38.0987 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #592: GFLOPs: 2765.0842. Time: 37.1999 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #593: GFLOPs: 4148.8799. Time: 24.7924 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #594: GFLOPs: 3699.4629. Time: 27.8043 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #595: GFLOPs: 808.1804. Time: 127.2746 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #596: GFLOPs: 3747.4009. Time: 27.4486 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #597: GFLOPs: 4095.7892. Time: 25.1138 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #598: GFLOPs: 3907.2860. Time: 26.3254 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #599: GFLOPs: 3646.3153. Time: 28.2095 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #600: GFLOPs: 4242.6198. Time: 24.2446 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #601: GFLOPs: 4192.5324. Time: 24.5343 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #602: GFLOPs: 4073.7996. Time: 25.2494 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #603: GFLOPs: 4130.1273. Time: 24.9050 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #604: GFLOPs: 4153.5617. Time: 24.7645 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #605: GFLOPs: 4262.3046. Time: 24.1327 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #606: GFLOPs: 4120.9879. Time: 24.9602 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #607: GFLOPs: 4121.0224. Time: 24.9600 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #608: GFLOPs: 4186.8796. Time: 24.5674 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #609: GFLOPs: 4120.2173. Time: 24.9649 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #610: GFLOPs: 3804.0884. Time: 27.0395 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #611: GFLOPs: 4144.6908. Time: 24.8175 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #612: GFLOPs: 3354.4086. Time: 30.6644 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #613: GFLOPs: 4146.3049. Time: 24.8078 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #614: GFLOPs: 4217.1965. Time: 24.3908 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #615: GFLOPs: 4118.3823. Time: 24.9760 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #616: GFLOPs: 4118.3496. Time: 24.9762 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #617: GFLOPs: 4122.4924. Time: 24.9511 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #618: GFLOPs: 4182.1581. Time: 24.5951 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #619: GFLOPs: 3027.3180. Time: 33.9775 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #620: GFLOPs: 4114.3216. Time: 25.0007 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #621: GFLOPs: 3434.4278. Time: 29.9499 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #622: GFLOPs: 4147.8696. Time: 24.7985 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #623: GFLOPs: 4119.8307. Time: 24.9672 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #624: GFLOPs: 4121.0644. Time: 24.9598 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #625: GFLOPs: 4120.8746. Time: 24.9609 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #626: GFLOPs: 4121.0395. Time: 24.9599 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #627: GFLOPs: 4001.8354. Time: 25.7034 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #628: GFLOPs: 3457.0321. Time: 29.7541 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #629: GFLOPs: 4109.3422. Time: 25.0310 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #630: GFLOPs: 4045.5479. Time: 25.4257 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #631: GFLOPs: 4101.4143. Time: 25.0793 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #632: GFLOPs: 4101.3324. Time: 25.0798 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #633: GFLOPs: 4093.6215. Time: 25.1271 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #634: GFLOPs: 4101.9040. Time: 25.0764 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #635: GFLOPs: 4153.2465. Time: 24.7664 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #636: GFLOPs: 3273.4465. Time: 31.4228 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #637: GFLOPs: 3880.4482. Time: 26.5075 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #638: GFLOPs: 114.7321. Time: 896.5303 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #639: GFLOPs: 601.1332. Time: 171.1115 us. Best GFLOPs: 4491.6855
2024-03-22 02:09:18 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #640: GFLOPs: 1743.6912. Time: 58.9903 us. Best GFLOPs: 4491.6855
2024-03-22 02:16:20 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 02:16:20 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 02:16:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 388 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:16:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 775 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:16:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1163 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:16:22 [INFO] [evolutionary_search.cc:723] Sampled 67 candidate(s)
2024-03-22 02:16:24 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 87 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:16:26 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 83 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:16:28 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 91 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:16:30 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 79 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:16:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.6779  1.2449  1.2415  1.2268  1.2161  1.2058  1.1942  1.1865  1.1854  1.1768  1.1730  1.1704  1.1658  1.1494  1.1242  1.1236
[17 : 32]:	1.1115  1.0960  1.0782  1.0782  1.0764  1.0711  1.0651  1.0548  1.0378  1.0346  1.0200  1.0147  1.0135  1.0027  0.9949  0.9852
[33 : 48]:	0.9841  0.9715  0.9701  0.9680  0.9671  0.9661  0.9652  0.9641  0.9573  0.9562  0.9549  0.9547  0.9538  0.9537  0.9517  0.9516
[49 : 64]:	0.9411  0.9400  0.9395  0.9386  0.9383  0.9381  0.9360  0.9350  0.9349  0.9348  0.9348  0.9318  0.9312  0.9304  0.9300  0.9299
2024-03-22 02:16:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 02:16:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #641: GFLOPs: 4135.4207. Time: 24.8731 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #642: GFLOPs: 1464.5191. Time: 70.2352 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #643: GFLOPs: 1092.3872. Time: 94.1615 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #644: GFLOPs: 1457.3686. Time: 70.5798 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #645: GFLOPs: 2074.4676. Time: 49.5842 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #646: GFLOPs: 720.0763. Time: 142.8471 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #647: GFLOPs: 4221.1309. Time: 24.3681 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #648: GFLOPs: 4167.9913. Time: 24.6787 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #649: GFLOPs: 4145.6839. Time: 24.8115 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #650: GFLOPs: 1396.7142. Time: 73.6448 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #651: GFLOPs: 3034.4107. Time: 33.8981 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #652: GFLOPs: 2973.0429. Time: 34.5978 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #653: GFLOPs: 1392.3151. Time: 73.8775 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #654: GFLOPs: 3033.0988. Time: 33.9128 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #655: GFLOPs: 1396.6897. Time: 73.6461 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #656: GFLOPs: 4191.1744. Time: 24.5422 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #657: GFLOPs: 1226.9339. Time: 83.8356 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #658: GFLOPs: 4338.6950. Time: 23.7078 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #659: GFLOPs: 3520.4331. Time: 29.2182 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #660: GFLOPs: 3645.2484. Time: 28.2178 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #661: GFLOPs: 4082.2633. Time: 25.1970 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #662: GFLOPs: 3067.0974. Time: 33.5369 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #663: GFLOPs: 4346.4102. Time: 23.6657 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #664: GFLOPs: 3508.6467. Time: 29.3164 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #665: GFLOPs: 4247.1202. Time: 24.2190 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #666: GFLOPs: 3013.6243. Time: 34.1319 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #667: GFLOPs: 2922.8474. Time: 35.1920 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #668: GFLOPs: 4211.3508. Time: 24.4247 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #669: GFLOPs: 4097.8025. Time: 25.1015 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #670: GFLOPs: 2950.5165. Time: 34.8620 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #671: GFLOPs: 2917.9027. Time: 35.2516 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #672: GFLOPs: 4324.2251. Time: 23.7871 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #673: GFLOPs: 4322.0817. Time: 23.7989 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #674: GFLOPs: 740.8743. Time: 138.8370 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #675: GFLOPs: 3718.7309. Time: 27.6602 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #676: GFLOPs: 4204.6647. Time: 24.4635 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #677: GFLOPs: 2731.6139. Time: 37.6557 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #678: GFLOPs: 749.6991. Time: 137.2028 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #679: GFLOPs: 4216.2799. Time: 24.3961 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #680: GFLOPs: 2658.8722. Time: 38.6859 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #681: GFLOPs: 4342.1572. Time: 23.6889 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #682: GFLOPs: 4342.0129. Time: 23.6897 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #683: GFLOPs: 4195.8715. Time: 24.5148 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #684: GFLOPs: 2820.4288. Time: 36.4699 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #685: GFLOPs: 4193.6820. Time: 24.5276 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #686: GFLOPs: 4249.3918. Time: 24.2060 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #687: GFLOPs: 4271.3757. Time: 24.0814 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #688: GFLOPs: 3211.7150. Time: 32.0268 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #689: GFLOPs: 4314.5579. Time: 23.8404 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #690: GFLOPs: 2926.9528. Time: 35.1426 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #691: GFLOPs: 435.9524. Time: 235.9450 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #692: GFLOPs: 3090.5051. Time: 33.2828 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #693: GFLOPs: 4338.7819. Time: 23.7073 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #694: GFLOPs: 4195.7901. Time: 24.5152 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #695: GFLOPs: 4194.8182. Time: 24.5209 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #696: GFLOPs: 4138.6628. Time: 24.8536 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #697: GFLOPs: 3955.7773. Time: 26.0027 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #698: GFLOPs: 2769.4460. Time: 37.1413 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #699: GFLOPs: 2786.4918. Time: 36.9141 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #700: GFLOPs: 3766.7101. Time: 27.3079 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #701: GFLOPs: 4319.3002. Time: 23.8142 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #702: GFLOPs: 251.8527. Time: 408.4165 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #703: GFLOPs: 1483.8604. Time: 69.3197 us. Best GFLOPs: 4491.6855
2024-03-22 02:17:20 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #704: GFLOPs: 107.1648. Time: 959.8378 us. Best GFLOPs: 4491.6855
2024-03-22 02:21:59 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 02:21:59 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 02:22:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 382 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:22:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 769 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:22:01 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-03-22 02:22:02 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:22:04 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:22:06 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 76 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:22:09 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:22:10 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0000  0.9961  0.9929  0.9928  0.9922  0.9904  0.9889  0.9851  0.9834  0.9831  0.9801  0.9780  0.9748  0.9708  0.9681  0.9671
[17 : 32]:	0.9664  0.9599  0.9593  0.9593  0.9593  0.9589  0.9588  0.9553  0.9533  0.9527  0.9527  0.9489  0.9489  0.9478  0.9470  0.9451
[33 : 48]:	0.9450  0.9441  0.9429  0.9416  0.9403  0.9403  0.9400  0.9391  0.9381  0.9381  0.9378  0.9373  0.9367  0.9365  0.9343  0.9342
[49 : 64]:	0.9342  0.9337  0.9337  0.9328  0.9322  0.9314  0.9310  0.9307  0.9300  0.9294  0.9290  0.9290  0.9285  0.9283  0.9283  0.9279
2024-03-22 02:22:10 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 02:22:10 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #705: GFLOPs: 4378.7956. Time: 23.4907 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #706: GFLOPs: 4261.0432. Time: 24.1398 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #707: GFLOPs: 4367.3038. Time: 23.5525 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #708: GFLOPs: 4351.9487. Time: 23.6356 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #709: GFLOPs: 4229.8609. Time: 24.3178 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #710: GFLOPs: 4365.3160. Time: 23.5632 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #711: GFLOPs: 4350.2437. Time: 23.6448 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #712: GFLOPs: 4222.9052. Time: 24.3578 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #713: GFLOPs: 4366.0940. Time: 23.5590 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #714: GFLOPs: 4223.5866. Time: 24.3539 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #715: GFLOPs: 4345.2142. Time: 23.6722 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #716: GFLOPs: 4198.7741. Time: 24.4978 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #717: GFLOPs: 4343.2010. Time: 23.6832 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #718: GFLOPs: 4327.2082. Time: 23.7707 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #719: GFLOPs: 3118.2576. Time: 32.9866 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #720: GFLOPs: 4326.9622. Time: 23.7721 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #721: GFLOPs: 4299.7270. Time: 23.9226 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #722: GFLOPs: 4342.7894. Time: 23.6854 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #723: GFLOPs: 4203.7331. Time: 24.4689 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #724: GFLOPs: 4243.4401. Time: 24.2400 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #725: GFLOPs: 4243.5533. Time: 24.2393 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #726: GFLOPs: 4327.6677. Time: 23.7682 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #727: GFLOPs: 4328.1845. Time: 23.7653 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #728: GFLOPs: 4299.4971. Time: 23.9239 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #729: GFLOPs: 4202.4958. Time: 24.4761 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #730: GFLOPs: 4311.7060. Time: 23.8562 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #731: GFLOPs: 4290.8022. Time: 23.9724 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #732: GFLOPs: 4126.6497. Time: 24.9260 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #733: GFLOPs: 4337.6272. Time: 23.7136 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #734: GFLOPs: 4197.3810. Time: 24.5059 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #735: GFLOPs: 4198.2058. Time: 24.5011 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #736: GFLOPs: 4199.6736. Time: 24.4926 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #737: GFLOPs: 4347.4241. Time: 23.6602 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #738: GFLOPs: 4293.2492. Time: 23.9587 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #739: GFLOPs: 4197.8743. Time: 24.5031 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #740: GFLOPs: 4215.6177. Time: 24.3999 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #741: GFLOPs: 4195.1958. Time: 24.5187 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #742: GFLOPs: 4195.1783. Time: 24.5188 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #743: GFLOPs: 4289.9630. Time: 23.9771 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #744: GFLOPs: 4197.6468. Time: 24.5044 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #745: GFLOPs: 4167.9392. Time: 24.6791 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #746: GFLOPs: 4167.9876. Time: 24.6788 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #747: GFLOPs: 4389.0011. Time: 23.4360 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #748: GFLOPs: 4164.6627. Time: 24.6985 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #749: GFLOPs: 4118.3017. Time: 24.9765 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #750: GFLOPs: 4321.9895. Time: 23.7994 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #751: GFLOPs: 4186.7689. Time: 24.5681 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #752: GFLOPs: 4241.2983. Time: 24.2522 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #753: GFLOPs: 4181.3943. Time: 24.5996 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #754: GFLOPs: 4163.8033. Time: 24.7036 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #755: GFLOPs: 4136.2036. Time: 24.8684 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #756: GFLOPs: 4157.6931. Time: 24.7399 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #757: GFLOPs: 4115.9964. Time: 24.9905 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #758: GFLOPs: 4268.5287. Time: 24.0975 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #759: GFLOPs: 4215.9025. Time: 24.3983 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #760: GFLOPs: 4135.3029. Time: 24.8738 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #761: GFLOPs: 4267.2259. Time: 24.1048 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #762: GFLOPs: 4121.7544. Time: 24.9556 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #763: GFLOPs: 4110.8256. Time: 25.0219 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #764: GFLOPs: 4143.4042. Time: 24.8252 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #765: GFLOPs: 4121.7259. Time: 24.9558 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #766: GFLOPs: 231.9787. Time: 443.4062 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #767: GFLOPs: 29.8633. Time: 3444.3914 us. Best GFLOPs: 4491.6855
2024-03-22 02:22:58 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #768: GFLOPs: 91.7556. Time: 1121.0297 us. Best GFLOPs: 4491.6855
2024-03-22 02:31:07 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 02:31:07 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 02:31:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 385 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:31:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 779 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:31:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1158 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:31:09 [INFO] [evolutionary_search.cc:723] Sampled 72 candidate(s)
2024-03-22 02:31:10 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 96 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:31:13 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 91 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:31:15 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 93 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:31:17 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 115 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:31:18 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3472  1.3396  1.3355  1.3305  1.3282  1.3216  1.3215  1.3177  1.2961  1.2905  1.2529  1.2089  1.2018  1.2003  1.1682  1.1648
[17 : 32]:	1.1542  1.1422  1.1422  1.1349  1.1332  1.1332  1.1324  1.1245  1.0401  1.0340  1.0340  1.0298  1.0279  1.0231  1.0198  1.0197
[33 : 48]:	1.0121  1.0011  1.0010  0.9999  0.9963  0.9948  0.9940  0.9938  0.9930  0.9916  0.9888  0.9882  0.9824  0.9785  0.9761  0.9758
[49 : 64]:	0.9751  0.9744  0.9724  0.9720  0.9672  0.9648  0.9633  0.9611  0.9583  0.9577  0.9572  0.9570  0.9562  0.9555  0.9554  0.9554
2024-03-22 02:31:18 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 02:31:18 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #769: GFLOPs: 1231.3311. Time: 83.5363 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #770: GFLOPs: 1276.2011. Time: 80.5992 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #771: GFLOPs: 1272.7671. Time: 80.8167 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #772: GFLOPs: 2402.9393. Time: 42.8062 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #773: GFLOPs: 2520.6373. Time: 40.8075 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #774: GFLOPs: 2523.2475. Time: 40.7652 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #775: GFLOPs: 4114.0946. Time: 25.0021 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #776: GFLOPs: 3366.4953. Time: 30.5543 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #777: GFLOPs: 3363.2493. Time: 30.5838 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #778: GFLOPs: 1262.0847. Time: 81.5007 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #779: GFLOPs: 1270.1822. Time: 80.9811 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #780: GFLOPs: 1699.9884. Time: 60.5068 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #781: GFLOPs: 4111.2853. Time: 25.0191 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #782: GFLOPs: 3441.5778. Time: 29.8877 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #783: GFLOPs: 2346.2878. Time: 43.8398 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #784: GFLOPs: 3180.3399. Time: 32.3427 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #785: GFLOPs: 2828.4689. Time: 36.3662 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #786: GFLOPs: 4031.9091. Time: 25.5117 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #787: GFLOPs: 2552.1627. Time: 40.3034 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #788: GFLOPs: 4101.3450. Time: 25.0798 us. Best GFLOPs: 4491.6855
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #789: GFLOPs: 4868.3327. Time: 21.1285 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #790: GFLOPs: 2695.5647. Time: 38.1593 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #791: GFLOPs: 2446.0035. Time: 42.0526 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #792: GFLOPs: 2435.2317. Time: 42.2386 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #793: GFLOPs: 4442.3705. Time: 23.1545 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #794: GFLOPs: 3977.9093. Time: 25.8580 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #795: GFLOPs: 4218.4235. Time: 24.3837 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #796: GFLOPs: 4338.4031. Time: 23.7094 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #797: GFLOPs: 3972.0659. Time: 25.8960 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #798: GFLOPs: 1255.2616. Time: 81.9437 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #799: GFLOPs: 3914.0779. Time: 26.2797 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #800: GFLOPs: 2281.1997. Time: 45.0907 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #801: GFLOPs: 3945.7433. Time: 26.0688 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #802: GFLOPs: 4430.0467. Time: 23.2189 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #803: GFLOPs: 4813.5573. Time: 21.3690 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #804: GFLOPs: 3824.6567. Time: 26.8941 us. Best GFLOPs: 4868.3327
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #805: GFLOPs: 4941.1970. Time: 20.8170 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #806: GFLOPs: 4345.8889. Time: 23.6685 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #807: GFLOPs: 4251.2719. Time: 24.1953 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #808: GFLOPs: 3963.0564. Time: 25.9549 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #809: GFLOPs: 4282.6172. Time: 24.0182 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #810: GFLOPs: 4250.6265. Time: 24.1990 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #811: GFLOPs: 4292.0734. Time: 23.9653 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #812: GFLOPs: 1254.7955. Time: 81.9742 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #813: GFLOPs: 4317.9616. Time: 23.8216 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #814: GFLOPs: 3138.6499. Time: 32.7723 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #815: GFLOPs: 4170.3722. Time: 24.6647 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #816: GFLOPs: 4317.9348. Time: 23.8218 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #817: GFLOPs: 4317.9438. Time: 23.8217 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #818: GFLOPs: 4348.3517. Time: 23.6551 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #819: GFLOPs: 4317.7278. Time: 23.8229 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #820: GFLOPs: 4348.7573. Time: 23.6529 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #821: GFLOPs: 4348.5836. Time: 23.6539 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #822: GFLOPs: 4348.9612. Time: 23.6518 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #823: GFLOPs: 4303.6556. Time: 23.9008 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #824: GFLOPs: 4293.0062. Time: 23.9601 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #825: GFLOPs: 4293.2887. Time: 23.9585 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #826: GFLOPs: 4339.1896. Time: 23.7051 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #827: GFLOPs: 4309.6859. Time: 23.8674 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #828: GFLOPs: 4290.3828. Time: 23.9747 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #829: GFLOPs: 4156.5700. Time: 24.7466 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #830: GFLOPs: 998.9846. Time: 102.9654 us. Best GFLOPs: 4941.1970
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #831: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  332: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  331: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  330: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  329: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  328: tvm::transform::Pass::operator()(tvm::IRModule) const
  327: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  326: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  325: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  324: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  323: _ZN3tvm7runtime13PackedFuncObj
  322: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  321: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  320: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  319: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  318: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  317: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  316: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  315: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  314: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  313: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  312: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  311: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  310: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  309: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  308: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  307: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  306: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  305: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  304: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  303: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  302: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  301: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  300: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  299: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  298: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  269: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  268: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  267: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  263: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  262: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  261: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  260: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  259: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  258: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  257: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  256: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  255: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  254: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  253: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  252: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  251: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  250: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  249: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  248: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  244: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  243: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  242: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  241: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  237: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  236: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  235: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  234: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  233: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  232: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  231: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  230: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  229: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  228: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  227: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  226: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  222: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  221: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  220: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  219: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  218: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  217: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  216: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  215: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  214: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  213: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  212: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  211: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  210: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  209: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  208: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  207: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  206: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  204: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  203: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  202: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  200: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  199: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  198: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  197: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  193: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  192: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  191: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  190: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  189: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  188: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  187: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  186: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  185: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  184: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  183: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  182: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  181: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  180: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  179: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  178: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  170: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  169: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  168: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  167: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  166: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  165: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  164: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  163: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  162: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  161: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  160: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  159: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  158: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  157: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  156: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  154: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  153: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  151: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  150: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  149: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  148: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  147: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  146: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  145: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  144: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  143: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  142: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  141: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  140: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  139: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  138: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  137: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  136: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  135: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  134: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  133: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  132: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  131: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  130: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  129: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  128: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  127: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  126: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  125: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  124: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  123: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  122: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  121: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  120: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  119: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  118: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  117: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  116: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  115: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  114: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  113: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  112: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  111: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  110: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  109: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  108: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  107: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  106: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  105: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  104: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  103: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  102: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  101: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  100: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  99: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  98: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  97: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  96: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  95: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  94: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  93: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  92: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  91: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  90: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  89: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  88: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  87: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  86: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  85: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  84: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  83: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  82: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  81: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  80: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  79: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  78: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  77: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  76: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  75: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  74: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  73: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  72: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  71: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  70: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  69: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  68: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  67: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  66: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  65: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  64: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  63: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  62: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  61: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  60: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  59: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  58: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  57: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  56: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  55: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  54: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  53: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  52: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  51: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  49: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  48: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  46: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  43: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  42: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  41: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  38: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  37: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  35: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  34: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  32: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  31: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  30: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  29: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  28: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  27: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  26: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  23: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  22: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  21: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  20: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  19: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  17: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  16: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(14), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(14)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_1_ff_1_yy_1_xx_1_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), xx_3_init * T.int64(14) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(196))
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(392))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(14), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(14)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_1_ff_1_yy_1_xx_1_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), xx_3 * T.int64(14) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(14), T.int64(14)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_1_ff_1_yy_1_xx_1_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 128, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b120)
l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b121)
l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b122)
b162 = sch.get_block(name="conv2d_nchw", func_name="main")
l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b162)
b181 = sch.decompose_reduction(block=b162, loop=l166)
2024-03-22 02:32:09 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #832: GFLOPs: 1606.1327. Time: 64.0425 us. Best GFLOPs: 4941.1970
2024-03-22 02:39:14 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 02:39:14 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 02:39:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 386 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:39:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 773 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:39:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:39:16 [INFO] [evolutionary_search.cc:723] Sampled 63 candidate(s)
2024-03-22 02:39:17 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 81 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:39:19 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 100 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:39:22 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:39:24 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 108 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:39:25 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2681  1.2468  1.1244  1.1174  1.0493  1.0357  1.0327  1.0180  1.0114  1.0032  0.9851  0.9842  0.9774  0.9756  0.9696  0.9676
[17 : 32]:	0.9671  0.9588  0.9571  0.9570  0.9542  0.9514  0.9457  0.9442  0.9442  0.9396  0.9383  0.9381  0.9371  0.9356  0.9353  0.9339
[33 : 48]:	0.9336  0.9329  0.9272  0.9269  0.9256  0.9209  0.9201  0.9172  0.9165  0.9161  0.9152  0.9136  0.9120  0.9118  0.9110  0.9109
[49 : 64]:	0.9090  0.9086  0.9080  0.9069  0.9062  0.9053  0.9044  0.9033  0.9029  0.9025  0.9017  0.9009  0.9003  0.9001  0.8996  0.8994
2024-03-22 02:39:25 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 02:39:25 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #833: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(14)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), xx_3_init * T.int64(14) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(112)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(14)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), xx_3 * T.int64(14) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(14)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #834: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(14)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), xx_3_init * T.int64(14) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(56)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(14)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), xx_3 * T.int64(14) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(14)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #835: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + xx_3_init * T.int64(7) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(56)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + xx_3 * T.int64(7) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #836: GFLOPs: 2351.0663. Time: 43.7507 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #837: GFLOPs: 2271.5421. Time: 45.2824 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #838: GFLOPs: 2323.2215. Time: 44.2751 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #839: GFLOPs: 2315.4084. Time: 44.4245 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #840: GFLOPs: 895.9797. Time: 114.8026 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #841: GFLOPs: 3268.9628. Time: 31.4659 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #842: GFLOPs: 1282.8066. Time: 80.1842 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #843: GFLOPs: 2280.4752. Time: 45.1050 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #844: GFLOPs: 1572.4781. Time: 65.4132 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #845: GFLOPs: 803.7649. Time: 127.9737 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #846: GFLOPs: 1100.2352. Time: 93.4898 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #847: GFLOPs: 1740.6343. Time: 59.0939 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #848: GFLOPs: 301.2396. Time: 341.4584 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #849: GFLOPs: 307.8297. Time: 334.1484 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #850: GFLOPs: 2902.5279. Time: 35.4383 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #851: GFLOPs: 1643.2023. Time: 62.5978 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #852: GFLOPs: 1404.5046. Time: 73.2364 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #853: GFLOPs: 3678.1608. Time: 27.9653 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #854: GFLOPs: 3033.6409. Time: 33.9067 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #855: GFLOPs: 2274.2067. Time: 45.2293 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #856: GFLOPs: 2825.4928. Time: 36.4046 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #857: GFLOPs: 1647.0073. Time: 62.4532 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #858: GFLOPs: 386.5252. Time: 266.1167 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #859: GFLOPs: 3013.3745. Time: 34.1348 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #860: GFLOPs: 3071.6343. Time: 33.4873 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #861: GFLOPs: 1332.5387. Time: 77.1916 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #862: GFLOPs: 1671.4102. Time: 61.5413 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #863: GFLOPs: 3046.0449. Time: 33.7686 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #864: GFLOPs: 2114.8758. Time: 48.6368 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #865: GFLOPs: 300.9600. Time: 341.7756 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #866: GFLOPs: 3004.5002. Time: 34.2356 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #867: GFLOPs: 1304.9478. Time: 78.8237 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #868: GFLOPs: 1512.5024. Time: 68.0070 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #869: GFLOPs: 1292.1504. Time: 79.6044 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #870: GFLOPs: 1650.3379. Time: 62.3271 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #871: GFLOPs: 1294.5368. Time: 79.4576 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #872: GFLOPs: 2822.9722. Time: 36.4371 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #873: GFLOPs: 390.2075. Time: 263.6054 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #874: GFLOPs: 3111.9051. Time: 33.0540 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #875: GFLOPs: 2966.5866. Time: 34.6731 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #876: GFLOPs: 394.2718. Time: 260.8880 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #877: GFLOPs: 390.1453. Time: 263.6474 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #878: GFLOPs: 3174.2901. Time: 32.4043 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #879: GFLOPs: 1501.2687. Time: 68.5159 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #880: GFLOPs: 3712.0380. Time: 27.7101 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #881: GFLOPs: 3156.9906. Time: 32.5819 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #882: GFLOPs: 3181.2324. Time: 32.3336 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #883: GFLOPs: 3113.7324. Time: 33.0346 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #884: GFLOPs: 3696.1838. Time: 27.8289 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #885: GFLOPs: 1599.9475. Time: 64.2901 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #886: GFLOPs: 1565.4083. Time: 65.7086 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #887: GFLOPs: 1407.2867. Time: 73.0916 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #888: GFLOPs: 4233.4215. Time: 24.2973 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #889: GFLOPs: 3152.8056. Time: 32.6252 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #890: GFLOPs: 3180.1755. Time: 32.3444 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #891: GFLOPs: 3178.8016. Time: 32.3584 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #892: GFLOPs: 3185.8073. Time: 32.2872 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #893: GFLOPs: 1604.3668. Time: 64.1130 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #894: GFLOPs: 76.0238. Time: 1353.0085 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #895: GFLOPs: 367.3370. Time: 280.0175 us. Best GFLOPs: 4941.1970
2024-03-22 02:40:15 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #896: GFLOPs: 1066.2646. Time: 96.4684 us. Best GFLOPs: 4941.1970
2024-03-22 02:44:14 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 02:44:15 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 02:44:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 392 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:44:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 779 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:44:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1166 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:44:16 [INFO] [evolutionary_search.cc:723] Sampled 64 candidate(s)
2024-03-22 02:44:18 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 106 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:44:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 111 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:44:22 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 117 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:44:25 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:44:26 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0563  1.0131  0.9670  0.9657  0.9413  0.9063  0.9037  0.9032  0.8984  0.8960  0.8911  0.8903  0.8902  0.8869  0.8868  0.8866
[17 : 32]:	0.8858  0.8847  0.8838  0.8820  0.8815  0.8793  0.8791  0.8780  0.8779  0.8775  0.8775  0.8770  0.8759  0.8759  0.8756  0.8753
[33 : 48]:	0.8753  0.8751  0.8751  0.8740  0.8739  0.8732  0.8732  0.8723  0.8722  0.8717  0.8710  0.8705  0.8697  0.8696  0.8696  0.8695
[49 : 64]:	0.8695  0.8691  0.8685  0.8685  0.8684  0.8683  0.8683  0.8682  0.8681  0.8681  0.8681  0.8678  0.8674  0.8674  0.8671  0.8669
2024-03-22 02:44:26 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 02:44:26 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #897: GFLOPs: 1841.2682. Time: 55.8641 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #898: GFLOPs: 3058.9166. Time: 33.6265 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #899: GFLOPs: 1494.2879. Time: 68.8360 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #900: GFLOPs: 3033.8731. Time: 33.9041 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #901: GFLOPs: 4457.9902. Time: 23.0734 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #902: GFLOPs: 3906.4510. Time: 26.3310 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #903: GFLOPs: 2930.6081. Time: 35.0988 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #904: GFLOPs: 4135.2571. Time: 24.8741 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #905: GFLOPs: 4296.1968. Time: 23.9423 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #906: GFLOPs: 4635.0330. Time: 22.1920 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #907: GFLOPs: 4401.3045. Time: 23.3705 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #908: GFLOPs: 3905.8936. Time: 26.3348 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #909: GFLOPs: 4541.0838. Time: 22.6512 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #910: GFLOPs: 4355.5811. Time: 23.6159 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #911: GFLOPs: 2974.7041. Time: 34.5785 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #912: GFLOPs: 4538.8811. Time: 22.6621 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #913: GFLOPs: 3340.2526. Time: 30.7943 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #914: GFLOPs: 4321.1331. Time: 23.8041 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #915: GFLOPs: 4272.1308. Time: 24.0772 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #916: GFLOPs: 4277.5791. Time: 24.0465 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #917: GFLOPs: 4315.5627. Time: 23.8349 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #918: GFLOPs: 4353.4301. Time: 23.6275 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #919: GFLOPs: 4202.2900. Time: 24.4773 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #920: GFLOPs: 4235.0577. Time: 24.2879 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #921: GFLOPs: 4225.1043. Time: 24.3452 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #922: GFLOPs: 4244.4874. Time: 24.2340 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #923: GFLOPs: 4191.8280. Time: 24.5384 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #924: GFLOPs: 4321.4462. Time: 23.8024 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #925: GFLOPs: 4299.5571. Time: 23.9236 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #926: GFLOPs: 4232.8744. Time: 24.3005 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #927: GFLOPs: 4216.7056. Time: 24.3936 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #928: GFLOPs: 4270.7100. Time: 24.0852 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #929: GFLOPs: 4171.2836. Time: 24.6593 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #930: GFLOPs: 4315.7914. Time: 23.8336 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #931: GFLOPs: 4241.9416. Time: 24.2485 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #932: GFLOPs: 3808.6876. Time: 27.0069 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #933: GFLOPs: 4293.2449. Time: 23.9588 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #934: GFLOPs: 4293.5313. Time: 23.9572 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #935: GFLOPs: 3812.5536. Time: 26.9795 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #936: GFLOPs: 4271.3617. Time: 24.0815 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #937: GFLOPs: 4598.6171. Time: 22.3678 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #938: GFLOPs: 4218.9000. Time: 24.3810 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #939: GFLOPs: 4190.7249. Time: 24.5449 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #940: GFLOPs: 4290.7600. Time: 23.9726 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #941: GFLOPs: 4215.4870. Time: 24.4007 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #942: GFLOPs: 4190.8479. Time: 24.5441 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #943: GFLOPs: 4191.0828. Time: 24.5428 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #944: GFLOPs: 4133.2568. Time: 24.8861 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #945: GFLOPs: 4173.0135. Time: 24.6490 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #946: GFLOPs: 4222.8758. Time: 24.3580 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #947: GFLOPs: 3740.6250. Time: 27.4983 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #948: GFLOPs: 4331.7617. Time: 23.7457 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #949: GFLOPs: 3301.0247. Time: 31.1603 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #950: GFLOPs: 3193.8687. Time: 32.2057 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #951: GFLOPs: 4223.2359. Time: 24.3559 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #952: GFLOPs: 4223.1806. Time: 24.3562 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #953: GFLOPs: 4331.8057. Time: 23.7455 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #954: GFLOPs: 4331.5532. Time: 23.7469 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #955: GFLOPs: 4210.1719. Time: 24.4315 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #956: GFLOPs: 4326.5411. Time: 23.7744 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #957: GFLOPs: 4216.9414. Time: 24.3923 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #958: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  336: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  335: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  334: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  333: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  332: tvm::transform::Pass::operator()(tvm::IRModule) const
  331: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  330: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  329: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  328: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  327: _ZN3tvm7runtime13PackedFuncObj
  326: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  325: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  324: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  323: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  322: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  321: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  320: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  319: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  318: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  317: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  316: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  315: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  314: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  313: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  312: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  311: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  310: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  309: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  308: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  307: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  306: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  305: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  304: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  303: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  302: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  301: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  300: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  299: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  298: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  282: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  281: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  280: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  279: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  278: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  277: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  276: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  275: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  274: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  273: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  272: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  271: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  260: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  259: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  258: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  257: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  256: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  255: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  254: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  253: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  252: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  251: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  250: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  249: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  248: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  244: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  243: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  242: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  241: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  237: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  236: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  235: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  234: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  233: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  232: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  231: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  230: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  229: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  228: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  227: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  226: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  222: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  221: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  220: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  219: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  218: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  217: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  216: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  215: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  214: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  213: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  212: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  211: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  210: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  209: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  208: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  207: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  206: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  204: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  203: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  202: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  200: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  199: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  198: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  197: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  191: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  181: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  180: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  179: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  178: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  155: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  154: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  153: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  152: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  151: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  150: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  149: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  148: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  117: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  116: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  106: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  105: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  82: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  81: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  77: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  76: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  21: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::Stmt>::value, void>::type> const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  16: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  13: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  12: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  11: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  10: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  9: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runti
  8: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  7: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  6: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runtime9
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS_7ru
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home/canesche/tvm-0.16.dev0/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b120)
l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l137, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l137, ann_key="pragma_unroll_explicit", ann_val=1)
l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b122)
b162 = sch.get_block(name="conv2d_nchw", func_name="main")
l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b162)
b181 = sch.decompose_reduction(block=b162, loop=l166)
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #959: GFLOPs: 1454.3634. Time: 70.7257 us. Best GFLOPs: 4941.1970
2024-03-22 02:45:16 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #960: GFLOPs: 48.8214. Time: 2106.8800 us. Best GFLOPs: 4941.1970
2024-03-22 02:52:01 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 02:52:01 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 02:52:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 384 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:52:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 771 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:52:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1169 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:52:03 [INFO] [evolutionary_search.cc:723] Sampled 61 candidate(s)
2024-03-22 02:52:04 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 91 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:52:06 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 96 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:52:09 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 114 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:52:11 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 84 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 02:52:12 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2518  1.2320  1.1719  1.1243  1.0955  1.0797  1.0792  1.0668  1.0547  1.0239  0.9844  0.9754  0.9730  0.9598  0.9531  0.9430
[17 : 32]:	0.9377  0.9345  0.9301  0.9288  0.9250  0.9237  0.9167  0.9114  0.9110  0.9089  0.9080  0.9064  0.9048  0.9020  0.8981  0.8977
[33 : 48]:	0.8961  0.8958  0.8952  0.8923  0.8916  0.8893  0.8854  0.8837  0.8830  0.8828  0.8814  0.8804  0.8802  0.8794  0.8791  0.8787
[49 : 64]:	0.8787  0.8785  0.8785  0.8780  0.8754  0.8754  0.8746  0.8746  0.8743  0.8742  0.8741  0.8732  0.8730  0.8727  0.8718  0.8718
2024-03-22 02:52:12 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 02:52:12 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #961: GFLOPs: 804.0224. Time: 127.9328 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #962: GFLOPs: 1141.6751. Time: 90.0964 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #963: GFLOPs: 3031.6849. Time: 33.9286 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #964: GFLOPs: 2757.5814. Time: 37.3011 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #965: GFLOPs: 2368.3690. Time: 43.4311 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #966: GFLOPs: 1467.3207. Time: 70.1011 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #967: GFLOPs: 1450.1943. Time: 70.9290 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #968: GFLOPs: 1467.3206. Time: 70.1011 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #969: GFLOPs: 1466.1123. Time: 70.1589 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #970: GFLOPs: 1436.3181. Time: 71.6142 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #971: GFLOPs: 3199.1455. Time: 32.1526 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #972: GFLOPs: 2145.2503. Time: 47.9482 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #973: GFLOPs: 1121.6285. Time: 91.7067 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #974: GFLOPs: 3157.5653. Time: 32.5760 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #975: GFLOPs: 1255.8431. Time: 81.9058 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #976: GFLOPs: 2126.3981. Time: 48.3733 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #977: GFLOPs: 3121.1157. Time: 32.9564 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #978: GFLOPs: 3090.6173. Time: 33.2816 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #979: GFLOPs: 2126.3975. Time: 48.3733 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #980: GFLOPs: 4149.9576. Time: 24.7860 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #981: GFLOPs: 4024.9991. Time: 25.5555 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #982: GFLOPs: 4148.4829. Time: 24.7948 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #983: GFLOPs: 3122.0197. Time: 32.9469 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #984: GFLOPs: 4905.5618. Time: 20.9682 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #985: GFLOPs: 4004.9166. Time: 25.6836 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #986: GFLOPs: 4905.3993. Time: 20.9689 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #987: GFLOPs: 3148.0808. Time: 32.6741 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #988: GFLOPs: 4902.9024. Time: 20.9796 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #989: GFLOPs: 3144.2186. Time: 32.7143 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #990: GFLOPs: 4043.7367. Time: 25.4371 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #991: GFLOPs: 2185.0615. Time: 47.0746 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #992: GFLOPs: 3369.5237. Time: 30.5268 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #993: GFLOPs: 3658.4714. Time: 28.1158 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #994: GFLOPs: 3975.8929. Time: 25.8711 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #995: GFLOPs: 4509.4039. Time: 22.8103 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #996: GFLOPs: 4473.4818. Time: 22.9935 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #997: GFLOPs: 4021.1227. Time: 25.5801 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #998: GFLOPs: 4116.9515. Time: 24.9847 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #999: GFLOPs: 3912.6244. Time: 26.2895 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1000: GFLOPs: 3693.4627. Time: 27.8494 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1001: GFLOPs: 3564.7039. Time: 28.8554 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1002: GFLOPs: 4581.1857. Time: 22.4529 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1003: GFLOPs: 4279.0582. Time: 24.0382 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1004: GFLOPs: 4294.5042. Time: 23.9517 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1005: GFLOPs: 4326.4654. Time: 23.7748 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1006: GFLOPs: 4268.2463. Time: 24.0991 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1007: GFLOPs: 4538.4500. Time: 22.6643 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1008: GFLOPs: 4265.6712. Time: 24.1136 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1009: GFLOPs: 4265.8410. Time: 24.1127 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1010: GFLOPs: 4265.8692. Time: 24.1125 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1011: GFLOPs: 4265.7024. Time: 24.1134 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1012: GFLOPs: 3192.4178. Time: 32.2203 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1013: GFLOPs: 3348.7328. Time: 30.7163 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1014: GFLOPs: 3348.7981. Time: 30.7157 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1015: GFLOPs: 3706.4796. Time: 27.7516 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1016: GFLOPs: 4310.9768. Time: 23.8602 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1017: GFLOPs: 4298.2273. Time: 23.9310 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1018: GFLOPs: 4196.6311. Time: 24.5103 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1019: GFLOPs: 4197.1267. Time: 24.5074 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1020: GFLOPs: 4834.2326. Time: 21.2776 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1021: GFLOPs: 4740.7992. Time: 21.6969 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1022: GFLOPs: 546.7337. Time: 188.1369 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1023: GFLOPs: 22.7198. Time: 4527.3613 us. Best GFLOPs: 4941.1970
2024-03-22 02:53:04 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1024: GFLOPs: 1062.9399. Time: 96.7701 us. Best GFLOPs: 4941.1970
2024-03-22 03:05:47 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 03:05:47 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 03:05:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 389 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:05:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 772 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:05:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:05:49 [INFO] [evolutionary_search.cc:723] Sampled 71 candidate(s)
2024-03-22 03:05:50 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:05:52 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 106 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:05:55 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 78 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:05:57 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:05:58 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1648  1.1491  1.1361  1.1278  1.1097  1.1047  1.0909  1.0865  1.0800  1.0774  1.0634  1.0535  1.0516  1.0516  1.0446  1.0348
[17 : 32]:	1.0330  1.0330  1.0307  1.0297  1.0243  1.0236  1.0212  1.0179  1.0169  1.0169  1.0166  1.0153  1.0107  1.0074  1.0017  1.0010
[33 : 48]:	0.9992  0.9934  0.9708  0.9676  0.9675  0.9655  0.9632  0.9571  0.9542  0.9511  0.9511  0.9462  0.9458  0.9426  0.9423  0.9393
[49 : 64]:	0.9383  0.9381  0.9373  0.9367  0.9353  0.9351  0.9327  0.9218  0.9212  0.9194  0.9184  0.9183  0.9175  0.9174  0.9112  0.9109
2024-03-22 03:05:58 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 03:05:58 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1025: GFLOPs: 3315.0959. Time: 31.0280 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1026: GFLOPs: 2743.9918. Time: 37.4858 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1027: GFLOPs: 3341.7068. Time: 30.7809 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1028: GFLOPs: 3341.7305. Time: 30.7807 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1029: GFLOPs: 3298.3958. Time: 31.1851 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1030: GFLOPs: 3270.9359. Time: 31.4469 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1031: GFLOPs: 3302.7210. Time: 31.1443 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1032: GFLOPs: 3318.7196. Time: 30.9941 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1033: GFLOPs: 3271.0997. Time: 31.4453 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1034: GFLOPs: 3270.3327. Time: 31.4527 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1035: GFLOPs: 3272.6744. Time: 31.4302 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1036: GFLOPs: 3283.0788. Time: 31.3306 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1037: GFLOPs: 3283.1309. Time: 31.3301 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1038: GFLOPs: 3283.2689. Time: 31.3288 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1039: GFLOPs: 3285.7105. Time: 31.3055 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1040: GFLOPs: 3277.5591. Time: 31.3834 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1041: GFLOPs: 3277.6235. Time: 31.3827 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1042: GFLOPs: 3277.7422. Time: 31.3816 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1043: GFLOPs: 3307.7423. Time: 31.0970 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1044: GFLOPs: 3355.2306. Time: 30.6568 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1045: GFLOPs: 3303.4400. Time: 31.1375 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1046: GFLOPs: 2444.5841. Time: 42.0770 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1047: GFLOPs: 3317.8747. Time: 31.0020 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1048: GFLOPs: 3276.7776. Time: 31.3908 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1049: GFLOPs: 3267.1567. Time: 31.4833 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1050: GFLOPs: 3244.5617. Time: 31.7025 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1051: GFLOPs: 2431.2586. Time: 42.3076 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1052: GFLOPs: 4250.0655. Time: 24.2022 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1053: GFLOPs: 3092.3450. Time: 33.2630 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1054: GFLOPs: 2998.2765. Time: 34.3066 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1055: GFLOPs: 3289.7564. Time: 31.2670 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1056: GFLOPs: 4113.7470. Time: 25.0042 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1057: GFLOPs: 3248.7574. Time: 31.6616 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1058: GFLOPs: 3262.5260. Time: 31.5280 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1059: GFLOPs: 4164.6121. Time: 24.6988 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1060: GFLOPs: 2019.1577. Time: 50.9424 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1061: GFLOPs: 3920.2315. Time: 26.2385 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1062: GFLOPs: 4096.9035. Time: 25.1070 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1063: GFLOPs: 3222.8925. Time: 31.9157 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1064: GFLOPs: 4869.3120. Time: 21.1243 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1065: GFLOPs: 4853.6120. Time: 21.1926 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1066: GFLOPs: 4868.4696. Time: 21.1280 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1067: GFLOPs: 4861.2256. Time: 21.1594 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1068: GFLOPs: 4857.9375. Time: 21.1738 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1069: GFLOPs: 4898.9891. Time: 20.9963 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1070: GFLOPs: 4741.0522. Time: 21.6958 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1071: GFLOPs: 3281.0918. Time: 31.3496 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1072: GFLOPs: 3277.6216. Time: 31.3828 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1073: GFLOPs: 4740.9622. Time: 21.6962 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1074: GFLOPs: 4844.1581. Time: 21.2340 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1075: GFLOPs: 4891.7060. Time: 21.0276 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1076: GFLOPs: 4741.1790. Time: 21.6952 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1077: GFLOPs: 2017.6294. Time: 50.9810 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1078: GFLOPs: 4741.1610. Time: 21.6953 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1079: GFLOPs: 4874.1658. Time: 21.1033 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1080: GFLOPs: 4038.8170. Time: 25.4681 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1081: GFLOPs: 2544.1560. Time: 40.4302 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1082: GFLOPs: 3328.2043. Time: 30.9058 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1083: GFLOPs: 4702.0991. Time: 21.8755 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1084: GFLOPs: 4856.2954. Time: 21.1809 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1085: GFLOPs: 4115.6633. Time: 24.9925 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1086: GFLOPs: 1175.8111. Time: 87.4807 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1087: GFLOPs: 978.1041. Time: 105.1635 us. Best GFLOPs: 4941.1970
2024-03-22 03:06:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1088: GFLOPs: 116.9396. Time: 879.6061 us. Best GFLOPs: 4941.1970
2024-03-22 03:13:00 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 03:13:00 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 03:13:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 392 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:13:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 783 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:13:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1168 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:13:02 [INFO] [evolutionary_search.cc:723] Sampled 62 candidate(s)
2024-03-22 03:13:04 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:13:06 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 108 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:13:08 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 111 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:13:10 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:13:12 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0961  1.0794  1.0774  1.0654  0.9981  0.9871  0.9697  0.9575  0.9531  0.9521  0.9505  0.9490  0.9481  0.9475  0.9458  0.9457
[17 : 32]:	0.9448  0.9424  0.9414  0.9409  0.9404  0.9404  0.9400  0.9395  0.9389  0.9355  0.9350  0.9344  0.9324  0.9310  0.9307  0.9294
[33 : 48]:	0.9294  0.9287  0.9277  0.9267  0.9266  0.9244  0.9229  0.9225  0.9223  0.9212  0.9196  0.9178  0.9173  0.9166  0.9160  0.9140
[49 : 64]:	0.9138  0.9134  0.9133  0.9120  0.9119  0.9092  0.9080  0.9079  0.9077  0.9059  0.9056  0.9038  0.9028  0.9018  0.9016  0.9009
2024-03-22 03:13:12 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 03:13:12 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1089: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 < T.int64(64))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[64, 1, 2, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108 = sch.split(loop=l106, factors=[None, 196], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113 = sch.get_loops(block=b87)
l114, l115 = sch.split(loop=l113, factors=[None, 196], preserve_unit_iters=True)
sch.bind(loop=l115, thread_axis="threadIdx.x")
b116 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b116, ann_key="meta_schedule.unroll_explicit")
b117, b118, b119, b120 = sch.get_child_blocks(b116)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b117)
l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b118)
l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b119)
l151, l152, l153, l154, l155, l156, l157 = sch.get_loops(block=b120)
b158 = sch.get_block(name="conv2d_nchw", func_name="main")
l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176 = sch.get_loops(block=b158)
b177 = sch.decompose_reduction(block=b158, loop=l162)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1090: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1 < T.int64(64))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[64, 2, 1, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 98, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 98], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1091: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 < T.int64(64))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[64, 1, 2, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108 = sch.split(loop=l106, factors=[None, 196], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113 = sch.get_loops(block=b87)
l114, l115 = sch.split(loop=l113, factors=[None, 196], preserve_unit_iters=True)
sch.bind(loop=l115, thread_axis="threadIdx.x")
b116 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b116, ann_key="meta_schedule.unroll_explicit")
b117, b118, b119, b120 = sch.get_child_blocks(b116)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b117)
l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b118)
l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l151, l152, l153, l154, l155, l156, l157 = sch.get_loops(block=b120)
b158 = sch.get_block(name="conv2d_nchw", func_name="main")
l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176 = sch.get_loops(block=b158)
b177 = sch.decompose_reduction(block=b158, loop=l162)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1092: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(784) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(784) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(784) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 < T.int64(64))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[64, 1, 2, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 196, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 196], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1093: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1094: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1095: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1096: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(64) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1097: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4096))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1098: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1099: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1100: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1101: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1102: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(64) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 8, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1103: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1104: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4096))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1105: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1106: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1107: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4096))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1108: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1109: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1110: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1111: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 64, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1112: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1113: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1114: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1115: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1116: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1117: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1118: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(64) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1119: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1120: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1121: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(64) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1122: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1123: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 64, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1124: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1125: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(392), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(392), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1568) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1568) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1568) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(392), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(784) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(784) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(98) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(98) // T.int64(14) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[64, 1, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 392, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 392, 2], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b120)
l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b121)
l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b122)
b162 = sch.get_block(name="conv2d_nchw", func_name="main")
l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b162)
b181 = sch.decompose_reduction(block=b162, loop=l166)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1126: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1127: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1128: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1129: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1130: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4096))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1131: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1132: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1133: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 64, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1134: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1135: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1136: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1137: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1138: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1139: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(2048))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(64) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 8, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1140: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1141: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1142: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1143: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(16) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1144: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1145: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(64) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1146: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4096))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 128, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1147: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(74)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4096))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1148: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4096))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1149: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4096))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1150: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(7) + yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(392))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(7) + yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108 = sch.split(loop=l106, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113 = sch.get_loops(block=b87)
l114, l115 = sch.split(loop=l113, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l115, thread_axis="threadIdx.x")
b116 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b116, ann_key="meta_schedule.unroll_explicit")
b117, b118, b119, b120 = sch.get_child_blocks(b116)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b117)
l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b118)
l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l151, l152, l153, l154, l155, l156, l157 = sch.get_loops(block=b120)
b158 = sch.get_block(name="conv2d_nchw", func_name="main")
l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176 = sch.get_loops(block=b158)
b177 = sch.decompose_reduction(block=b158, loop=l162)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1151: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(64) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(21)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1 < T.int64(2048))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(64) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused * T.int64(16) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(7) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 16, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 98, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 98], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 03:13:29 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1152: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(7), T.int64(7)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(7) + xx_3_init * T.int64(7) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(1024), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused)
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(196))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(T.int64(1024), rc_0_ry_0_rx_0_fused)
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(7), T.int64(7)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(7) + xx_3 * T.int64(7) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0_ry_0_rx_0_fused + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(7), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106 = sch.get_loops(block=b74)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b87)
l115, l116 = sch.split(loop=l114, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b117 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b117, ann_key="meta_schedule.unroll_explicit")
b118, b119, b120, b121 = sch.get_child_blocks(b117)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b118)
l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b119)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-03-22 03:16:37 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 03:16:37 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 03:16:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 395 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:16:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 776 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:16:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1161 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:16:39 [INFO] [evolutionary_search.cc:723] Sampled 69 candidate(s)
2024-03-22 03:16:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:16:43 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:16:45 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 117 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:16:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 103 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:16:49 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0739  0.9792  0.9709  0.9632  0.9595  0.9569  0.9531  0.9531  0.9515  0.9490  0.9450  0.9446  0.9426  0.9375  0.9343  0.9330
[17 : 32]:	0.9312  0.9308  0.9302  0.9263  0.9251  0.9248  0.9245  0.9228  0.9207  0.9206  0.9196  0.9183  0.9165  0.9162  0.9144  0.9139
[33 : 48]:	0.9127  0.9125  0.9097  0.9085  0.9082  0.9080  0.9064  0.9060  0.9049  0.9041  0.9017  0.9006  0.9000  0.8995  0.8977  0.8971
[49 : 64]:	0.8949  0.8941  0.8929  0.8897  0.8888  0.8885  0.8881  0.8856  0.8853  0.8851  0.8845  0.8844  0.8841  0.8834  0.8829  0.8828
2024-03-22 03:16:49 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 03:16:49 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1153: GFLOPs: 3387.1692. Time: 30.3678 us. Best GFLOPs: 4941.1970
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1154: GFLOPs: 4965.6841. Time: 20.7143 us. Best GFLOPs: 4965.6841
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1155: GFLOPs: 4924.6977. Time: 20.8867 us. Best GFLOPs: 4965.6841
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1156: GFLOPs: 4937.8675. Time: 20.8310 us. Best GFLOPs: 4965.6841
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1157: GFLOPs: 2415.4129. Time: 42.5852 us. Best GFLOPs: 4965.6841
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1158: GFLOPs: 4914.7466. Time: 20.9290 us. Best GFLOPs: 4965.6841
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1159: GFLOPs: 3314.6620. Time: 31.0321 us. Best GFLOPs: 4965.6841
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1160: GFLOPs: 3314.7364. Time: 31.0314 us. Best GFLOPs: 4965.6841
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1161: GFLOPs: 4925.6852. Time: 20.8825 us. Best GFLOPs: 4965.6841
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1162: GFLOPs: 4979.9167. Time: 20.6551 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1163: GFLOPs: 4912.6460. Time: 20.9380 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1164: GFLOPs: 4933.5113. Time: 20.8494 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1165: GFLOPs: 4892.8184. Time: 21.0228 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1166: GFLOPs: 4901.2107. Time: 20.9868 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1167: GFLOPs: 4162.3647. Time: 24.7121 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1168: GFLOPs: 4947.6446. Time: 20.7899 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1169: GFLOPs: 4754.6915. Time: 21.6335 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1170: GFLOPs: 4888.2133. Time: 21.0426 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1171: GFLOPs: 4967.2135. Time: 20.7079 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1172: GFLOPs: 4886.2243. Time: 21.0512 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1173: GFLOPs: 4894.0911. Time: 21.0173 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1174: GFLOPs: 4744.5339. Time: 21.6799 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1175: GFLOPs: 4655.3973. Time: 22.0950 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1176: GFLOPs: 4894.2548. Time: 21.0166 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1177: GFLOPs: 4.3208. Time: 23806.1568 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1178: GFLOPs: 4899.8596. Time: 20.9926 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1179: GFLOPs: 4752.0210. Time: 21.6457 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1180: GFLOPs: 4877.6030. Time: 21.0884 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1181: GFLOPs: 4596.9939. Time: 22.3757 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1182: GFLOPs: 4632.9789. Time: 22.2019 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1183: GFLOPs: 4752.6724. Time: 21.6427 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1184: GFLOPs: 4898.2339. Time: 20.9996 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1185: GFLOPs: 3841.2788. Time: 26.7777 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1186: GFLOPs: 4395.7525. Time: 23.4000 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1187: GFLOPs: 4424.8154. Time: 23.2463 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1188: GFLOPs: 3767.8264. Time: 27.2998 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1189: GFLOPs: 4697.7965. Time: 21.8955 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1190: GFLOPs: 4833.0993. Time: 21.2826 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1191: GFLOPs: 4882.0921. Time: 21.0690 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1192: GFLOPs: 4751.5071. Time: 21.6480 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1193: GFLOPs: 4582.9362. Time: 22.4443 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1194: GFLOPs: 4052.5647. Time: 25.3817 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1195: GFLOPs: 4685.4261. Time: 21.9534 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1196: GFLOPs: 4853.1194. Time: 21.1948 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1197: GFLOPs: 4321.3378. Time: 23.8030 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1198: GFLOPs: 4329.0465. Time: 23.7606 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1199: GFLOPs: 4185.1409. Time: 24.5776 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1200: GFLOPs: 4858.5506. Time: 21.1711 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1201: GFLOPs: 4855.7652. Time: 21.1832 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1202: GFLOPs: 4288.6387. Time: 23.9845 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1203: GFLOPs: 3997.9279. Time: 25.7285 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1204: GFLOPs: 4592.5016. Time: 22.3976 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1205: GFLOPs: 4174.8740. Time: 24.6381 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1206: GFLOPs: 4683.2641. Time: 21.9635 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1207: GFLOPs: 4180.5392. Time: 24.6047 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1208: GFLOPs: 4194.0174. Time: 24.5256 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1209: GFLOPs: 3668.5744. Time: 28.0384 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1210: GFLOPs: 4298.1476. Time: 23.9314 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1211: GFLOPs: 4471.9871. Time: 23.0011 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1212: GFLOPs: 3849.2211. Time: 26.7225 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1213: GFLOPs: 4159.4603. Time: 24.7294 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1214: GFLOPs: 446.6596. Time: 230.2890 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1215: GFLOPs: 1888.1235. Time: 54.4778 us. Best GFLOPs: 4979.9167
2024-03-22 03:17:45 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1216: GFLOPs: 409.3751. Time: 251.2630 us. Best GFLOPs: 4979.9167
2024-03-22 03:19:36 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 03:19:36 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 03:19:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 383 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:19:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 768 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:19:37 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-03-22 03:19:39 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 140 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:19:41 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 119 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:19:43 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:19:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 116 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:19:47 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1749  1.0084  0.9913  0.9902  0.9892  0.9867  0.9823  0.9774  0.9753  0.9713  0.9708  0.9705  0.9697  0.9694  0.9691  0.9685
[17 : 32]:	0.9684  0.9681  0.9678  0.9675  0.9674  0.9669  0.9666  0.9663  0.9661  0.9660  0.9657  0.9652  0.9648  0.9643  0.9630  0.9630
[33 : 48]:	0.9629  0.9628  0.9627  0.9619  0.9617  0.9616  0.9615  0.9597  0.9593  0.9580  0.9579  0.9577  0.9576  0.9574  0.9571  0.9567
[49 : 64]:	0.9562  0.9555  0.9551  0.9550  0.9546  0.9543  0.9539  0.9520  0.9519  0.9511  0.9504  0.9504  0.9492  0.9475  0.9472  0.9469
2024-03-22 03:19:47 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 03:19:47 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1217: GFLOPs: 2835.2933. Time: 36.2787 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1218: GFLOPs: 4813.0434. Time: 21.3713 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1219: GFLOPs: 4936.2636. Time: 20.8378 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1220: GFLOPs: 4889.4789. Time: 21.0372 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1221: GFLOPs: 4935.6415. Time: 20.8404 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1222: GFLOPs: 4933.8777. Time: 20.8479 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1223: GFLOPs: 4935.7050. Time: 20.8401 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1224: GFLOPs: 4799.7437. Time: 21.4305 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1225: GFLOPs: 3239.3916. Time: 31.7531 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1226: GFLOPs: 4889.1330. Time: 21.0387 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1227: GFLOPs: 2371.8190. Time: 43.3679 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1228: GFLOPs: 4929.8206. Time: 20.8650 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1229: GFLOPs: 4917.8173. Time: 20.9159 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1230: GFLOPs: 4936.7551. Time: 20.8357 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1231: GFLOPs: 4870.2748. Time: 21.1201 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1232: GFLOPs: 4874.5847. Time: 21.1014 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1233: GFLOPs: 4896.7322. Time: 21.0060 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1234: GFLOPs: 4733.0326. Time: 21.7325 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1235: GFLOPs: 4786.1681. Time: 21.4913 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1236: GFLOPs: 4908.7171. Time: 20.9547 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1237: GFLOPs: 3251.8575. Time: 31.6314 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1238: GFLOPs: 4949.8105. Time: 20.7808 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1239: GFLOPs: 2322.9457. Time: 44.2803 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1240: GFLOPs: 4905.7883. Time: 20.9672 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1241: GFLOPs: 4878.5170. Time: 21.0844 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1242: GFLOPs: 4883.2278. Time: 21.0641 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1243: GFLOPs: 4903.8869. Time: 20.9754 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1244: GFLOPs: 4906.2172. Time: 20.9654 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1245: GFLOPs: 3662.7946. Time: 28.0826 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1246: GFLOPs: 4900.6895. Time: 20.9890 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1247: GFLOPs: 4902.0725. Time: 20.9831 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1248: GFLOPs: 4902.0532. Time: 20.9832 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1249: GFLOPs: 4892.6265. Time: 21.0236 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1250: GFLOPs: 4883.0915. Time: 21.0647 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1251: GFLOPs: 4727.3340. Time: 21.7587 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1252: GFLOPs: 4882.5770. Time: 21.0669 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1253: GFLOPs: 4875.3964. Time: 21.0979 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1254: GFLOPs: 4744.5360. Time: 21.6798 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1255: GFLOPs: 4807.0464. Time: 21.3979 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1256: GFLOPs: 4713.4177. Time: 21.8230 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1257: GFLOPs: 4887.8407. Time: 21.0442 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1258: GFLOPs: 4739.8391. Time: 21.7013 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1259: GFLOPs: 4935.1222. Time: 20.8426 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1260: GFLOPs: 4815.6819. Time: 21.3596 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1261: GFLOPs: 4923.2192. Time: 20.8930 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1262: GFLOPs: 4873.7981. Time: 21.1049 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1263: GFLOPs: 4879.5850. Time: 21.0798 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1264: GFLOPs: 4784.0728. Time: 21.5007 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1265: GFLOPs: 3203.7008. Time: 32.1069 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1266: GFLOPs: 4528.1302. Time: 22.7160 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1267: GFLOPs: 4839.5169. Time: 21.2544 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1268: GFLOPs: 4869.6921. Time: 21.1226 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1269: GFLOPs: 4806.5338. Time: 21.4002 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1270: GFLOPs: 4900.0817. Time: 20.9917 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1271: GFLOPs: 2854.5246. Time: 36.0343 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1272: GFLOPs: 4877.8692. Time: 21.0872 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1273: GFLOPs: 4836.0983. Time: 21.2694 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1274: GFLOPs: 4724.3399. Time: 21.7725 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1275: GFLOPs: 4877.5718. Time: 21.0885 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1276: GFLOPs: 4877.2770. Time: 21.0898 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1277: GFLOPs: 3194.7840. Time: 32.1965 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1278: GFLOPs: 585.2957. Time: 175.7416 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1279: GFLOPs: 126.9368. Time: 810.3308 us. Best GFLOPs: 4979.9167
2024-03-22 03:20:43 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1280: GFLOPs: 102.0119. Time: 1008.3216 us. Best GFLOPs: 4979.9167
2024-03-22 03:26:49 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 03:26:49 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 03:26:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 390 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:26:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 775 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:26:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:26:51 [INFO] [evolutionary_search.cc:723] Sampled 73 candidate(s)
2024-03-22 03:26:53 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:26:55 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 134 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:26:58 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:27:00 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 108 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:27:01 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3753  1.3483  1.3332  1.3309  1.3249  1.3197  1.3136  1.3000  1.1843  1.1180  1.1138  1.1124  1.0985  1.0985  1.0666  1.0651
[17 : 32]:	1.0490  1.0474  1.0472  1.0455  1.0443  1.0443  1.0423  1.0400  1.0319  1.0304  1.0289  1.0284  1.0257  1.0252  1.0242  1.0198
[33 : 48]:	1.0146  1.0118  1.0086  1.0066  1.0046  1.0016  1.0016  0.9997  0.9995  0.9972  0.9964  0.9963  0.9950  0.9944  0.9930  0.9901
[49 : 64]:	0.9868  0.9849  0.9844  0.9844  0.9812  0.9806  0.9805  0.9802  0.9800  0.9796  0.9773  0.9769  0.9767  0.9763  0.9763  0.9761
2024-03-22 03:27:01 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 03:27:01 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1281: GFLOPs: 2733.9016. Time: 37.6242 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1282: GFLOPs: 2446.4908. Time: 42.0442 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1283: GFLOPs: 2710.7999. Time: 37.9448 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1284: GFLOPs: 2722.5829. Time: 37.7806 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1285: GFLOPs: 2709.6522. Time: 37.9609 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1286: GFLOPs: 2710.6921. Time: 37.9463 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1287: GFLOPs: 2678.4262. Time: 38.4034 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1288: GFLOPs: 2588.6060. Time: 39.7360 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1289: GFLOPs: 1357.3800. Time: 75.7789 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1290: GFLOPs: 3408.2347. Time: 30.1801 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1291: GFLOPs: 3266.2521. Time: 31.4920 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1292: GFLOPs: 3106.0258. Time: 33.1165 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1293: GFLOPs: 3512.9002. Time: 29.2809 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1294: GFLOPs: 3682.8518. Time: 27.9297 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1295: GFLOPs: 2539.4644. Time: 40.5049 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1296: GFLOPs: 3521.6382. Time: 29.2082 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1297: GFLOPs: 4911.5102. Time: 20.9428 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1298: GFLOPs: 185.3250. Time: 555.0293 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1299: GFLOPs: 3595.4887. Time: 28.6083 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1300: GFLOPs: 4903.7587. Time: 20.9759 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1301: GFLOPs: 4910.0992. Time: 20.9488 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1302: GFLOPs: 4909.5068. Time: 20.9514 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1303: GFLOPs: 4907.9563. Time: 20.9580 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1304: GFLOPs: 4935.3519. Time: 20.8416 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1305: GFLOPs: 4871.3802. Time: 21.1153 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1306: GFLOPs: 4777.6095. Time: 21.5298 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1307: GFLOPs: 4940.9020. Time: 20.8182 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1308: GFLOPs: 4772.5115. Time: 21.5528 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1309: GFLOPs: 4918.4722. Time: 20.9132 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1310: GFLOPs: 4640.4228. Time: 22.1663 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1311: GFLOPs: 4941.4825. Time: 20.8158 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1312: GFLOPs: 4910.0329. Time: 20.9491 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1313: GFLOPs: 4913.4253. Time: 20.9346 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1314: GFLOPs: 4778.9719. Time: 21.5236 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1315: GFLOPs: 4834.5028. Time: 21.2764 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1316: GFLOPs: 4798.3817. Time: 21.4366 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1317: GFLOPs: 4726.7994. Time: 21.7612 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1318: GFLOPs: 4911.9756. Time: 20.9408 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1319: GFLOPs: 4912.2038. Time: 20.9398 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1320: GFLOPs: 4667.6619. Time: 22.0369 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1321: GFLOPs: 4855.4137. Time: 21.1848 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1322: GFLOPs: 4877.9463. Time: 21.0869 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1323: GFLOPs: 4870.7950. Time: 21.1179 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1324: GFLOPs: 4740.7086. Time: 21.6973 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1325: GFLOPs: 4667.6239. Time: 22.0371 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1326: GFLOPs: 4855.6191. Time: 21.1839 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1327: GFLOPs: 4652.4590. Time: 22.1089 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1328: GFLOPs: 4870.0481. Time: 21.1211 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1329: GFLOPs: 4915.6454. Time: 20.9252 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1330: GFLOPs: 4860.8718. Time: 21.1610 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1331: GFLOPs: 4740.6413. Time: 21.6977 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1332: GFLOPs: 4740.5466. Time: 21.6981 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1333: GFLOPs: 4833.7117. Time: 21.2799 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1334: GFLOPs: 4829.3204. Time: 21.2992 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1335: GFLOPs: 4850.2204. Time: 21.2074 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1336: GFLOPs: 4862.8715. Time: 21.1523 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1337: GFLOPs: 4854.3965. Time: 21.1892 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1338: GFLOPs: 4867.8477. Time: 21.1307 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1339: GFLOPs: 4737.2554. Time: 21.7132 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1340: GFLOPs: 4887.5513. Time: 21.0455 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1341: GFLOPs: 4860.5484. Time: 21.1624 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1342: GFLOPs: 109.5099. Time: 939.2834 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1343: GFLOPs: 405.6792. Time: 253.5521 us. Best GFLOPs: 4979.9167
2024-03-22 03:27:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1344: GFLOPs: 1213.7435. Time: 84.7467 us. Best GFLOPs: 4979.9167
2024-03-22 03:30:59 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 03:31:00 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 03:31:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 387 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:31:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 771 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:31:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1164 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:31:01 [INFO] [evolutionary_search.cc:723] Sampled 66 candidate(s)
2024-03-22 03:31:03 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 148 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:31:05 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:31:08 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:31:10 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 122 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:31:11 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1095  1.1079  1.1037  1.0791  1.0613  1.0547  1.0535  1.0529  1.0516  1.0455  1.0378  1.0373  1.0195  1.0147  1.0049  0.9969
[17 : 32]:	0.9932  0.9929  0.9921  0.9910  0.9901  0.9893  0.9885  0.9882  0.9880  0.9875  0.9869  0.9851  0.9848  0.9846  0.9843  0.9825
[33 : 48]:	0.9821  0.9821  0.9814  0.9810  0.9802  0.9801  0.9798  0.9789  0.9786  0.9783  0.9781  0.9781  0.9780  0.9769  0.9758  0.9750
[49 : 64]:	0.9727  0.9721  0.9714  0.9711  0.9703  0.9698  0.9695  0.9690  0.9688  0.9678  0.9677  0.9676  0.9672  0.9658  0.9655  0.9623
2024-03-22 03:31:11 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 03:31:11 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1345: GFLOPs: 4463.1626. Time: 23.0466 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1346: GFLOPs: 4752.8068. Time: 21.6421 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1347: GFLOPs: 4673.9119. Time: 22.0074 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1348: GFLOPs: 3672.6802. Time: 28.0070 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1349: GFLOPs: 3669.2598. Time: 28.0331 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1350: GFLOPs: 3770.3326. Time: 27.2816 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1351: GFLOPs: 3500.1578. Time: 29.3875 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1352: GFLOPs: 3885.2944. Time: 26.4744 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1353: GFLOPs: 3316.1722. Time: 31.0179 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1354: GFLOPs: 4116.3765. Time: 24.9882 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1355: GFLOPs: 3042.9177. Time: 33.8033 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1356: GFLOPs: 4937.8687. Time: 20.8310 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1357: GFLOPs: 4944.1359. Time: 20.8046 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1358: GFLOPs: 4925.4473. Time: 20.8835 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1359: GFLOPs: 4910.8888. Time: 20.9455 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1360: GFLOPs: 4882.8699. Time: 21.0656 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1361: GFLOPs: 4913.7548. Time: 20.9332 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1362: GFLOPs: 4888.9308. Time: 21.0395 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1363: GFLOPs: 4859.8947. Time: 21.1652 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1364: GFLOPs: 4857.4733. Time: 21.1758 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1365: GFLOPs: 4874.4083. Time: 21.1022 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1366: GFLOPs: 4889.8119. Time: 21.0357 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1367: GFLOPs: 4882.2443. Time: 21.0683 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1368: GFLOPs: 4911.2398. Time: 20.9440 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1369: GFLOPs: 4888.9097. Time: 21.0396 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1370: GFLOPs: 4910.5268. Time: 20.9470 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1371: GFLOPs: 4911.0451. Time: 20.9448 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1372: GFLOPs: 4906.4817. Time: 20.9643 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1373: GFLOPs: 4919.1183. Time: 20.9104 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1374: GFLOPs: 4874.0128. Time: 21.1039 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1375: GFLOPs: 4903.3288. Time: 20.9777 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1376: GFLOPs: 4880.3377. Time: 21.0766 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1377: GFLOPs: 4920.0324. Time: 20.9065 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1378: GFLOPs: 4928.5467. Time: 20.8704 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1379: GFLOPs: 4906.8431. Time: 20.9627 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1380: GFLOPs: 4904.0179. Time: 20.9748 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1381: GFLOPs: 4951.8031. Time: 20.7724 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1382: GFLOPs: 4865.6509. Time: 21.1402 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1383: GFLOPs: 4877.8357. Time: 21.0874 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1384: GFLOPs: 4882.1559. Time: 21.0687 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1385: GFLOPs: 4904.0512. Time: 20.9747 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1386: GFLOPs: 4852.0828. Time: 21.1993 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1387: GFLOPs: 4848.7851. Time: 21.2137 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1388: GFLOPs: 4834.8257. Time: 21.2750 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1389: GFLOPs: 3781.3588. Time: 27.2021 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1390: GFLOPs: 4861.8707. Time: 21.1566 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1391: GFLOPs: 4846.0269. Time: 21.2258 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1392: GFLOPs: 4890.8242. Time: 21.0314 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1393: GFLOPs: 4201.0909. Time: 24.4843 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1394: GFLOPs: 4880.5620. Time: 21.0756 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1395: GFLOPs: 4878.1413. Time: 21.0861 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1396: GFLOPs: 4857.4146. Time: 21.1760 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1397: GFLOPs: 4888.6193. Time: 21.0409 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1398: GFLOPs: 4862.2141. Time: 21.1551 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1399: GFLOPs: 4845.9191. Time: 21.2263 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1400: GFLOPs: 4810.9251. Time: 21.3807 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1401: GFLOPs: 4878.8455. Time: 21.0830 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1402: GFLOPs: 4793.7906. Time: 21.4571 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1403: GFLOPs: 4186.2989. Time: 24.5708 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1404: GFLOPs: 4912.6015. Time: 20.9382 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1405: GFLOPs: 4848.7418. Time: 21.2139 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1406: GFLOPs: 54.6529. Time: 1882.0741 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1407: GFLOPs: 2300.2278. Time: 44.7177 us. Best GFLOPs: 4979.9167
2024-03-22 03:32:07 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1408: GFLOPs: 33.9601. Time: 3028.8714 us. Best GFLOPs: 4979.9167
2024-03-22 03:41:32 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 03:41:33 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 03:41:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 390 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:41:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 785 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:41:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1169 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:41:34 [INFO] [evolutionary_search.cc:723] Sampled 61 candidate(s)
2024-03-22 03:41:36 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 107 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:41:38 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 148 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:41:41 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:41:43 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 151 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:41:44 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1470  1.1333  1.1333  1.0851  1.0826  1.0779  1.0718  1.0671  1.0516  1.0513  1.0400  1.0349  0.9991  0.9847  0.9834  0.9833
[17 : 32]:	0.9824  0.9818  0.9805  0.9805  0.9801  0.9789  0.9784  0.9782  0.9780  0.9773  0.9772  0.9771  0.9767  0.9761  0.9741  0.9739
[33 : 48]:	0.9739  0.9724  0.9722  0.9715  0.9713  0.9700  0.9670  0.9652  0.9651  0.9642  0.9640  0.9640  0.9638  0.9629  0.9628  0.9616
[49 : 64]:	0.9614  0.9600  0.9598  0.9598  0.9596  0.9587  0.9576  0.9576  0.9576  0.9576  0.9571  0.9570  0.9565  0.9565  0.9565  0.9564
2024-03-22 03:41:44 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 03:41:44 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1409: GFLOPs: 660.9053. Time: 155.6362 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1410: GFLOPs: 635.2767. Time: 161.9150 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1411: GFLOPs: 802.8568. Time: 128.1185 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1412: GFLOPs: 528.8191. Time: 194.5104 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1413: GFLOPs: 660.9606. Time: 155.6232 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1414: GFLOPs: 662.4808. Time: 155.2661 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1415: GFLOPs: 593.2540. Time: 173.3841 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1416: GFLOPs: 593.1331. Time: 173.4194 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1417: GFLOPs: 646.8680. Time: 159.0136 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1418: GFLOPs: 468.0129. Time: 219.7820 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1419: GFLOPs: 485.9978. Time: 211.6487 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1420: GFLOPs: 642.0664. Time: 160.2027 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1421: GFLOPs: 468.0480. Time: 219.7655 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1422: GFLOPs: 4933.2776. Time: 20.8504 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1423: GFLOPs: 4942.6980. Time: 20.8107 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1424: GFLOPs: 4942.0014. Time: 20.8136 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1425: GFLOPs: 4916.3267. Time: 20.9223 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1426: GFLOPs: 4949.9250. Time: 20.7803 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1427: GFLOPs: 4913.8864. Time: 20.9327 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1428: GFLOPs: 4914.6778. Time: 20.9293 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1429: GFLOPs: 4911.4531. Time: 20.9430 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1430: GFLOPs: 4905.1345. Time: 20.9700 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1431: GFLOPs: 4905.6930. Time: 20.9676 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1432: GFLOPs: 4898.7058. Time: 20.9975 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1433: GFLOPs: 4903.3033. Time: 20.9779 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1434: GFLOPs: 4912.7639. Time: 20.9375 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1435: GFLOPs: 4897.4320. Time: 21.0030 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1436: GFLOPs: 4875.4930. Time: 21.0975 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1437: GFLOPs: 4886.8940. Time: 21.0483 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1438: GFLOPs: 4870.3433. Time: 21.1198 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1439: GFLOPs: 4895.5394. Time: 21.0111 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1440: GFLOPs: 4861.8596. Time: 21.1567 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1441: GFLOPs: 4861.8269. Time: 21.1568 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1442: GFLOPs: 4881.0902. Time: 21.0733 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1443: GFLOPs: 4841.0519. Time: 21.2476 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1444: GFLOPs: 4861.3284. Time: 21.1590 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1445: GFLOPs: 4840.5939. Time: 21.2496 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1446: GFLOPs: 4900.0163. Time: 20.9919 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1447: GFLOPs: 4858.8397. Time: 21.1698 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1448: GFLOPs: 4857.6476. Time: 21.1750 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1449: GFLOPs: 4878.7400. Time: 21.0835 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1450: GFLOPs: 4838.3662. Time: 21.2594 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1451: GFLOPs: 4862.1958. Time: 21.1552 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1452: GFLOPs: 4860.7098. Time: 21.1617 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1453: GFLOPs: 4894.6005. Time: 21.0152 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1454: GFLOPs: 4832.5972. Time: 21.2848 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1455: GFLOPs: 4742.7293. Time: 21.6881 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1456: GFLOPs: 4847.9074. Time: 21.2176 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1457: GFLOPs: 4741.1231. Time: 21.6955 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1458: GFLOPs: 4833.1312. Time: 21.2824 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1459: GFLOPs: 4742.8240. Time: 21.6877 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1460: GFLOPs: 4742.5147. Time: 21.6891 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1461: GFLOPs: 4742.7992. Time: 21.6878 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1462: GFLOPs: 4568.1136. Time: 22.5171 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1463: GFLOPs: 4809.3987. Time: 21.3875 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1464: GFLOPs: 4809.1932. Time: 21.3884 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1465: GFLOPs: 4809.1612. Time: 21.3885 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1466: GFLOPs: 4808.1998. Time: 21.3928 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1467: GFLOPs: 4841.7556. Time: 21.2445 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1468: GFLOPs: 4740.7854. Time: 21.6970 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1469: GFLOPs: 4841.2759. Time: 21.2466 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1470: GFLOPs: 188.9432. Time: 544.4007 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1471: GFLOPs: 699.4104. Time: 147.0679 us. Best GFLOPs: 4979.9167
2024-03-22 03:42:40 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1472: GFLOPs: 997.3257. Time: 103.1366 us. Best GFLOPs: 4979.9167
2024-03-22 03:48:55 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 03:48:55 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 03:48:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 386 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:48:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 765 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:48:56 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-03-22 03:48:58 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 175 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:49:01 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:49:03 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:49:05 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 134 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:49:06 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9903  0.9891  0.9886  0.9870  0.9859  0.9855  0.9843  0.9843  0.9838  0.9838  0.9824  0.9822  0.9818  0.9813  0.9808  0.9797
[17 : 32]:	0.9792  0.9792  0.9791  0.9786  0.9786  0.9786  0.9773  0.9770  0.9764  0.9750  0.9748  0.9744  0.9743  0.9743  0.9738  0.9725
[33 : 48]:	0.9718  0.9662  0.9656  0.9646  0.9646  0.9644  0.9630  0.9615  0.9615  0.9613  0.9612  0.9609  0.9606  0.9591  0.9584  0.9582
[49 : 64]:	0.9576  0.9576  0.9572  0.9560  0.9556  0.9555  0.9553  0.9552  0.9552  0.9551  0.9550  0.9550  0.9544  0.9543  0.9542  0.9528
2024-03-22 03:49:06 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 03:49:07 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1473: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1474: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 128, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1475: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 128, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1476: GFLOPs: 4818.1251. Time: 21.3487 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1477: GFLOPs: 4915.6922. Time: 20.9250 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1478: GFLOPs: 4925.6344. Time: 20.8828 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1479: GFLOPs: 4896.8587. Time: 21.0055 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1480: GFLOPs: 4897.9216. Time: 21.0009 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1481: GFLOPs: 4906.0543. Time: 20.9661 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1482: GFLOPs: 4895.7920. Time: 21.0100 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1483: GFLOPs: 4902.3773. Time: 20.9818 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1484: GFLOPs: 4890.6933. Time: 21.0319 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1485: GFLOPs: 4927.1436. Time: 20.8764 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1486: GFLOPs: 4893.4191. Time: 21.0202 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1487: GFLOPs: 4894.3201. Time: 21.0164 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1488: GFLOPs: 4905.3288. Time: 20.9692 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1489: GFLOPs: 4905.1673. Time: 20.9699 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1490: GFLOPs: 4883.0314. Time: 21.0649 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1491: GFLOPs: 4900.8389. Time: 20.9884 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1492: GFLOPs: 4871.7000. Time: 21.1139 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1493: GFLOPs: 4883.7551. Time: 21.0618 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1494: GFLOPs: 4838.0261. Time: 21.2609 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1495: GFLOPs: 4927.5039. Time: 20.8748 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1496: GFLOPs: 4845.9836. Time: 21.2260 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1497: GFLOPs: 4867.9446. Time: 21.1302 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1498: GFLOPs: 4896.8772. Time: 21.0054 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1499: GFLOPs: 4883.2928. Time: 21.0638 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1500: GFLOPs: 4882.3816. Time: 21.0678 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1501: GFLOPs: 4851.7235. Time: 21.2009 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1502: GFLOPs: 4858.7110. Time: 21.1704 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1503: GFLOPs: 4850.1254. Time: 21.2079 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1504: GFLOPs: 4865.6812. Time: 21.1401 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1505: GFLOPs: 4854.8907. Time: 21.1870 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1506: GFLOPs: 4844.0323. Time: 21.2345 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1507: GFLOPs: 4854.2681. Time: 21.1898 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1508: GFLOPs: 4797.3487. Time: 21.4412 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1509: GFLOPs: 4792.8348. Time: 21.4614 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1510: GFLOPs: 4791.1481. Time: 21.4689 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1511: GFLOPs: 4803.5895. Time: 21.4133 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1512: GFLOPs: 4678.3026. Time: 21.9868 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1513: GFLOPs: 4679.1329. Time: 21.9829 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1514: GFLOPs: 4678.6270. Time: 21.9853 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1515: GFLOPs: 4722.2512. Time: 21.7822 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1516: GFLOPs: 4823.8983. Time: 21.3232 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1517: GFLOPs: 4677.8918. Time: 21.9887 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1518: GFLOPs: 4755.5546. Time: 21.6296 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1519: GFLOPs: 4733.7890. Time: 21.7291 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1520: GFLOPs: 4229.6619. Time: 24.3189 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1521: GFLOPs: 4820.4739. Time: 21.3383 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1522: GFLOPs: 4747.0747. Time: 21.6682 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1523: GFLOPs: 4583.3765. Time: 22.4421 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1524: GFLOPs: 4756.3699. Time: 21.6259 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1525: GFLOPs: 4816.1704. Time: 21.3574 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1526: GFLOPs: 4914.4140. Time: 20.9304 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1527: GFLOPs: 4690.0767. Time: 21.9316 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1528: GFLOPs: 4323.1934. Time: 23.7928 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1529: GFLOPs: 4744.8822. Time: 21.6783 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1530: GFLOPs: 4726.2650. Time: 21.7637 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1531: GFLOPs: 4748.3421. Time: 21.6625 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1532: GFLOPs: 4545.3462. Time: 22.6299 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1533: GFLOPs: 4638.8921. Time: 22.1736 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1534: GFLOPs: 948.8030. Time: 108.4111 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1535: GFLOPs: 1130.2925. Time: 91.0037 us. Best GFLOPs: 4979.9167
2024-03-22 03:50:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1536: GFLOPs: 314.4724. Time: 327.0901 us. Best GFLOPs: 4979.9167
2024-03-22 03:55:46 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 03:55:46 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 03:55:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 390 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:55:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 773 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:55:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:55:48 [INFO] [evolutionary_search.cc:723] Sampled 60 candidate(s)
2024-03-22 03:55:49 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:55:52 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:55:54 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:55:56 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 03:55:58 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9916  0.9875  0.9852  0.9796  0.9791  0.9743  0.9733  0.9729  0.9725  0.9686  0.9669  0.9643  0.9637  0.9620  0.9612  0.9597
[17 : 32]:	0.9576  0.9560  0.9519  0.9516  0.9512  0.9506  0.9505  0.9490  0.9481  0.9466  0.9466  0.9453  0.9445  0.9440  0.9434  0.9429
[33 : 48]:	0.9410  0.9370  0.9362  0.9361  0.9357  0.9332  0.9326  0.9317  0.9316  0.9309  0.9304  0.9304  0.9282  0.9282  0.9272  0.9267
[49 : 64]:	0.9267  0.9267  0.9266  0.9262  0.9259  0.9255  0.9251  0.9251  0.9250  0.9246  0.9246  0.9246  0.9240  0.9237  0.9235  0.9234
2024-03-22 03:55:58 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 03:55:58 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1537: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1538: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 64, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1539: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1540: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1541: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/canesche/tvm-0.16.dev0/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/canesche/tvm-0.16.dev0/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFuncObj
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home/canesche/tvm-0.16.dev0/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1542: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1024), rc_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1024), rc_0 * T.int64(128) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1543: GFLOPs: 4942.5483. Time: 20.8113 us. Best GFLOPs: 4979.9167
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1544: GFLOPs: 4952.2643. Time: 20.7705 us. Best GFLOPs: 4979.9167
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1545: GFLOPs: 4936.2963. Time: 20.8376 us. Best GFLOPs: 4979.9167
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1546: GFLOPs: 5040.6106. Time: 20.4064 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1547: GFLOPs: 4989.2583. Time: 20.6165 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1548: GFLOPs: 4941.7597. Time: 20.8146 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1549: GFLOPs: 4942.5969. Time: 20.8111 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1550: GFLOPs: 4077.7574. Time: 25.2248 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1551: GFLOPs: 3997.6687. Time: 25.7302 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1552: GFLOPs: 4888.2133. Time: 21.0426 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1553: GFLOPs: 4884.1457. Time: 21.0601 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1554: GFLOPs: 4771.2538. Time: 21.5584 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1555: GFLOPs: 4822.0330. Time: 21.3314 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1556: GFLOPs: 4881.6646. Time: 21.0708 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1557: GFLOPs: 4971.4232. Time: 20.6904 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1558: GFLOPs: 4816.0971. Time: 21.3577 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1559: GFLOPs: 4776.1325. Time: 21.5364 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1560: GFLOPs: 4737.8612. Time: 21.7104 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1561: GFLOPs: 4783.9677. Time: 21.5011 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1562: GFLOPs: 4744.1868. Time: 21.6814 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1563: GFLOPs: 4743.9322. Time: 21.6826 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1564: GFLOPs: 4809.0012. Time: 21.3892 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1565: GFLOPs: 2799.0352. Time: 36.7487 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1566: GFLOPs: 4637.8221. Time: 22.1787 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1567: GFLOPs: 4652.3411. Time: 22.1095 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1568: GFLOPs: 4723.0612. Time: 21.7784 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1569: GFLOPs: 4684.9875. Time: 21.9554 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1570: GFLOPs: 3863.9499. Time: 26.6206 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1571: GFLOPs: 4503.9721. Time: 22.8378 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1572: GFLOPs: 3864.2844. Time: 26.6183 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1573: GFLOPs: 4632.2666. Time: 22.2053 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1574: GFLOPs: 4325.8023. Time: 23.7784 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1575: GFLOPs: 3707.8371. Time: 27.7415 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1576: GFLOPs: 4401.0098. Time: 23.3721 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1577: GFLOPs: 3702.9351. Time: 27.7782 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1578: GFLOPs: 4760.7115. Time: 21.6062 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1579: GFLOPs: 4764.5096. Time: 21.5890 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1580: GFLOPs: 4764.3480. Time: 21.5897 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1581: GFLOPs: 4723.4696. Time: 21.7765 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1582: GFLOPs: 4831.0595. Time: 21.2916 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1583: GFLOPs: 4638.3811. Time: 22.1760 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1584: GFLOPs: 4667.2217. Time: 22.0390 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1585: GFLOPs: 4667.4470. Time: 22.0379 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1586: GFLOPs: 4667.2553. Time: 22.0388 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1587: GFLOPs: 4693.8118. Time: 21.9141 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1588: GFLOPs: 4646.4832. Time: 22.1373 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1589: GFLOPs: 4587.2958. Time: 22.4230 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1590: GFLOPs: 4213.1359. Time: 24.4143 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1591: GFLOPs: 4212.9376. Time: 24.4155 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1592: GFLOPs: 4212.9141. Time: 24.4156 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1593: GFLOPs: 4629.0024. Time: 22.2209 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1594: GFLOPs: 4628.7937. Time: 22.2219 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1595: GFLOPs: 4673.0128. Time: 22.0117 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1596: GFLOPs: 4673.2142. Time: 22.0107 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1597: GFLOPs: 4213.9721. Time: 24.4095 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1598: GFLOPs: 112.7811. Time: 912.0395 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1599: GFLOPs: 552.9463. Time: 186.0231 us. Best GFLOPs: 5040.6106
2024-03-22 03:56:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1600: GFLOPs: 52.3129. Time: 1966.2607 us. Best GFLOPs: 5040.6106
2024-03-22 04:01:42 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 04:01:42 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 04:01:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 388 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:01:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 777 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:01:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1169 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:01:43 [INFO] [evolutionary_search.cc:723] Sampled 61 candidate(s)
2024-03-22 04:01:45 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:01:47 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:01:50 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:01:52 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:01:53 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9768  0.9765  0.9695  0.9681  0.9662  0.9659  0.9608  0.9599  0.9585  0.9559  0.9543  0.9539  0.9537  0.9479  0.9479  0.9477
[17 : 32]:	0.9413  0.9382  0.9380  0.9378  0.9377  0.9371  0.9349  0.9305  0.9271  0.9263  0.9248  0.9248  0.9243  0.9243  0.9243  0.9217
[33 : 48]:	0.9217  0.9217  0.9187  0.9185  0.9168  0.9149  0.9149  0.9147  0.9147  0.9124  0.9113  0.9095  0.9079  0.9075  0.9074  0.9074
[49 : 64]:	0.9071  0.9070  0.9061  0.9058  0.9057  0.9055  0.9055  0.9054  0.9050  0.9049  0.9047  0.9047  0.9046  0.9046  0.9040  0.9036
2024-03-22 04:01:53 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 04:01:53 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1601: GFLOPs: 4953.1444. Time: 20.7668 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1602: GFLOPs: 4923.8503. Time: 20.8903 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1603: GFLOPs: 4907.1473. Time: 20.9614 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1604: GFLOPs: 4939.2457. Time: 20.8252 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1605: GFLOPs: 4879.9100. Time: 21.0784 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1606: GFLOPs: 4911.4157. Time: 20.9432 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1607: GFLOPs: 4909.2021. Time: 20.9527 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1608: GFLOPs: 4892.4015. Time: 21.0246 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1609: GFLOPs: 4882.3316. Time: 21.0680 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1610: GFLOPs: 4882.4470. Time: 21.0675 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1611: GFLOPs: 4873.7743. Time: 21.1050 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1612: GFLOPs: 4875.8175. Time: 21.0961 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1613: GFLOPs: 4875.3006. Time: 21.0984 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1614: GFLOPs: 4517.4017. Time: 22.7699 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1615: GFLOPs: 4729.7249. Time: 21.7477 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1616: GFLOPs: 4754.0555. Time: 21.6364 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1617: GFLOPs: 4795.4154. Time: 21.4498 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1618: GFLOPs: 4314.7288. Time: 23.8395 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1619: GFLOPs: 4613.6202. Time: 22.2950 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1620: GFLOPs: 4768.0258. Time: 21.5730 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1621: GFLOPs: 4772.2228. Time: 21.5541 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1622: GFLOPs: 4770.6177. Time: 21.5613 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1623: GFLOPs: 4729.6621. Time: 21.7480 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1624: GFLOPs: 4308.3740. Time: 23.8746 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1625: GFLOPs: 4289.8966. Time: 23.9775 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1626: GFLOPs: 4639.4992. Time: 22.1707 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1627: GFLOPs: 4662.8997. Time: 22.0594 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1628: GFLOPs: 4758.4544. Time: 21.6164 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1629: GFLOPs: 4662.7762. Time: 22.0600 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1630: GFLOPs: 4662.9131. Time: 22.0593 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1631: GFLOPs: 4662.7788. Time: 22.0600 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1632: GFLOPs: 4728.9699. Time: 21.7512 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1633: GFLOPs: 4711.2885. Time: 21.8328 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1634: GFLOPs: 4760.9406. Time: 21.6051 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1635: GFLOPs: 4541.4204. Time: 22.6495 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1636: GFLOPs: 4269.7465. Time: 24.0906 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1637: GFLOPs: 4638.6516. Time: 22.1747 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1638: GFLOPs: 4637.8989. Time: 22.1783 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1639: GFLOPs: 4638.6688. Time: 22.1746 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1640: GFLOPs: 4626.6734. Time: 22.2321 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1641: GFLOPs: 4632.3610. Time: 22.2048 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1642: GFLOPs: 4711.8735. Time: 21.8301 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1643: GFLOPs: 4664.3299. Time: 22.0526 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1644: GFLOPs: 4756.1870. Time: 21.6267 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1645: GFLOPs: 3816.0611. Time: 26.9547 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1646: GFLOPs: 4593.9596. Time: 22.3904 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1647: GFLOPs: 4593.9575. Time: 22.3905 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1648: GFLOPs: 4593.7805. Time: 22.3913 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1649: GFLOPs: 4587.6794. Time: 22.4211 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1650: GFLOPs: 4574.9598. Time: 22.4834 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1651: GFLOPs: 4436.5537. Time: 23.1848 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1652: GFLOPs: 4629.5914. Time: 22.2181 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1653: GFLOPs: 4226.4542. Time: 24.3374 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1654: GFLOPs: 4520.8840. Time: 22.7524 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1655: GFLOPs: 4520.9078. Time: 22.7522 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1656: GFLOPs: 4214.5795. Time: 24.4059 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1657: GFLOPs: 4235.0747. Time: 24.2878 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1658: GFLOPs: 4361.7320. Time: 23.5826 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1659: GFLOPs: 4630.0639. Time: 22.2158 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1660: GFLOPs: 4626.3943. Time: 22.2335 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1661: GFLOPs: 4633.7878. Time: 22.1980 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1662: GFLOPs: 298.0068. Time: 345.1626 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1663: GFLOPs: 90.7546. Time: 1133.3951 us. Best GFLOPs: 5040.6106
2024-03-22 04:02:49 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1664: GFLOPs: 11.4823. Time: 8958.2081 us. Best GFLOPs: 5040.6106
2024-03-22 04:07:18 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-03-22 04:07:18 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-03-22 04:07:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 388 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:07:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 778 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:07:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 1173 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:07:20 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2024-03-22 04:07:22 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:07:24 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:07:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 147 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:07:29 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5c885f2c8e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5c8860de2528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5c887123f928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5c8868d87628)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5c885a398148)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5c885d83c378)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x5c886e2b4f98)]: 0 failure(s)
2024-03-22 04:07:30 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9661  0.9648  0.9640  0.9611  0.9605  0.9547  0.9488  0.9480  0.9469  0.9459  0.9415  0.9413  0.9407  0.9403  0.9399  0.9393
[17 : 32]:	0.9393  0.9380  0.9318  0.9309  0.9304  0.9304  0.9304  0.9294  0.9149  0.9116  0.9116  0.9114  0.9110  0.9108  0.9108  0.9105
[33 : 48]:	0.9105  0.9104  0.9104  0.9100  0.9097  0.9097  0.9094  0.9091  0.9087  0.9080  0.9065  0.9065  0.9061  0.9061  0.9054  0.9052
[49 : 64]:	0.9052  0.9052  0.9050  0.9046  0.9041  0.9040  0.9040  0.9039  0.9034  0.9030  0.9022  0.9016  0.9016  0.9013  0.9012  0.9009
2024-03-22 04:07:30 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-03-22 04:07:30 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1665: GFLOPs: 4990.5432. Time: 20.6111 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1666: GFLOPs: 4925.9136. Time: 20.8816 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1667: GFLOPs: 4902.6398. Time: 20.9807 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1668: GFLOPs: 4931.0738. Time: 20.8597 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1669: GFLOPs: 4893.3197. Time: 21.0207 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1670: GFLOPs: 4872.0886. Time: 21.1123 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1671: GFLOPs: 4746.5088. Time: 21.6708 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1672: GFLOPs: 4954.2760. Time: 20.7620 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1673: GFLOPs: 4793.5357. Time: 21.4582 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1674: GFLOPs: 4913.7548. Time: 20.9332 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1675: GFLOPs: 4877.8953. Time: 21.0871 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1676: GFLOPs: 4832.0490. Time: 21.2872 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1677: GFLOPs: 4885.1493. Time: 21.0558 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1678: GFLOPs: 4740.2630. Time: 21.6994 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1679: GFLOPs: 4736.7212. Time: 21.7156 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1680: GFLOPs: 4690.4554. Time: 21.9298 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1681: GFLOPs: 4754.8508. Time: 21.6328 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1682: GFLOPs: 4736.2546. Time: 21.7178 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1683: GFLOPs: 4646.2362. Time: 22.1385 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1684: GFLOPs: 4637.6495. Time: 22.1795 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1685: GFLOPs: 4674.0201. Time: 22.0069 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1686: GFLOPs: 4673.9484. Time: 22.0073 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1687: GFLOPs: 4735.8870. Time: 21.7194 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1688: GFLOPs: 4654.4691. Time: 22.0994 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1689: GFLOPs: 4675.6961. Time: 21.9990 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1690: GFLOPs: 4651.0734. Time: 22.1155 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1691: GFLOPs: 4669.5849. Time: 22.0278 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1692: GFLOPs: 4372.1199. Time: 23.5265 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1693: GFLOPs: 4683.4518. Time: 21.9626 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1694: GFLOPs: 4235.5485. Time: 24.2851 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1695: GFLOPs: 4218.6260. Time: 24.3825 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1696: GFLOPs: 4227.3826. Time: 24.3320 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1697: GFLOPs: 4228.5850. Time: 24.3251 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1698: GFLOPs: 4737.3865. Time: 21.7126 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1699: GFLOPs: 4224.9074. Time: 24.3463 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1700: GFLOPs: 4735.3978. Time: 21.7217 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1701: GFLOPs: 4683.5148. Time: 21.9623 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1702: GFLOPs: 4683.4123. Time: 21.9628 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1703: GFLOPs: 4228.9357. Time: 24.3231 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1704: GFLOPs: 4683.7448. Time: 21.9612 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1705: GFLOPs: 3748.2518. Time: 27.4423 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1706: GFLOPs: 4197.2304. Time: 24.5068 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1707: GFLOPs: 4675.6942. Time: 21.9990 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1708: GFLOPs: 4691.5443. Time: 21.9247 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1709: GFLOPs: 4197.7543. Time: 24.5038 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1710: GFLOPs: 4160.5680. Time: 24.7228 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1711: GFLOPs: 4523.6551. Time: 22.7384 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1712: GFLOPs: 4691.7669. Time: 21.9237 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1713: GFLOPs: 4617.5182. Time: 22.2762 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1714: GFLOPs: 4621.9945. Time: 22.2546 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1715: GFLOPs: 4659.1767. Time: 22.0770 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1716: GFLOPs: 4621.4064. Time: 22.2575 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1717: GFLOPs: 4153.4792. Time: 24.7650 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1718: GFLOPs: 4508.4727. Time: 22.8150 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1719: GFLOPs: 4507.6700. Time: 22.8191 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1720: GFLOPs: 4210.1441. Time: 24.4317 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1721: GFLOPs: 4366.6896. Time: 23.5558 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1722: GFLOPs: 4156.4002. Time: 24.7476 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1723: GFLOPs: 4208.6298. Time: 24.4404 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1724: GFLOPs: 4658.3397. Time: 22.0810 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1725: GFLOPs: 4634.3328. Time: 22.1954 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1726: GFLOPs: 296.2956. Time: 347.1560 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1727: GFLOPs: 8.4185. Time: 12218.3677 us. Best GFLOPs: 5040.6106
2024-03-22 04:08:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_conv2d_add_nn_relu_8] Trial #1728: GFLOPs: 10.4900. Time: 9805.6376 us. Best GFLOPs: 5040.6106
